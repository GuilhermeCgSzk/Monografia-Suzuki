%The tests were performed using the \gls{BUTPPG}~\cite{butppg}. In such a database, it was utilized a smartphone to record 48 \acrshort{ppg} signals of 12 \acrlong{sbjs} (\acrshort{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. This database can be consumed through Physionet~\cite{physionet} interface via \texttt{wfdb} package. The proposed method was implemented in Python. Implementations of \acrshort{gaf}, \acrshort{mtf} and \acrshort{rp} were provided by PyTS \cite{pyts} library. The PyTorch library~\cite{pytorch} was used to perform the training and the classification operations. The models used in this study are listed in Table~\ref{tab:results} and provided by TorchVision. Hyper-parameters optimization was performed using the Optuna library~\cite{optuna}. For each model, 50 Optuna trials were performed for fitting and validating the \acrshort{ml} model with median pruner to avoid excessive computation of trial epochs that do not show hope of better results.

%After the optimization, the metrics of the best trial were evaluated in the testing dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \acrshort{ml} model in hands, the dataset was divided into folds using the cross-validation \acrfull{loso} re-sampling method, into pieces matching each of the 12 \acrshort{sbjs}. For each fold, the smaller split was used as the testing dataset for the evaluation of the model's metrics, while the bigger split was subdivided into the training dataset, of size 7 \acrshort{sbjs}; and into the validation dataset, of size 4 \acrshort{sbjs}. Applying such an experimental setup allowed the generation of results concerning the metrics present in Table~\ref{tab:results}, where, for each projection method, all models were seen as samples of a statistics population possessing 5 values corresponding to the mean of all 12 folds of each metric.

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \acrshort{sbjs}; validation, with 2 \acrshort{sbjs}; and test, with 3 \acrshort{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \acrshort{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%In order to evaluate those projection methods, the following metrics were considered accuracy score, the proportion of hits $Accuracy = \frac{(TP+TN)}{TP+TN+FP+FN}$; precision score, the proportion of correct positive guesses, $Precision = \frac{TP}{TP+FP}$; recall score, the proportion of found positive samples, $Recall = \frac{TP}{TP+FN}$; and F1 score, the harmonic mean between the precision score and the recall score, $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$. Additionally, it was measured the Cohen Kappa score, the degree of agreement of annotators for a classification problem, $CohenKappa = 1-\frac{1-p_o}{1-p_e}$, where $p_o$ is the proportion of observed concordance and $p_e$ is the probability of concordance between all annotators. In the supervised binary classification domain, only two annotators, corresponding to the predicted and the real labels, and two classes, corresponding to positive or negative, are considered, in a manner that the confusion matrix can be used directly to evaluate the score, by the formula $CohenKappa=\frac{2\cdot (TP\cdot TN - FP \cdot FN)}{(TP+FP)\cdot(FP+TN)+(TP+FN)\cdot(FN+TN)}$.

% Those metrics were evaluated for 70 \acrshort{ml} models implemented in \href{https://pytorch.org/}{PyTorch}, the ones listed on Table~ \ref{tab:results}
% For such networks, the same experiment framework was applied, highlighting three major process blocks: dataset construction, when the database was loaded and transformed; hyperparameters selection, when, for each \acrshort{ml} model, a search was done to try to find the set of its best hyperparameters for the built dataset; and projection metrics evaluation, when each model, in conjunction with its set of best hyper-parameters found, was tested, generating the metrics present in this article. %(the framework is shown in figure \ref{figure:framework})
% Also, hyper-parameters selection and projections evaluation apply training and testing cycles.  To clarify those processes, they will be described in detail in the following sections.


%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\linewidth]{imgs/framework2.pdf}
%    \caption{Flowchart representing the experimental framework. Gray elements are entities, while blue elements are processes.}
%    \label{figure:framework}
%\end{figure*}

% \subsection{Training and Testing}

% Machine learning tasks involve 2 main steps: training and testing. Such a format was applied several times on the hyper-parameters selection process, for testing certain sets of hyper-parameters; and the projections evaluation process, for evaluating the model and its set of hyper-parameters efficiency. That core task was done using \href{https://pytorch.org/}{PyTorch} python library, which automatically computes the gradients based on the user's implementation \cite{pytorch}. Also, it allows parallel GPU processing to be done, by applying tensor-based operations to batches of the dataset, which size used in this experiment was the whole dataset. 

% The training process parameters also involve an optimizer and and loss function. The optimizer algorithm to be used was Adam\cite{adam}, while the Cross-Entropy Loss function was used, %\cite{cross-entropy-loss} 
% both implemented in \href{https://pytorch.org/}{PyTorch}. Moreover, the training process needs to stop at some point, fact that was done using an early stopping approach, computing the median absolute deviation, deviation metric robust to outliers, %\cite{?}
% of a window of the last 10 loss function values on the validation dataset, stopping the training process when such windows converge to, at least, a deviation of value 1. When the training process stopped, the best result found was chosen.

% \subsection{Dataset construction}

% In order to provide data to train and test \acrshort{ml} models, the Brno University of Technology Smartphone PPG Database was used as a starting point. In such a database, it was utilized a smartphone to record 48 \acrshort{ppg} signals of 12 \acrlong{sbjs} (\acrshort{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. Additionally, other refinement procedures were done to perfect that database, such as cropping 20 seconds of the recording, leaving the middle 10 seconds intact\cite{butppg}. And, finally, the database is publicly available on Physionet through the link \url{https://physionet.org/content/butppg/1.0.0/}. 

% However, the database is in \acrfull{wfdb} format \cite{butppg}, one dimensional signal format in function of time, %\cite{wfdb}
% letting yet to be done the application of projection methods. For such purpose, PyTS was used, a Python package developed for time series classification, containing the desired projection algorithms, \acrshort{gaf}, \acrshort{mtf} and \acrshort{rp} \cite{pyts}, with its implementations of PyTS version 0.13.0 stored in \href{https://zenodo.org/}{Zenodo} repository \cite{pyts-v0.13.0}.
    
% With such a package, it was possible to produce a dataset containing all projections as 2D images with 3 channels with shape $3 \times siglen^2 \times siglen^2$, where $siglen$ is the signal length. For each projection, its matrix was repeated on every channel of the image, while the \gls{PM} filled every channel with each of the 3 mentioned projections. Finally, it was necessary to resize every image for each \acrshort{ml} model input shape, for such purpose that it was used \href{https://pytorch.org/}{PyTorch} resize transform with bi-linear interpolation. After those procedures, the dataset was ready for the following processes, as the hyper-parameters selection.

% \subsection{Hyper-parameters selection}

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \acrshort{sbjs}; validation, with 2 \acrshort{sbjs}; and test, with 3 \acrshort{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \acrshort{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%\subsection{Projection Metrics Evaluation}

%After the optimization, the metrics of the best trial were evaluated in the test dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \acrshort{ml} model in hands, the dataset was divided into folds using the cross-validation \acrfull{loso} re-sampling method, into pieces matching each of the 12 \acrshort{sbjs}. For each fold, the left split was used as the test dataset for the evaluation of the model's metrics, while the other split was subdivided into the train dataset, of size 7 \acrshort{sbjs}; and into the valid dataset, of size 4 \acrshort{sbjs}. Applying such an experimental setup allowed to generate results with respect to the before-mentioned metrics, where, for each projection method, all models are seen as samples of a statistics population possessing 5 values corresponding to the metrics.

%Figure~\ref{fig:boxplots} depicts presents the boxplot of Accuracy, F1-score, Precision, Recall, and Cohen's Kappa distributions for different \glspl{CV} models tested on the \gls{BUTPPG} database. From these graphs, we can notice that, for the recall score, all projections are very equated. Moreover, it is noticeable that \acrshort{rp} and the proposed \gls{PM} have improved performances when compared with other \gls{mtf} and \gls{gaf} projections, especially when we observe the minimum, maximum, and the first quartiles of distributions, presented in Table~\ref{tab:boxplots}. From this table, we can perceive, at first glance, that the \gls{rp} and the \gls{PM} are generally superior methods if compared with the other two. In fact, \acrshort{rp} and \gls{PM} first quartiles are greater than \acrshort{gaf} and \acrshort{mtf} third quartiles considering all metrics except Recall Score, which shows that 75\% of samples in \acrshort{rp} and \gls{PM} are generally greater than 75\% of samples in \acrshort{gaf} and \acrshort{mtf}. The detailed performance of the points in the distributions considered in Figure~\ref{fig:boxplots} is depicted per model in Table~\ref{tab:results}.

%\input{tables/results}

% Each of the 4 populations produced box plots shown in figure \ref{fig:boxplots}, allowing, at first glance, to see the Recurrence Plot and the \gls{PM} as generally superior methods if compared with the other two. In fact, \acrshort{rp} and P\gls{PM} first quartiles are greater than \acrshort{gaf} and \acrshort{mtf} third quartiles considering all metrics except Recall Score, what shows that 75\% of samples in \acrshort{rp} and \gls{PM} are generally greater than 75\% of samples in \acrshort{gaf} and \acrshort{mtf}.    %TODO: falar do Cohen Cappa Score 

\section{Experimental Setup}

This section discusses the experimental setup, analyzing elements used in the experiment, such as datasets, programming libraries and predictive models. Additionally, it clarifies metrics to be evaluated and approaches to measure them.  

\subsection{The dataset}

As for every machine learning task, we need a dataset to provide data to feed the predictive models for their parameters fitting, molding them to the domain of the specific task. In this work, the task of assessing the quality of the signal is cleary a supervised classification problem, that is, can be described as the problem of finding a function that best defines a predefined set of pairs of variables and label, $(X,y)$. In this scope, the pair corresponds to the signal itself mapped to its quality label, ``Good'' or ``Bad''. For the purpose of training the predictive methods and evaluating their ability to fit to the problem of classifying the quality of heartbeat time series, I employed the \acrshort{BUTPPG}~\cite{butppg} dataset.

\input{tex/figuras/samples}

The \acrfull{BUTPPG} is a publicly available database produced by the Department of Biomedical Engineering of the the Brno University of Technology. It contains samples of \acrshort{PPG} signals, its quality labels and its heat rate estimations. Those signals were extracted using a low-cost method: recording with the camera of a smartphone. To be more precise, they recorded the subject's index finger, covering the lens of the camera and its LED light. Then, they measured, for each video frame, the average of the intensities of the red channel of every image pixel, resulting in a time series of averages. Finally, they inverted the signal. The Figure \ref{fig:butppg_samples} shows examples of the results of such a sequence of procedures.  

They done this method of obtaining \acrshort{PPG} signals  48 times, ammount distributed equally between 12 subjects. That is, for each subject, they did 4 measurements. Moreover, they did those recordings in two possible situations: one which the subject sat down and stayed static, case which the quality label ``Good'' was probable; and other in which the subject walked, hence, case likely to be a ``Bad'' recording. That distinction is relevant, since the walking situation occurred only 1 time for each subject, biasing the labels proportion to be nearly 25\% of ``Bad'' ones. Therefore, this dataset is imbalanced, factor that we need to handle in our experiment.

As for the definition of the signal quality labels, they designated specialists to estimate the heart rate associated with the \acrshort{PPG} signals, with the help of a software specialized for the analysis. Then, the organizers compared the number they gave with the one given by a gold standard method that, instead of using the \acrshort{PPG} signal, used an ECG recording. The measurer syncronized the ECG mannually. If the specialist measured with an error less or equal than 5 bpm, then the organizers considered the estimative correct. Finally, if 3 specialists of 5 gave correct estimations, the organizers considered the quality of the \acrshort{PPG} signal as correct. Thus, the dataset ``Good'' labels, in essence, identify if a signal is human-readable. 

\subsection{The dataset split}

Machine learning tasks also requires the separation the dataset into fragments. One of them is the training dataset, used for the models parameters adjustment. Another is the test dataset, used for evaluating the models efficiency. An optional one is the validation dataset, used to choose the set of best hyperparameters of a trained models. In this experiment case, to define the training-test splits, I used a cross validation method named \acrfull{LOSO}, which partitions the dataset into k pieces and k train-test splits. It makes the i-th train-test split assigning the i-th piece as the test dataset, lefting the other k-1 pieces as the training dataset. In the \acrshort{BUTPPG} case, the k value is 12, the number of subjects. Notice that the smaller unity of division is the subject, not the signals that are associated with it. I did it to increase the difference between training samples and testing samples. Since the dataset is small, such a split method allows the maximum provect of the avaliable resources, because it uses every sample in the dataset as a test object at least once, without biasing the results. Additionally, the training datset was divided producing a validation dataset of size 3, with usage that will be explained later.

\subsection{The models}

To evaluate the proposed projection-based framework and find particular overperforming cases, it is necessary to involve a big ammount of machine learning models. Firstly, I compared the projection-based framework with other existing approaches, by utilizing the Aeon-toolkit\footnote{Documentation avaliable at \url{https://www.aeon-toolkit.org/en/stable/}} python library, with models listed in table \ref{tab:non_cv_list}. Furthermore, I combined the proposed method with a wide variet of classification \acrshort{CV} models. For that, I used the Pytorch\footnote{Documentation avaliable at \url{https://pytorch.org/docs/stable/index.html}} python library, which aggregates a wide variety of neural network design strategies. Those strategies varies from simply convolutional networks to vision transformers. The table \ref{tab:cv_list} lists all \acrshort{CV} models involved in the experiment. With that, I submited the method to an abrangent set of scenarios.

\input{tex/tabelas/cv_listing}

\input{tex/tabelas/aeon_listing}

However, even though I defined the models, my setting lacks the choice of their hyperparameters. Those kind of parameters are high level parameters that does not change during the training procedure. For the Aeon models, I chosed the default hyperparameters provided by the library. But, for the computational vision model, while I set most of them to default, I did hyperparameter search for the learning rate hyperparameter, used by the optimizing algorithm. I did that seach by applying the Optuna\footnote{Documentation avaliable at \url{https://optuna.readthedocs.io/en/stable/}} python library, which does heuristic search over the set of all parameters requested dynamically in the user code. That libray prunes ramifications of the search-space tree with a variety of methods, of which I chose the median prunning. This functionallity allowed me to find a near-optimal combination of parameters without testing cases exhaustively, using as heuristic the score of the model in the validation dataset.

\subsection{The training strategy}

Given the before-mentioned models, the dataset and its divisions, I need to estabilish the training method for the models parameters adjustment. Since the Aeon implementation already contained a default training procedure, I only estabilished the fitting framework for the pytorch computational vision models and the data feeding method. The Pytorch library implements a multithreading data feeding solution named Data Loader. I configured it to load in batches of size 32. However, before constructing those batches, I applied random over sampling, since, as already mentioned, the dataset was unbalanced. As for the pytorch models fitting, I used the Adam optimizing algorithm \cite{Adam} to minimize the Cross Entropy loss function. My implementation of the training startegy did this optimization cycle with an ammount of epochs depending on an median-based early stop technique. The formula bellow gives the score of the n-th epoch:

\begin{equation}
\Mathenize{Early-Stop-Score}(n) = M_{i=0}^{9}(|l_{n-i} - M_{i=0}^{9}(l_{n-i})|)
\end{equation}

\noindent Where $l_k$ is the loss value (the Cross Entropy Loss) of the k-th epoch and $M_{i=0}^k(f(i))$ is the median of the values of $f(i)$ that result from the variation of $i$ from $0$ to $k$. In other words, the formula calculates the median absolute deviation of the last 10 losses values using the median as the central value. If $\Mathenize{Early-Stop-Score(n)} \leq 0.1$, the training stops in the n-th epoch. With that estabilished, I am only left to determine the metrics to be measured.     

\subsection{The measurements}

Finally, I chose the metrics, used to evaluate objectivelly efficacy of the sollution. I separated the metrics that I used into two groups: prediction metrics, which measures the quality of the models signal quality assessments; and benchmarking metrics, which measures the resources usage and the model speed. As the prediction metrics, I used the Cohen kappa score, the f1-score, and the precision. The Cohen kappa score, in binary classification tasks, measures the relation between the obtained accuracy $acc_o$ and the expected accuracy $acc_e$. The following equations define those accuracies and the Cohen Kappa score, in terms of confusion matrix cell values:

\begin{equation} 
acc_o(R) = \frac{TP+TN}{N}
\end{equation}

\begin{equation}
acc_e(R)  = \left(\frac{TP+FP}{N} \cdot \frac{TP+FN}{N}\right) + \left(\frac{TN+FP}{N} \cdot \frac{TN+FN}{N}\right)
\end{equation}

\begin{equation} \label{eq:Cohen Kappa}
\Mathenize{Cohen-Kappa(R)}  = \frac{acc_o(R) - acc_e(R)}{1 - acc_e(R)} 
\end{equation}  

\noindent Where N is the total number of samples and R is the set of pairs $\{(f(X),y') | \forall (X,y') \in Dataset\}$, where $f$ is the predictor. For the purpose of aligning this metric with others, I reescaled that metric from $[-1,1]$ to $[0,1]$:

\begin{equation}
\Mathenize{Cohen-Kappa-Rescaled(R)} = \frac{\Mathenize{Cohen-Kappa(R)}+1}{2} 
\end{equation}  

In sequence, the Precision is a metric that measures the ratio of hits in the set of positive predictions. In our context, a higher Precision imply that the predictor aproved a low ammount of ``Bad'' signals, which is desireable in applications where we do not want to show to the user wrong results. From the Precision and from the Recall, the ratio of hits in the set of all existing positives, we can obtain the F1-Score. Precisely, the F1-Score is the harmonic mean between those two metrics. In other words, a high F1-Score indicates a good balance between Precision and Recall scores. In our application, it measures the same as the Precision plus the Recall, which would measure the ammount of ``Good'' signals that would feed the application. This is an desireable quality when we want to provide constant feedback to the user. The following equations define those metrics:

\begin{equation} \label{eq:Precision}
Precision = \frac{TP}{TP+FP}
\end{equation}


\begin{equation} \label{eq:Recall}
Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation} \label{eq:F1-Score}
\Mathenize{F1-Score}  = \Mathenize{Harmonic-Mean}(Precision,Recall) = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
\end{equation}

\noindent Therefore, the Cohen Kappa provides an overall sense of accuracy, the F1-Score suggest the usability level of the model and the Precision indicates the security of the predictor. 

Regarding the benchmarking metrics, I measured the memory usage of the model in bytes and the inference time (added to the 1d-to-2d projection time for the projection-based models) in seconds. The memory measurement is important since practical applications of heart rate estimations often imposes hardware constraints that limits the allowed memory occupation. Additionally, the inference time, even though less important than the antecedent metric, is desireable for an almost-instant evaluation of the result, making the application more responsive.

\subsection{The overall schema}

\input{tex/figuras/framework}

The Figure \ref{fig:framework} expresses how I did the experiment for the \acrshort{CV} models. A first observation is that the main difference from the framework applied to non-\acrshort{CV} models is that the 1d to 2d conversion acts as a frontier between the dataset and the rest of the components. That means the non-\acrshort{CV} models experiment can be expressed using almost the same schema by removing that conversion block. With that, the experiment began by the Hyperparameters Selecion, splitting the \acrshort{BUTPPG} dataset using simple division for the purpose of, subsequentially, selecting the best hyperparameters of the \acrshort{CV}. With the best hyperparameters chosen, all models, including the non \acrshort{CV} models, will be evaluated using the \acrshort{LOSO} strategy. For each fold, I subjected the model to a training procedure that repeats epocs of training-and-validating until the early stopper interferes. Then, the model is tested, producing the metrics for that fold.


\section{Experimental results}


In this section, I analyze the results will by comparing the metrics in the following order: firstly, I verify if the reescaled Cohen Kappa Score value is superior o equal to 90\%. Secondly, the F1-Score; and, lastly, the Precision score. Moreover, I considered the trade-off between models with respect to the memory consumption and the inference time, in that order. Since I considered a large set of models, I did the comparisions in sections. In each section, I consider one of the seven before-presented computer vision model classifications: Transformers, Residual Nets, Purely Convolutional, Mobile Oriented, the RegNet, Extreme Nets and Diverse. In each of those sections, I selected the best combinatons of model variant and projection method, if at least one of them is sufficiently good. Subsequently, I will choose the seven best non-\acrshort{CV} models present in the Aeon toolkit. Lastly, I will determine the final best choices, also discussing the differences between the projection methods.

\subsection{Categories analysis}

\subsubsection{Transformers}

I tested four transformers the Vision Transformer, the Multi-axis Vision Transformer, the Shifted Windows Transformer and its second version. The Vision Transformer is the base model of the others. It transforms the visual input into into a word where the letters are linear embedings of subimages obtained by particioning the original image in a grid-like shape. Then, the other modules increments the original by using multiple layers and by changing the attention mechanisms. For instance, the MaxViT employs achitectural blocks where each alternates between two self-atention modes: grid attention, a mode of high granularity, and block attention, a mode of low granularity. The Swin Transformer, in the other hand, changes its attention in layer-level, by merging patches of the antecedent layer, and in block-level, by shifting the self-attention windows into different positions. Finally, the Swin Transformer V2 propose several specific improvements to the older version.

About the results, we can see on the tables \ref{tab:Averages_of_VisionTransformer}, \ref{tab:Averages_of_Maxvit}, \ref{tab:Averages_of_SwinTransformer}, and \ref{tab:Averages_of_SwinTransformer V2} that only the Swin Transformer V2 and the Vision Transformer variants obtained a Cohen Kappa score superior than 90\%. Specifically, for the Swin Transformer V2, only the standard Swin Transformer V2 variant combined with the \acrshort{Mix} method achieved such a score. For the Vision Transformer, only the large variants with patch size $16\times 16$ combined with \acrshort{Mix}, and with patch size $32 \times 32$ combined with \acrshort{Mix} and \acrshort{RP} suprassed the limit. Since both of the models reached a similar score, we observe the benchmarking metrics. We see in the Table \ref{tab:Memory_of_Transformers} that the Swin Transformer V2 uses considerably less memory than the Vision Transformer, with the second being at least 3x bigger than the first. On the other hand, we observe in Figure \ref{fig:Time_of_Transformers} that most of the Swin Transformer V2 variants have a slower inference speed if compared to the Vision Transformer variants, having as the only exception the Swin Transformer V2 tiny variant. However, because I consider the memory consumption as a factor of major importance, I selected for this section the Swin Transformer V2 standard variant with \acrshort{Mix}.

\input{tex/tabelas/resultados/averages/Averages_of_Transformers}

\IncludeMemoryTable{Transformers}{Transformers}

\input{tex/figuras/resultados/boxplots/Transformers}

\FloatBarrier

\subsubsection{Residual Nets}

This section analyzes the ResNet itself and its variations. The ResNet is a model that introduced the residual conections, links between non-adjacent layers that bypasses intermediate layers. As its variations, I analyzed two: the Wide ResNet and the ResNeXt. The first one widens the original net by increasing the number of channels per block, with the intention of providing an alternative to increasing the layer depth. The second one employs a multipath philosophy, agreggating the paths by an additive operation. As oposed to the antecedent, it avoids increasing the width and the depth of the network by providing an additional dimension to increase. 

Of those models, only the original ResNet and the Wide ResNet achieved a suficient Cohen Kappa score, as we can see in the tables \ref{tab:Averages_of_ResNet}, \ref{tab:Averages_of_ResNeXt} and \ref{tab:Averages_of_Wide ResNet}. From the ResNet variants, the ones with 50 and 101 layers reached that score. Solely the Wide ResNet with 101 layers and 2 convolutions per block sucessfully score above the limit. Those models variants got those scores by employing the \acrshort{Mix} projection method. Comparing these last variants, we conclude that the Wide ResNet 101-2 variant with the \acrshort{Mix} method obtained the best score, both for Cohen Kappa and F1 scores. However, that Wide ResNet posesses the largest memory occupation, according to the table \ref{tab:Memory_of_ResNet based}. It also spends the fourth higher inference time, as seen in the figure \ref{fig:Time_of_ResNet based}. Despite those disavantages, the Wide ResNet 101-2 variant with the \acrshort{Mix} was the choice of this section, because I prioritize the classification metrics over the benchmarking ones.

\input{tex/tabelas/resultados/averages/Averages_of_ResNet based}

\IncludeMemoryTable{ResNet based}{Residual Nets}

\input{tex/figuras/resultados/boxplots/ResNet based}

\FloatBarrier

\subsubsection{Mobile Oriented}

This section explores the mobile oriented networks, that is, networks designed specifically for mobile hardware constraints. I tested three models: the MNASNet, the Mobile Net V2 and the Mobile Net V3. In cronologial order, the Mobile Net V2 comes first. This newtork proposes several achitectural changes to use less memory while mantaining accuracy, with inverted residual blocks being one of them. This proposed alteration swaps places of the high-channeled and low-channeled layers, which causes the connection between layers with a lower ammount of channels, reducing the number of parameters in that block. After the antecedent, researchers introduced the MNASNet. Its name stands for Mobile Neural Architecture Search. It select blocks to fit in a predefined achitectural skeleton, optimizing the model performance on real-world mobile hardware. Finally, the Mobile Net V3 combined both approaches, while also made changes such as adding the NetAdapt \cite{NetAdapt} algorithm in the achitectural search.

In those set of models, only the MNASNet obtained a sufficient Cohen Kappa score, as visible in the tables \ref{tab:Averages_of_MNASNet}, \ref{tab:Averages_of_MobileNet V2} e \ref{tab:Averages_of_MobileNet V3}. Specifically, the MNASNet with depth multiplier of 1.0, being that multiplier related to the number of channels dimension. It achieved that score combined with the \acrshort{Mix} projection method method. According to the table \ref{tab:Memory_of_Mobile nets} and the figure \ref{fig:Time_of_Mobile nets}, all of those networks have very small memory size, with values inferior to 30 MB. They also have very fast inference time, with medians bellow to 20 ms. For those reasons and since it was the only net to achieve the minimum accuracy requirements, I chose the MNASNet 1.0 with \acrshort{Mix}. 

\input{tex/tabelas/resultados/averages/Averages_of_Mobile nets}

\IncludeMemoryTable{Mobile nets}{Mobile Oriented}

\input{tex/figuras/resultados/boxplots/Mobile nets}

\FloatBarrier

\subsubsection{Extreme Nets}

In this section, I discuss the results of neural models which focus on one concept and push it to the limit, such as the VGG, the DenseNet and the SqueezeNet. The VGG is a model that uses filters of size $3\times 3$ to allow adding more layers, increasing the depth model. On the other hand, the DenseNet applies skipping connections of the residual nets to all pairs of achitectural blocks present in the network, to achieve the benefits of having each layer closer to the input and the output of the model. Finally, the SqueezeNet tries to reach minimum memory usage not only by applying model compression techiniques but by introducing a new achitectural module which reduces the number of channels of a layer before another one with large convolutions filters, such as $3 \times 3$ filters. 

Accordingly to the tables \ref{tab:Averages_of_DenseNet}, \ref{tab:Averages_of_VGG} and \ref{tab:Averages_of_AlexNet}, only the SqueezeNet and the VGG achieved the minimum Cohen Kappa score. For the Squeezenet, only the version 1.1, a more economic version if compared to the 1.0, reached the score, when combined to the \acrshort{RP} and the \acrshort{Mix}, with the \acrshort{Mix} giving the best score. The second was the VGG with 16 weight layers without batch normalization, when combined to the \acrshort{RP} projection method. Of those combinations, the SqueezeNet 1.1 with \acrshort{Mix} performed better than the others, while having less variance. Adding to that performance, the SqueezeNet 1.1 is the smalest model in memory, according to the table \ref{tab:Memory_of_Extreme models}. Furthermore, the SqueezeNets have low inference time if copared to the other models, as the Figure \ref{fig:Time_of_Extreme nets} depicts. Therefore, I selected for this section the SqueezeNet 1.1 with \acrshort{Mix}.

\input{tex/tabelas/resultados/averages/Averages_of_Extreme models}

\IncludeMemoryTable{Extreme models}{Extreme Nets}

\input{tex/figuras/resultados/boxplots/Extreme nets}

\FloatBarrier

\subsubsection{Efficiency Oriented}

In this setion I discuss the models that researchers designed to efficiently utilize resources. For this work, I elaborate on the ShuffleNet V2, EfficientNet and EfficientNet V2. The first one is the ShuffleNet V2, sucessor of the ShuffleNet, which introduced the channel shuffle operator to allow information exchange among channels. It changes its predecessor by proposing, for each block, a channel split operation, to avoid costly operators such as grouped convolutions. Beside the shuffle nets, we have the efficient nets. The EfficientNet researchers studied the model scaling and created a compound resizing method that proportionally increase multiple dimensions, such as depth, number of channels and resolution. This allowed to create a very efficient base model that the researchers scaled up to bigger variants that mantain the advantages of the original. Finnaly, the EfficientNet V2 reformulated the antecessor by proposing a non-proportional scaling and by employing network architectural search. Additionally, it employed progressive learning by gradually increasing the dataset regularization.  

We observe in the tables \ref{tab:Averages_of_ShuffleNet V2}, \ref{tab:Averages_of_EfficientNet}, and \ref{tab:Averages_of_EfficientNetV2} that none of the EfficientNet variants accomplished the minimum Cohen Kappa Score. Only a few variants of the other models remained. For the ShuffleNet V2, we see that the variants with $\times 1$ and $\times 0.5$ channels were the remainders, when combined with the \acrshort{Mix} method. For the EfficientNet V2, the unique variant that I tested, we observe that only the \acrshort{Mix} method allowed the minimum score. The scores that the ShuffleNet V2 variants reached are similar to the ones of the EfficientNet V2. However, the ShuffleNet V2 variants are smaller than the EfficientNet V2, as we can visualize in the Table \ref{tab:Memory_of_Efficiency Oriented}. Furthermore, the Figure \ref{fig:Time_of_Efficiency Oriented} shows that the ShuffleNet V2 variants are generally faster than the EfficientNet V2. For those reasons, I chose the smallest variant, ShuffleNet V2 $\times 0.5$, combined with the \acrshort{Mix} projection method. 

\input{tex/tabelas/resultados/averages/Averages_of_Efficiency Oriented}

\input{tex/figuras/resultados/boxplots/Efficiency Oriented}

\IncludeMemoryTable{Efficiency Oriented}{Efficiency Oriented}

\FloatBarrier



\subsubsection{Diverse}

In this section, I analyze the remaining models. They are: AlexNet, ConvNeXt and RegNet. Researchers introduced the first one in the old year of 2012. That net tested the idea of deep learning trained on multiple GPUs, allowing a faster training. Furthermore, it applyied dropout to reduce overfitting. In contrast, the ConvNeXt is a modern model of 2022, which reunites several convolutional techiniques of the past years, such as patchfied convolutions, inverted bottlenecks and grouped convolutions. The researchers proposed that model to push the state of traditional convolutional networks to the limit. Differently of the antecedents, the RegNet employs the ideia of designing network spaces rather than individual nets. Those populations are caracterized by the restriction of the parameter space to a linear function. This created populations of networks that are more apropriate for random achitectural search.  

%In this section, the AlexNet, a classic model, and the ConvNeXt, a mordenized pure convolutional network, are present. As seen in tables \ref{tab:Averages_of_AlexNet} and \ref{tab:Averages_of_ConvNeXt}, only the AlexNet combined with the \acrshort{Mix} projection method obtained a CohenKappa score superior than 90\%, while achieving a very fast inference speed, with less than 20 ms latency, and comparable to the ConvNeXt memory occupation. Therefore, such combination was selected.  

%In this section, I inspect the set of variants of the RegNet networks. They are networks selected from a population of networks in a design space, that is, a space of networks resulting from a limited set of hyperparameters. Regarding those nets, we can look at the table \ref{tab:Averages_of_RegNet} to notice that, of those variants, only 4 got a sufficient Cohen Kappa score: from the RegNet X design space, the 800 MF and 3.2 GF variants; and from the RegNet Y design space, the 400 MF and 800 MF variants. All of those variants obtained such a score by applying the \acrshort{Mix} method, except for the Reg Net Y 400 MF variant, that was combined with the \acrshort{RP} approach. Since all of those variants obtained a similar score in all metrics, I considered the memory usage to select the RegNet Y with $400 \cdot 10^6$  FLOPs. In fact, this model is the smallest in memory occupation, using, according to the table \ref{tab:Memory_of_RegNet}, only 15 MB. A disavantage of this variant is that the RegNet Y type have, in general, a slower inference speed if compared to the RegNet X counterparts, as sighly noticeable in the figure \ref{fig:Time_of_RegNet}. 

%Finally, this section discusses the remaining computational vision models, the ShuffleNet V2, which uses an operator that shuffles the channels of certain input layer to increase accuracy by comunicating distinct channels; the SqueezeNet, which applies a squeeze operation that outputs a layer with a lesser number of channels to avoid linking that layer to 3x3 convolutional filters of the later layer; and the Efficient Net, which proposes a model scaling thechnique that conserve the proportion between different hyperparameter dimensions, and its second version, Efficient Net V2, that improves the first version by using network architectural search and changes the scaling method. As exhibited in the tables \ref{tab:Averages_of_ShuffleNet V2}, \ref{tab:Averages_of_SqueezeNet}, \ref{tab:Averages_of_EfficientNet} and \ref{tab:Averages_of_EfficientNetV2}, only the Efficient Net model didn't obtained sufficient scores for the Cohen Kappa metric. As for the other models, only the following variants achieved such a requirement, with similar scores: SuffleNet V2 with $\times 0.5$ and $\times 1.0$ channels; SqueezeNet v1.1; and the EfficientNet V2. All of those variants were capable of reaching the score by using the \acrshort{Mix} projection method. Since there are multiple options, it's observed in the table \ref{tab:Memory_of_Diverse} that the SqueezeNet achitecture is the smallest and it's noticed in the figure \ref{fig:Time_of_Diverse} that such a model is the fastest as well. For this reason, the SqueezeNet v1.1 variant in conjunction with the \acrshort{Mix} method is chosen as this category representative.

\input{tex/tabelas/resultados/averages/Averages_of_Diverse}

\input{tex/figuras/resultados/boxplots/Diverse}

\IncludeMemoryTable{Diverse}{Diverse}

\FloatBarrier

\subsection{Non-CV models comparison}

This section evaluate non-computationally-visual models. 

It is noticeable from the table \ref{tab:Memory_of_Non CV} and the figure \ref{fig:Time_of_Non CV} that the Non \acrshort{CV} models, in general, if compared with the \acrshort{CV} models are both very fast in inferece and small in memory occupation, with most of the models achieving less than 10 ms and than 10 MB in those aspects, respectively. Of those models, observing the table \ref{tab:Averages_of_Non CV}, it can be concluded that, by the metric Cohen Kappa score and the F1 score, the best models are the Random Interval Spectral Ensemble Classifier and the Temporal Dictionary Ensemble. Those models are competitive in memory size if compared to the other models, but the Temporal Dicionary Ensemble is slow in inference speed in the context of Non-\acrshort{CV} models, even thoght it is fast if juxtaposed with the \acrshort{CV} models. For this reason, the Random Interval Spectral Ensemble Classifier was selected as the best model of this section.

\input{tex/tabelas/resultados/averages/Averages_of_Non CV}

\IncludeMemoryTable{Non CV}{Non-CV}

\input{tex/figuras/resultados/boxplots/Non CV}

\FloatBarrier

\subsection{Best models comparison}

%This section aggregates all choices made in the past sections. In terms of the Cohen Kappa score, it can be observed in the table \ref{tab:Averages_of_Selected Models} that most of the models achieved a similiar score, with exception of the VGG 16 with \acrshort{RP}, which was the worse in performance for such a metric; and the Wide ResNet 101-2, which was the best model not only for this metric but also for the F1-Score metric, while mantaining the lowest standard deviation. However, that last model is the worst in inference speed, according to the figure \ref{fig:Time_of_Selected Models}. If it's considered only the models with less than 20 ms latency in inference, the SqueezeNet 1.1 is the best option, since, even though the score is not any better than the other models, it's the smallest in memory size, as seen in table \ref{tab:Memory_of_Selected Models}, and the second best in inference speed. Therefore, if we only consider the rate of correct predictions, the Wide ResNet 101-2 was the best, while if we consider the trade-off in speed and in memory consumption, the SqueezeNet 1.1 performed better.

%Moreover, besides recognizing the best models, it's possible to verify that two of the projection methods applied in this experiment, were dominant: the \acrshort{RP} and the \acrshort{Mix}. Of those methods, the \acrshort{Mix} was the one that allowed the Wide ResNet 101-2 and the SqueezeNet 1.1 to be preferable choices above the others.	

%Finally, it's compared in this section the best models that were selected from the past sections. Primally, it can be seen in the table \ref{tab:Averages_of_Best Models} that the Wide ResNet with \acrshort{Mix} was the best in all metrics except Precision, both in mean and in standard deviation. However, as confirmed in the table \ref{tab:Memory_of_Best Models}, that same model is, by far, the biggest, being more than $200\times$ bigger than every other model of this section. Also, that model is the slowest according to the figure \ref{fig:Time_of_Best Models}. When considering the other models, the Non \acrshort{CV} model, Random Interval Spectral Ensemble Classifier, not only performed better than the SqueezeNet in the classification metrics, but also is faster and smaller. From this analysis, it's infered that the Wide ResNet with \acrshort{Mix} was the best in classification and the Random Interval Spectral Ensemble Classifier was the most compact and faster when compared with the best models, which summarizes the advantages and the disavantages of each phylosophy: the projection-and-\acrshort{CV} approach requires more memory usage and inference time to perform a better classification performance, while the non-\acrshort{CV} approach don't have those disadvantages, but it did't achieved the same score in the predictions.

\input{tex/tabelas/resultados/averages/Averages_of_Best Models}

\IncludeMemoryTable{Best Models}{Best Models}

\input{tex/figuras/resultados/boxplots/Best Models}

\section{Limitations}
