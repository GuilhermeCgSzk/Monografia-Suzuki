%The tests were performed using the \gls{BUTPPG}~\cite{butppg}. In such a database, it was utilized a smartphone to record 48 \acrshort{ppg} signals of 12 \acrlong{sbjs} (\acrshort{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. This database can be consumed through Physionet~\cite{physionet} interface via \texttt{wfdb} package. The proposed method was implemented in Python. Implementations of \acrshort{gaf}, \acrshort{mtf} and \acrshort{rp} were provided by PyTS \cite{pyts} library. The PyTorch library~\cite{pytorch} was used to perform the training and the classification operations. The models used in this study are listed in Table~\ref{tab:results} and provided by TorchVision. Hyper-parameters optimization was performed using the Optuna library~\cite{optuna}. For each model, 50 Optuna trials were performed for fitting and validating the \acrshort{ml} model with median pruner to avoid excessive computation of trial epochs that do not show hope of better results.

%After the optimization, the metrics of the best trial were evaluated in the testing dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \acrshort{ml} model in hands, the dataset was divided into folds using the cross-validation \acrfull{loso} re-sampling method, into pieces matching each of the 12 \acrshort{sbjs}. For each fold, the smaller split was used as the testing dataset for the evaluation of the model's metrics, while the bigger split was subdivided into the training dataset, of size 7 \acrshort{sbjs}; and into the validation dataset, of size 4 \acrshort{sbjs}. Applying such an experimental setup allowed the generation of results concerning the metrics present in Table~\ref{tab:results}, where, for each projection method, all models were seen as samples of a statistics population possessing 5 values corresponding to the mean of all 12 folds of each metric.

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \acrshort{sbjs}; validation, with 2 \acrshort{sbjs}; and test, with 3 \acrshort{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \acrshort{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%In order to evaluate those projection methods, the following metrics were considered accuracy score, the proportion of hits $Accuracy = \frac{(TP+TN)}{TP+TN+FP+FN}$; precision score, the proportion of correct positive guesses, $Precision = \frac{TP}{TP+FP}$; recall score, the proportion of found positive samples, $Recall = \frac{TP}{TP+FN}$; and F1 score, the harmonic mean between the precision score and the recall score, $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$. Additionally, it was measured the Cohen Kappa score, the degree of agreement of annotators for a classification problem, $CohenKappa = 1-\frac{1-p_o}{1-p_e}$, where $p_o$ is the proportion of observed concordance and $p_e$ is the probability of concordance between all annotators. In the supervised binary classification domain, only two annotators, corresponding to the predicted and the real labels, and two classes, corresponding to positive or negative, are considered, in a manner that the confusion matrix can be used directly to evaluate the score, by the formula $CohenKappa=\frac{2\cdot (TP\cdot TN - FP \cdot FN)}{(TP+FP)\cdot(FP+TN)+(TP+FN)\cdot(FN+TN)}$.

% Those metrics were evaluated for 70 \acrshort{ml} models implemented in \href{https://pytorch.org/}{PyTorch}, the ones listed on Table~ \ref{tab:results}
% For such networks, the same experiment framework was applied, highlighting three major process blocks: dataset construction, when the database was loaded and transformed; hyperparameters selection, when, for each \acrshort{ml} model, a search was done to try to find the set of its best hyperparameters for the built dataset; and projection metrics evaluation, when each model, in conjunction with its set of best hyper-parameters found, was tested, generating the metrics present in this article. %(the framework is shown in figure \ref{figure:framework})
% Also, hyper-parameters selection and projections evaluation apply training and testing cycles.  To clarify those processes, they will be described in detail in the following sections.


%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\linewidth]{imgs/framework2.pdf}
%    \caption{Flowchart representing the experimental framework. Gray elements are entities, while blue elements are processes.}
%    \label{figure:framework}
%\end{figure*}

% \subsection{Training and Testing}

% Machine learning tasks involve 2 main steps: training and testing. Such a format was applied several times on the hyper-parameters selection process, for testing certain sets of hyper-parameters; and the projections evaluation process, for evaluating the model and its set of hyper-parameters efficiency. That core task was done using \href{https://pytorch.org/}{PyTorch} python library, which automatically computes the gradients based on the user's implementation \cite{pytorch}. Also, it allows parallel GPU processing to be done, by applying tensor-based operations to batches of the dataset, which size used in this experiment was the whole dataset. 

% The training process parameters also involve an optimizer and and loss function. The optimizer algorithm to be used was Adam\cite{adam}, while the Cross-Entropy Loss function was used, %\cite{cross-entropy-loss} 
% both implemented in \href{https://pytorch.org/}{PyTorch}. Moreover, the training process needs to stop at some point, fact that was done using an early stopping approach, computing the median absolute deviation, deviation metric robust to outliers, %\cite{?}
% of a window of the last 10 loss function values on the validation dataset, stopping the training process when such windows converge to, at least, a deviation of value 1. When the training process stopped, the best result found was chosen.

% \subsection{Dataset construction}

% In order to provide data to train and test \acrshort{ml} models, the Brno University of Technology Smartphone PPG Database was used as a starting point. In such a database, it was utilized a smartphone to record 48 \acrshort{ppg} signals of 12 \acrlong{sbjs} (\acrshort{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. Additionally, other refinement procedures were done to perfect that database, such as cropping 20 seconds of the recording, leaving the middle 10 seconds intact\cite{butppg}. And, finally, the database is publicly available on Physionet through the link \url{https://physionet.org/content/butppg/1.0.0/}. 

% However, the database is in \acrfull{wfdb} format \cite{butppg}, one dimensional signal format in function of time, %\cite{wfdb}
% letting yet to be done the application of projection methods. For such purpose, PyTS was used, a Python package developed for time series classification, containing the desired projection algorithms, \acrshort{gaf}, \acrshort{mtf} and \acrshort{rp} \cite{pyts}, with its implementations of PyTS version 0.13.0 stored in \href{https://zenodo.org/}{Zenodo} repository \cite{pyts-v0.13.0}.
    
% With such a package, it was possible to produce a dataset containing all projections as 2D images with 3 channels with shape $3 \times siglen^2 \times siglen^2$, where $siglen$ is the signal length. For each projection, its matrix was repeated on every channel of the image, while the \gls{PM} filled every channel with each of the 3 mentioned projections. Finally, it was necessary to resize every image for each \acrshort{ml} model input shape, for such purpose that it was used \href{https://pytorch.org/}{PyTorch} resize transform with bi-linear interpolation. After those procedures, the dataset was ready for the following processes, as the hyper-parameters selection.

% \subsection{Hyper-parameters selection}

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \acrshort{sbjs}; validation, with 2 \acrshort{sbjs}; and test, with 3 \acrshort{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \acrshort{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%\subsection{Projection Metrics Evaluation}

%After the optimization, the metrics of the best trial were evaluated in the test dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \acrshort{ml} model in hands, the dataset was divided into folds using the cross-validation \acrfull{loso} re-sampling method, into pieces matching each of the 12 \acrshort{sbjs}. For each fold, the left split was used as the test dataset for the evaluation of the model's metrics, while the other split was subdivided into the train dataset, of size 7 \acrshort{sbjs}; and into the valid dataset, of size 4 \acrshort{sbjs}. Applying such an experimental setup allowed to generate results with respect to the before-mentioned metrics, where, for each projection method, all models are seen as samples of a statistics population possessing 5 values corresponding to the metrics.

%Figure~\ref{fig:boxplots} depicts presents the boxplot of Accuracy, F1-score, Precision, Recall, and Cohen's Kappa distributions for different \glspl{CV} models tested on the \gls{BUTPPG} database. From these graphs, we can notice that, for the recall score, all projections are very equated. Moreover, it is noticeable that \acrshort{rp} and the proposed \gls{PM} have improved performances when compared with other \gls{mtf} and \gls{gaf} projections, especially when we observe the minimum, maximum, and the first quartiles of distributions, presented in Table~\ref{tab:boxplots}. From this table, we can perceive, at first glance, that the \gls{rp} and the \gls{PM} are generally superior methods if compared with the other two. In fact, \acrshort{rp} and \gls{PM} first quartiles are greater than \acrshort{gaf} and \acrshort{mtf} third quartiles considering all metrics except Recall Score, which shows that 75\% of samples in \acrshort{rp} and \gls{PM} are generally greater than 75\% of samples in \acrshort{gaf} and \acrshort{mtf}. The detailed performance of the points in the distributions considered in Figure~\ref{fig:boxplots} is depicted per model in Table~\ref{tab:results}.

%\input{tables/results}

% Each of the 4 populations produced box plots shown in figure \ref{fig:boxplots}, allowing, at first glance, to see the Recurrence Plot and the \gls{PM} as generally superior methods if compared with the other two. In fact, \acrshort{rp} and P\gls{PM} first quartiles are greater than \acrshort{gaf} and \acrshort{mtf} third quartiles considering all metrics except Recall Score, what shows that 75\% of samples in \acrshort{rp} and \gls{PM} are generally greater than 75\% of samples in \acrshort{gaf} and \acrshort{mtf}.    %TODO: falar do Cohen Cappa Score 

\section{Experimental Setup}

This section discusses the experimental setup, analyzing elements used in the experiment, such as datasets, programming libraries and predictive models. Additionally, it clarifies metrics to be evaluated and approaches to measure them.  

\subsection{The dataset}

As for every machine learning task, we need a dataset to provide data to feed the predictive models for their parameters fitting, molding them to the domain of the specific task. In this work, the task of assessing the quality of the signal is cleary a supervised classification problem, that is, can be described as the problem of finding a function that best defines a predefined set of pairs of variables and label, $(X,y)$. In this scope, the pair corresponds to the signal itself mapped to its quality label, ``Good'' or ``Bad''. For the purpose of training the predictive methods and evaluating their ability to fit to the problem of classifying the quality of heartbeat time series, I employed the \acrshort{BUTPPG}~\cite{butppg} dataset.

\input{tex/figuras/samples}

The \acrfull{BUTPPG} is a publicly available database produced by the Department of Biomedical Engineering of the the Brno University of Technology. It contains samples of \acrshort{PPG} signals, its quality labels and its heat rate estimations. Those signals were extracted using a low-cost method: recording with the camera of a smartphone. To be more precise, they recorded the subject's index finger, covering the lens of the camera and its LED light. Then, they measured, for each video frame, the average of the intensities of the red channel of every image pixel, resulting in a time series of averages. Finally, they inverted the signal. The Figure \ref{fig:butppg_samples} shows examples of the results of such a sequence of procedures.  

They done this method of obtaining \acrshort{PPG} signals  48 times, ammount distributed equally between 12 subjects. That is, for each subject, they did 4 measurements. Moreover, they did those recordings in two possible situations: one which the subject sat down and stayed static, case which the quality label ``Good'' was probable; and other in which the subject walked, hence, case likely to be a ``Bad'' recording. That distinction is relevant, since the walking situation occurred only 1 time for each subject, biasing the labels proportion to be nearly 25\% of ``Bad'' ones. Therefore, this dataset is imbalanced, factor that we need to handle in our experiment.

As for the definition of the signal quality labels, they designated specialists to estimate the heart rate associated with the \acrshort{PPG} signals, with the help of a software specialized for the analysis. Then, the organizers compared the number they gave with the one given by a gold standard method that, instead of using the \acrshort{PPG} signal, used an ECG recording. The measurer syncronized the ECG mannually. If the specialist measured with an error less or equal than 5 bpm, then the organizers considered the estimative correct. Finally, if 3 specialists of 5 gave correct estimations, the organizers considered the quality of the \acrshort{PPG} signal as correct. Thus, the dataset ``Good'' labels, in essence, identify if a signal is human-readable. 

\subsection{The dataset split}

Machine learning tasks also requires the separation the dataset into fragments. One of them is the training dataset, used for the models parameters adjustment. Another is the test dataset, used for evaluating the models efficiency. An optional one is the validation dataset, used to choose the set of best hyperparameters of a trained models. In this experiment case, to define the training-test splits, I used a cross validation method named \acrfull{LOSO}, which partitions the dataset into k pieces and k train-test splits. It makes the i-th train-test split assigning the i-th piece as the test dataset, lefting the other k-1 pieces as the training dataset. In the \acrshort{BUTPPG} case, the k value is 12, the number of subjects. Notice that the smaller unity of division is the subject, not the signals that are associated with it. I did it to increase the difference between training samples and testing samples. Since the dataset is small, such a split method allows the maximum provect of the avaliable resources, because it uses every sample in the dataset as a test object at least once, without biasing the results. Additionally, the training datset was divided producing a validation dataset of size 3, with usage that will be explained later.

\subsection{The models}

To evaluate the proposed projection-based framework and find particular overperforming cases, it is necessary to involve a big ammount of machine learning models. Firstly, I compared the projection-based framework with other existing approaches, by utilizing the Aeon-toolkit\footnote{Documentation avaliable at \url{https://www.aeon-toolkit.org/en/stable/}} python library, with models listed in table \ref{tab:non_cv_list}. Furthermore, I combined the proposed method with a wide variet of classification \acrshort{CV} models. For that, I used the Pytorch\footnote{Documentation avaliable at \url{https://pytorch.org/docs/stable/index.html}} python library, which aggregates a wide variety of neural network design strategies. Those strategies varies from simply convolutional networks to vision transformers. The table \ref{tab:cv_list} lists all \acrshort{CV} models involved in the experiment. With that, I submited the method to an abrangent set of scenarios.

\input{tex/tabelas/cv_listing}

\input{tex/tabelas/aeon_listing}

However, even though I defined the models, my setting lacks the choice of their hyperparameters. Those kind of parameters are high level parameters that does not change during the training procedure. For the Aeon models, I chosed the default hyperparameters provided by the library. But, for the computational vision model, while I set most of them to default, I did hyperparameter search for the learning rate hyperparameter, used by the optimizing algorithm. I did that seach by applying the Optuna\footnote{Documentation avaliable at \url{https://optuna.readthedocs.io/en/stable/}} python library, which does heuristic search over the set of all parameters requested dynamically in the user code. That libray prunes ramifications of the search-space tree with a variety of methods, of which I choosed the median prunning. This functionallity allowed me to find a near-optimal combination of parameters without testing cases exhaustively, using as heuristic the score of the model in the validation dataset.

\subsection{The training strategy}

Given the before-mentioned models, the dataset and its divisions, I need to estabilish the training method for the models parameters adjustment. Since the Aeon implementation already contained a default training procedure, I only estabilished the fitting framework for the pytorch computational vision models and the data feeding method. The Pytorch library implements a multithreading data feeding solution named Data Loader. I configured it to load in batches of size 32. However, before constructing those batches, I applied random over sampling, since, as already mentioned, the dataset was unbalanced. As for the pytorch models fitting, I used the Adam optimizing algorithm \cite{Adam} to minimize the Cross Entropy loss function. My implementation of the training startegy did this optimization cycle with an ammount of epochs depending on an median-based early stop technique. The formula bellow gives the score of the n-th epoch:

\begin{equation}
\Mathenize{Early-Stop-Score}(n) = M_{i=0}^{9}(|l_{n-i} - M_{i=0}^{9}(l_{n-i})|)
\end{equation}

\noindent Where $l_k$ is the loss value (the Cross Entropy Loss) of the k-th epoch and $M_{i=0}^k(f(i))$ is the median of the values of $f(i)$ that result from the variation of $i$ from $0$ to $k$. In other words, the formula calculates the median absolute deviation of the last 10 losses values using the median as the central value. If $\Mathenize{Early-Stop-Score(n)} \leq 0.1$, the training stops in the n-th epoch. With that estabilished, I am only left to determine the metrics to be measured.     

\subsection{The measurements}

Finally, I chose the metrics, used to evaluate objectivelly efficacy of the sollution. I separated the metrics that I used into two groups: prediction metrics, which measures the quality of the models signal quality assessments; and benchmarking metrics, which measures the resources usage and the model speed. As the prediction metrics, I used the Cohen kappa score, the f1-score, and the precision. The Cohen kappa score, in binary classification tasks, measures the relation between the obtained accuracy $acc_o$ and the expected accuracy $acc_e$. The following equations define those accuracies and the Cohen Kappa score, in terms of confusion matrix cell values:

\begin{equation} 
acc_o(R) = \frac{TP+TN}{N}
\end{equation}

\begin{equation}
acc_e(R)  = \left(\frac{TP+FP}{N} \cdot \frac{TP+FN}{N}\right) + \left(\frac{TN+FP}{N} \cdot \frac{TN+FN}{N}\right)
\end{equation}

\begin{equation} \label{eq:Cohen Kappa}
\Mathenize{Cohen-Kappa(R)}  = \frac{acc_o(R) - acc_e(R)}{1 - acc_e(R)} 
\end{equation}  

\noindent Where N is the total number of samples and R is the set of pairs $\{(f(X),y') | \forall (X,y') \in Dataset\}$, where $f$ is the predictor. For the purpose of aligning this metric with others, I reescaled that metric from $[-1,1]$ to $[0,1]$:

\begin{equation}
\Mathenize{Cohen-Kappa-Rescaled(R)} = \frac{\Mathenize{Cohen-Kappa(R)}+1}{2} 
\end{equation}  

In sequence, the Precision is a metric that measures the ratio of hits in the set of positive predictions. In our context, a higher Precision imply that the predictor aproved a low ammount of ``Bad'' signals, which is desireable in applications where we do not want to show to the user wrong results. From the Precision and from the Recall, the ratio of hits in the set of all existing positives, we can obtain the F1-Score. Precisely, the F1-Score is the harmonic mean between those two metrics. In other words, a high F1-Score indicates a good balance between Precision and Recall scores. In our application, it measures the same as the Precision plus the Recall, which would measure the ammount of ``Good'' signals that would feed the application. This is an desireable quality when we want to provide constant feedback to the user. Therefore, the Cohen Kappa provides an overall sense of accuracy, the F1-Score suggest the usability level of the model and the Precision indicates the security of the predictor. The following equations define those metrics:

\begin{equation} \label{eq:Precision}
Precision = \frac{TP}{TP+FP}
\end{equation}


\begin{equation} \label{eq:Recall}
Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation} \label{eq:F1-Score}
\Mathenize{F1-Score}  = \Mathenize{Harmonic-Mean}(Precision,Recall) = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
\end{equation}

Regarding the benchmarking metrics, I measured the memory usage of the model in bytes and the inference time (added to the 1d-to-2d projection time for the projection-based models) in seconds. The memory measurement is important since practical applications of heart rate estimations often imposes hardware constraints that limits the allowed memory occupation. Additionally, the inference time, even though less important than the antecedent metric, is desireable for an almost-instant evaluation of the result, making the application more responsive.

\subsection{The overall schema}

\input{tex/figuras/framework}

Reviewing what was presented, the experiment will begin by splitting the \acrshort{BUTPPG} dataset using simple division for the purpose of, subsequentially, selecting the best hyperparameters of the \acrshort{CV} models. With the best hyperparameters chosen, all models, including the non \acrshort{CV} models, will be evaluated using the \acrshort{LOSO} strategy, producing the metrics for each fold, as the Figure \ref{fig:framework} expresses.


\section{Experimental results}


In this section, the results will be analyzed by comparing the metrics in the following order: firstly, the Cohen Kappa Score, verifying if it's value is superior o equal to 90\%; then, the F1-Score; and, finally, the Precision score. Moreover, it will be considered the trade-off between models with respect to the inference time and the memory consumption. Since a large set of models was considered, the model comparision was done in sections, accordingly to the before-presented computer vision model classifications: Transformers, Residual nets, Parameter efficiency, Mobile nets and Diverse. In each of those sections, the best combinatons of model variant and projection method will be selected, if at least one of them is sufficiently good. Afterwards, the selections of each section will be compared, also discussing the differences between the projection methods. Lastly, the best combinations of models variants and projection methods will be compared to the best standard time series classification models present in the Aeon toolkit.

\pagebreak

\subsection{Categories analysis}

\subsubsection{Transformers}

Three transformers were considered: the Vision Transformer, the Multi-axis Vision Transformer and the Shifted Windows Transformer. Firsly, observing the tables \ref{tab:Averages_of_VisionTransformer} \ref{tab:Averages_of_SwinTransformer} \ref{tab:Averages_of_Maxvit}, it's noticeable that only some variants of the Vision Transformer obtained Cohen Kappa scores superior than 90\%, while the tiny Swin Transformer obtained almost 90\% in the same score. The highest Cohen Kappa scores and F1-scores were obtained by the large Vision Transformer variants for the \acrshort{RP} and \acrshort{Mix} projection methods, in contrast to the huge variant, that performed significantly worse for all projections. Specifically, the models Vision Transformer L 16, with \acrshort{RP} and \acrshort{Mix} and Vision Transformer L 32 with \acrshort{Mix} obtained the best scores, while having similar memory consumption and a sighly difference in inference time, with the 32x32 patch sized variant being the slowest, fact observed in the figure \ref{fig:Time_of_Transformers}. One drawback of the Vision Transformer models, if compared to the other transformers in this section, is that its memory consumption is drastically higher, as exposed in the table \ref{tab:Memory_of_Transformers}. Regardless, the Vision Transformer L 16 with \acrshort{RP} was selected as the best model of this section, since this model was, primally, one of those that achieved the highest score, and, if compared with the Vision Transformer L 32 and with the \acrshort{Mix} projection, was the fastest in inference speed.

\pagebreak

\input{tex/tabelas/resultados/averages/Averages_of_Transformers}

\IncludeMemoryTable{Transformers}

\input{tex/figuras/resultados/boxplots/Transformers}

\pagebreak

\subsubsection{ResNet based}

This section contains the ResNet itself and two derivated models, the Wide ResNet, a model with more channels per block; and the ResNeXt, a model that, as oposed to the before-mentioned wide in channels model, employs an multipath philosophy, agreggating the paths by an additive operation. As it's clear in the table \ref{tab:Averages_of_ResNeXt}, every ResNeXt variant didn't obtained a sufficient Cohen Kappa score, which was the case for the Wide Res Net with 101 layers and 2 convolutions per block combined with the \acrshort{Mix} projection, as seen in \ref{tab:Averages_of_WideResNet}; and for the ResNets variants with 50 and 101 layers and the \acrshort{Mix} projection as well,  as seen in \ref{tab:Averages_of_ResNet}. Comparing these last variants, it can be concluded that the Wide ResNet 101-2 variant with the \acrshort{Mix} method obtained the best score, both for Cohen Kappa and F1 scores. However, that Wide ResNet posesses the largest memory occupation, according to the table \ref{tab:Memory_of_ResNet based}, and spends the fourth higher inference time, as seen in the figure \ref{fig:Time_of_ResNet based}.  Despite this disavantage, the Wide ResNet 101-2 variant with the \acrshort{Mix} was the choice of this section.

\pagebreak


\input{tex/tabelas/resultados/averages/Averages_of_ResNet based}

\IncludeMemoryTable{ResNet based}

\input{tex/figuras/resultados/boxplots/ResNet based}

\FloatBarrier

\subsubsection{Purely Convolutional}

In this section, the AlexNet, a classic model, and the ConvNeXt, a mordenized pure convolutional network, are present. As seen in tables \ref{tab:Averages_of_AlexNet} and \ref{tab:Averages_of_ConvNeXt}, only the AlexNet combined with the \acrshort{Mix} projection method obtained a CohenKappa score superior than 90\%, while achieving a very fast inference speed, with less than 20 ms latency, and comparable to the ConvNeXt memory occupation. Therefore, such combination was selected.  

\input{tex/tabelas/resultados/averages/Averages_of_Purely convolutional}

\input{tex/figuras/resultados/boxplots/Purely convolutional}

\IncludeMemoryTable{Purely convolutional}

\FloatBarrier

\subsubsection{Mobile Oriented}

This section contain the mobile oriented networks MNASNet, a mobile plataform aware seach algorithm; Mobile Net V2, which uses inverted residuals and linear bottlenecks to better fit the mobile context; and Mobile Net V3, which combines architectural search with various network improvements. Of those models, only the MNASNet obtained a sufficient Cohen Kappa score, as visible in the tables \ref{tab:Averages_of_MNASNet}, \ref{tab:Averages_of_MobileNet V2} e \ref{tab:Averages_of_MobileNet V3}. Specifically, the MNASNet with depth multiplier of 1.0, that is, with exaclty the same number of channels in each layer of the base model; combined with the \acrshort{Mix} projection method method. According to the table \ref{tab:Memory_of_Mobile nets} and the figure \ref{fig:Time_of_Mobile nets}, all of those networks have very small memory size, with values inferior to 30 MB; and very fast inference time, with medians bellow to 20 ms. For this reason, the MNASNet 1.0 with \acrshort{Mix} was chosen. 

\input{tex/tabelas/resultados/averages/Averages_of_Mobile nets}

\IncludeMemoryTable{Mobile nets}

\input{tex/figuras/resultados/boxplots/Mobile nets}

\FloatBarrier

\subsubsection{RegNet}

In this section, the set of variants of the RegNet networks are analyzed. RegNets are networks selected from a population of networks in a design space, that is, a space of networks resulting from a limited set of hyperparameters. By looking at the table \ref{tab:Averages_of_RegNet}, it can be seen that, of those variants, only 4 got a sufficient Cohen Kappa score: from the RegNet X design space, the 800 MF and 3.2 GF variants; and from the RegNet Y design space, the 400 MF and 800 MF variants. All of those variants obtained such a score by applying the \acrshort{Mix} method, except for the Reg Net Y 400 MF variant, that was combined with the \acrshort{RP} approach. Since all of those variants obtained a similar score in all metrics, the memory usage was considered to select the RegNet Y 400 MF variant, since this model is the smallest in memory occupation, using, according to the table \ref{tab:Memory_of_RegNet}, only 15 MB; even though the RegNet Y variants have, in general, a slower inference speed if compared to the RegNet X counterparts, as sighly noticeable in the figure \ref{fig:Time_of_RegNet}.

\input{tex/tabelas/resultados/averages/Averages_of_RegNet}

\IncludeMemoryTable{RegNet}

\input{tex/figuras/resultados/boxplots/RegNet}

\FloatBarrier

\subsubsection{Extreme Nets}

In this section, it's discussed the results of neural models which are an exageration of a concept, as the VGG, which is a model that, for its biggest variants, is a very deep model when considering its number of layers; and the DenseNet, which apply the skipping connections of the residual nets to all pairs of blocks in the network. Accordingly to the tables \ref{tab:Averages_of_DenseNet} and \ref{tab:Averages_of_VGG}, only one variant have succeded to barely achieve an over the quota Cohen Kappa score: the VGG with 16 blocks without Batch Normalization, when combined with the \acrshort{RP} method. Additionally, as presented in the figure \ref{fig:Time_of_Extreme nets}, the VGGs have a considerable lower inference speed, with medians that have values near 20 ms. In contrast, the VGGs are not small-in-memory models, with occupation suprassing 500 MB, as seen in table \ref{tab:Memory_of_Extreme nets}, while the biggest DenseNet occupies 105 MB. Nonetheless, the VGG 16 variant combined with the \acrshort{RP} projection method is selected as the representative model of this section.

\input{tex/tabelas/resultados/averages/Averages_of_Extreme models}

\IncludeMemoryTable{Extreme models}

\input{tex/figuras/resultados/boxplots/Extreme nets}

\FloatBarrier

\subsubsection{Diverse}

Finally, this section discusses the remaining computational vision models, the ShuffleNet V2, which uses an operator that shuffles the channels of certain input layer to increase accuracy by comunicating distinct channels; the SqueezeNet, which applies a squeeze operation that outputs a layer with a lesser number of channels to avoid linking that layer to 3x3 convolutional filters of the later layer; and the Efficient Net, which proposes a model scaling thechnique that conserve the proportion between different hyperparameter dimensions, and its second version, Efficient Net V2, that improves the first version by using network architectural search and changes the scaling method. As exhibited in the tables \ref{tab:Averages_of_ShuffleNet V2}, \ref{tab:Averages_of_SqueezeNet}, \ref{tab:Averages_of_EfficientNet} and \ref{tab:Averages_of_EfficientNetV2}, only the Efficient Net model didn't obtained sufficient scores for the Cohen Kappa metric. As for the other models, only the following variants achieved such a requirement, with similar scores: SuffleNet V2 with $\times 0.5$ and $\times 1.0$ channels; SqueezeNet v1.1; and the EfficientNet V2. All of those variants were capable of reaching the score by using the \acrshort{Mix} projection method. Since there are multiple options, it's observed in the table \ref{tab:Memory_of_Diverse} that the SqueezeNet achitecture is the smallest and it's noticed in the figure \ref{fig:Time_of_Diverse} that such a model is the fastest as well. For this reason, the SqueezeNet v1.1 variant in conjunction with the \acrshort{Mix} method is chosen as this category representative.

\input{tex/tabelas/resultados/averages/Averages_of_Diverse}

\input{tex/figuras/resultados/boxplots/Diverse}

\IncludeMemoryTable{Diverse}

\FloatBarrier

\subsection{Non-CV models comparison}

In this section the non computationally visual models are exposed and dissected. It's noticeable as well from the table \ref{tab:Memory_of_Non CV} and the figure \ref{fig:Time_of_Non CV} that the Non \acrshort{CV} models, in general, if compared with the \acrshort{CV} models are both very fast in inferece and small in memory occupation, with most of the models achieving less than 10 ms and than 10 MB in those aspects, respectively. Of those models, observing the table \ref{tab:Averages_of_Non CV}, it can be concluded that, by the metric Cohen Kappa score and the F1 score, the best models are the Random Interval Spectral Ensemble Classifier and the Temporal Dictionary Ensemble. Those models are competitive in memory size if compared to the other models, but the Temporal Dicionary Ensemble is slow in inference speed in the context of Non-\acrshort{CV} models, even thoght it is fast if juxtaposed with the \acrshort{CV} models. For this reason, the Random Interval Spectral Ensemble Classifier was selected as the best model of this section.

\input{tex/tabelas/resultados/averages/Averages_of_Non CV}

\IncludeMemoryTable{Non CV}

\input{tex/figuras/resultados/boxplots/Non CV}

\FloatBarrier

\subsection{Best models comparison}

%This section aggregates all choices made in the past sections. In terms of the Cohen Kappa score, it can be observed in the table \ref{tab:Averages_of_Selected Models} that most of the models achieved a similiar score, with exception of the VGG 16 with \acrshort{RP}, which was the worse in performance for such a metric; and the Wide ResNet 101-2, which was the best model not only for this metric but also for the F1-Score metric, while mantaining the lowest standard deviation. However, that last model is the worst in inference speed, according to the figure \ref{fig:Time_of_Selected Models}. If it's considered only the models with less than 20 ms latency in inference, the SqueezeNet 1.1 is the best option, since, even though the score is not any better than the other models, it's the smallest in memory size, as seen in table \ref{tab:Memory_of_Selected Models}, and the second best in inference speed. Therefore, if we only consider the rate of correct predictions, the Wide ResNet 101-2 was the best, while if we consider the trade-off in speed and in memory consumption, the SqueezeNet 1.1 performed better.

%Moreover, besides recognizing the best models, it's possible to verify that two of the projection methods applied in this experiment, were dominant: the \acrshort{RP} and the \acrshort{Mix}. Of those methods, the \acrshort{Mix} was the one that allowed the Wide ResNet 101-2 and the SqueezeNet 1.1 to be preferable choices above the others.	

Finally, it's compared in this section the best models that were selected from the past sections. Primally, it can be seen in the table \ref{tab:Averages_of_Best Models} that the Wide ResNet with \acrshort{Mix} was the best in all metrics except Precision, both in mean and in standard deviation. However, as confirmed in the table \ref{tab:Memory_of_Best Models}, that same model is, by far, the biggest, being more than $200\times$ bigger than every other model of this section. Also, that model is the slowest according to the figure \ref{fig:Time_of_Best Models}. When considering the other models, the Non \acrshort{CV} model, Random Interval Spectral Ensemble Classifier, not only performed better than the SqueezeNet in the classification metrics, but also is faster and smaller. From this analysis, it's infered that the Wide ResNet with \acrshort{Mix} was the best in classification and the Random Interval Spectral Ensemble Classifier was the most compact and faster when compared with the best models, which summarizes the advantages and the disavantages of each phylosophy: the projection-and-\acrshort{CV} approach requires more memory usage and inference time to perform a better classification performance, while the non-\acrshort{CV} approach don't have those disadvantages, but it did't achieved the same score in the predictions.

\input{tex/tabelas/resultados/averages/Averages_of_Best Models}

\IncludeMemoryTable{Best Models}

\input{tex/figuras/resultados/boxplots/Best Models}
