%The tests were performed using the \gls{BUTPPG}~\cite{butppg}. In such a database, it was utilized a smartphone to record 48 \gls{ppg} signals of 12 \gls{sbjs} (\gls{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. This database can be consumed through Physionet~\cite{physionet} interface via \texttt{wfdb} package. The proposed method was implemented in Python. Implementations of \gls{gaf}, \gls{mtf} and \gls{rp} were provided by PyTS \cite{pyts} library. The PyTorch library~\cite{pytorch} was used to perform the training and the classification operations. The models used in this study are listed in Table~\ref{tab:results} and provided by TorchVision. Hyper-parameters optimization was performed using the Optuna library~\cite{optuna}. For each model, 50 Optuna trials were performed for fitting and validating the \gls{ml} model with median pruner to avoid excessive computation of trial epochs that do not show hope of better results.

%After the optimization, the metrics of the best trial were evaluated in the testing dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \gls{ml} model in hands, the dataset was divided into folds using the cross-validation \gls{loso} re-sampling method, into pieces matching each of the 12 \gls{sbjs}. For each fold, the smaller split was used as the testing dataset for the evaluation of the model's metrics, while the bigger split was subdivided into the training dataset, of size 7 \gls{sbjs}; and into the validation dataset, of size 4 \gls{sbjs}. Applying such an experimental setup allowed the generation of results concerning the metrics present in Table~\ref{tab:results}, where, for each projection method, all models were seen as samples of a statistics population possessing 5 values corresponding to the mean of all 12 folds of each metric.

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \gls{sbjs}; validation, with 2 \gls{sbjs}; and test, with 3 \gls{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \gls{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%In order to evaluate those projection methods, the following metrics were considered accuracy score, the proportion of hits $Accuracy = \frac{(TP+TN)}{TP+TN+FP+FN}$; precision score, the proportion of correct positive guesses, $Precision = \frac{TP}{TP+FP}$; recall score, the proportion of found positive samples, $Recall = \frac{TP}{TP+FN}$; and F1 score, the harmonic mean between the precision score and the recall score, $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$. Additionally, it was measured the Cohen Kappa score, the degree of agreement of annotators for a classification problem, $CohenKappa = 1-\frac{1-p_o}{1-p_e}$, where $p_o$ is the proportion of observed concordance and $p_e$ is the probability of concordance between all annotators. In the supervised binary classification domain, only two annotators, corresponding to the predicted and the real labels, and two classes, corresponding to positive or negative, are considered, in a manner that the confusion matrix can be used directly to evaluate the score, by the formula $CohenKappa=\frac{2\cdot (TP\cdot TN - FP \cdot FN)}{(TP+FP)\cdot(FP+TN)+(TP+FN)\cdot(FN+TN)}$.

% Those metrics were evaluated for 70 \gls{ml} models implemented in \href{https://pytorch.org/}{PyTorch}, the ones listed on Table~ \ref{tab:results}
% For such networks, the same experiment framework was applied, highlighting three major process blocks: dataset construction, when the database was loaded and transformed; hyperparameters selection, when, for each \gls{ml} model, a search was done to try to find the set of its best hyperparameters for the built dataset; and projection metrics evaluation, when each model, in conjunction with its set of best hyper-parameters found, was tested, generating the metrics present in this article. %(the framework is shown in figure \ref{figure:framework})
% Also, hyper-parameters selection and projections evaluation apply training and testing cycles.  To clarify those processes, they will be described in detail in the following sections.


%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\linewidth]{imgs/framework2.pdf}
%    \caption{Flowchart representing the experimental framework. Gray elements are entities, while blue elements are processes.}
%    \label{figure:framework}
%\end{figure*}

% \subsection{Training and Testing}

% Machine learning tasks involve 2 main steps: training and testing. Such a format was applied several times on the hyper-parameters selection process, for testing certain sets of hyper-parameters; and the projections evaluation process, for evaluating the model and its set of hyper-parameters efficiency. That core task was done using \href{https://pytorch.org/}{PyTorch} python library, which automatically computes the gradients based on the user's implementation \cite{pytorch}. Also, it allows parallel GPU processing to be done, by applying tensor-based operations to batches of the dataset, which size used in this experiment was the whole dataset. 

% The training process parameters also involve an optimizer and and loss function. The optimizer algorithm to be used was Adam\cite{adam}, while the Cross-Entropy Loss function was used, %\cite{cross-entropy-loss} 
% both implemented in \href{https://pytorch.org/}{PyTorch}. Moreover, the training process needs to stop at some point, fact that was done using an early stopping approach, computing the median absolute deviation, deviation metric robust to outliers, %\cite{?}
% of a window of the last 10 loss function values on the validation dataset, stopping the training process when such windows converge to, at least, a deviation of value 1. When the training process stopped, the best result found was chosen.

% \subsection{Dataset construction}

% In order to provide data to train and test \gls{ml} models, the Brno University of Technology Smartphone PPG Database was used as a starting point. In such a database, it was utilized a smartphone to record 48 \gls{ppg} signals of 12 \gls{sbjs} (\gls{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. Additionally, other refinement procedures were done to perfect that database, such as cropping 20 seconds of the recording, leaving the middle 10 seconds intact\cite{butppg}. And, finally, the database is publicly available on Physionet through the link \url{https://physionet.org/content/butppg/1.0.0/}. 

% However, the database is in \gls{wfdb} format \cite{butppg}, one dimensional signal format in function of time, %\cite{wfdb}
% letting yet to be done the application of projection methods. For such purpose, PyTS was used, a Python package developed for time series classification, containing the desired projection algorithms, \gls{gaf}, \gls{mtf} and \gls{rp} \cite{pyts}, with its implementations of PyTS version 0.13.0 stored in \href{https://zenodo.org/}{Zenodo} repository \cite{pyts-v0.13.0}.
    
% With such a package, it was possible to produce a dataset containing all projections as 2D images with 3 channels with shape $3 \times siglen^2 \times siglen^2$, where $siglen$ is the signal length. For each projection, its matrix was repeated on every channel of the image, while the \gls{PM} filled every channel with each of the 3 mentioned projections. Finally, it was necessary to resize every image for each \gls{ml} model input shape, for such purpose that it was used \href{https://pytorch.org/}{PyTorch} resize transform with bi-linear interpolation. After those procedures, the dataset was ready for the following processes, as the hyper-parameters selection.

% \subsection{Hyper-parameters selection}

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \gls{sbjs}; validation, with 2 \gls{sbjs}; and test, with 3 \gls{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \gls{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%\subsection{Projection Metrics Evaluation}

%After the optimization, the metrics of the best trial were evaluated in the test dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \gls{ml} model in hands, the dataset was divided into folds using the cross-validation \gls{loso} re-sampling method, into pieces matching each of the 12 \gls{sbjs}. For each fold, the left split was used as the test dataset for the evaluation of the model's metrics, while the other split was subdivided into the train dataset, of size 7 \gls{sbjs}; and into the valid dataset, of size 4 \gls{sbjs}. Applying such an experimental setup allowed to generate results with respect to the before-mentioned metrics, where, for each projection method, all models are seen as samples of a statistics population possessing 5 values corresponding to the metrics.

%Figure~\ref{fig:boxplots} depicts presents the boxplot of Accuracy, F1-score, Precision, Recall, and Cohen's Kappa distributions for different \glspl{CV} models tested on the \gls{BUTPPG} database. From these graphs, we can notice that, for the recall score, all projections are very equated. Moreover, it is noticeable that \gls{rp} and the proposed \gls{PM} have improved performances when compared with other \gls{mtf} and \gls{gaf} projections, especially when we observe the minimum, maximum, and the first quartiles of distributions, presented in Table~\ref{tab:boxplots}. From this table, we can perceive, at first glance, that the \gls{rp} and the \gls{PM} are generally superior methods if compared with the other two. In fact, \gls{rp} and \gls{PM} first quartiles are greater than \gls{gaf} and \gls{mtf} third quartiles considering all metrics except Recall Score, which shows that 75\% of samples in \gls{rp} and \gls{PM} are generally greater than 75\% of samples in \gls{gaf} and \gls{mtf}. The detailed performance of the points in the distributions considered in Figure~\ref{fig:boxplots} is depicted per model in Table~\ref{tab:results}.

%\input{tables/results}

% Each of the 4 populations produced box plots shown in figure \ref{fig:boxplots}, allowing, at first glance, to see the Recurrence Plot and the \gls{PM} as generally superior methods if compared with the other two. In fact, \gls{rp} and P\gls{PM} first quartiles are greater than \gls{gaf} and \gls{mtf} third quartiles considering all metrics except Recall Score, what shows that 75\% of samples in \gls{rp} and \gls{PM} are generally greater than 75\% of samples in \gls{gaf} and \gls{mtf}.    %TODO: falar do Cohen Cappa Score 

\textcolor{red}{
This chapter first presents the experimental setup. Then, it exposes and discusses the experimental results. Finally, it investigates the experiments limitations. 
}

\section{Experimental Setup}

This section discusses the experimental setup, analyzing elements used in the experiments, such as datasets, programming libraries and predictive models. Additionally, it clarifies metrics to be evaluated and approaches to measure them.  

\subsection{The dataset}

As for every machine learning task, we need a dataset to provide data to feed the predictive models for their parameters fitting, molding them to the domain of the specific task. In this work, the task of assessing the quality of the signal is a supervised classification problem, that is, can be described as the problem of finding a function that best defines a predefined set of pairs of variables and label, $(X,y)$. In this scope, the pair corresponds to the signal itself mapped to its quality label, ``Good'' or ``Bad''. For the purpose of training the predictive methods and evaluating their ability to fit to the problem of classifying the quality of heartbeat time series, the \gls{BUTPPG}~\cite{butppg} dataset was employed.

%\input{tex/figuras/samples}

The \gls{BUTPPG} is a publicly available database produced by the Department of Biomedical Engineering of the the Brno University of Technology. It contains samples of \gls{PPG} signals, its quality labels and its heat rate estimations. Those signals were extracted using a low-cost method: recording with the camera of a smartphone. To be more precise, the researchers recorded the subject's index finger, covering the lens of the camera and its \gls{LED} light. Then, they measured, for each video frame, the average of the intensities of the red channel of every image pixel, resulting in a time series of averages. Finally, they inverted the signal. %The Figure \ref{fig:butppg_samples} shows examples of the results of such a sequence of procedures.  

They done this method of obtaining \gls{PPG} signals  48 times, amount distributed equally between 12 subjects. That is, for each subject, they did 4 measurements. Moreover, they did those recordings in two possible situations: one which the subject sat down and stayed static, case which the quality label ``Good'' was probable; and other in which the subject walked, hence, case likely to be a ``Bad'' recording. That distinction is relevant, since the walking situation occurred only 1 time for each subject, biasing the labels proportion to be nearly 25\% of ``Bad'' ones. Therefore, this dataset is imbalanced, factor that we need to handle in our experiment.

As for the definition of the signal quality labels, they designated specialists to estimate the heart rate associated with the \gls{PPG} signals, with the help of a software specialized for the analysis, made by the researchers. Then, the organizers compared the number they gave with the one given by a gold standard method that, instead of using the \gls{PPG} signal, used an ECG recording as reference. The measurer synchronized the ECG manually. If the specialist measured with an error less or equal than 5 bpm, then the organizers considered the estimative correct. Finally, if 3 specialists of 5 gave correct estimations, the organizers considered the quality of the \gls{PPG} signal as correct. Thus, the dataset ``Good'' labels, in essence, identify if a signal is human-readable. 

\subsection{The dataset split}

Machine learning tasks also requires the separation the dataset into fragments. One of them is the training dataset, used for the models parameters fitting. Another is the test dataset, used for evaluating the models efficiency. An optional one is the validation dataset, used to choose the set of best hyperparameters of a trained models. In our experiment case, to define the training-test splits, it was used a cross validation method named \gls{LOSO}, which partitions the dataset into $K$ train-test splits. It makes the $k$-th train-test split assigning the $k$-th piece as the test dataset, lefting the other $K-1$ pieces as the training dataset. In the \gls{BUTPPG} case, the $K$ value is 12, the number of subjects. Notice that the smaller unity of division is the subject, not the signals that are associated with it. It has the advantage of increasing the difference between training samples and testing samples. Since the dataset is small, such a split method allows the maximum benefit of the available resources, because it uses every sample in the dataset as a test object at least once, without biasing the results. Additionally, the training dataset was divided producing a validation dataset of size 3, with usage that will be explained later.

\subsection{The models}

To evaluate the proposed projection-based framework and find particular overperforming cases, it is necessary to involve a big amount of machine learning models. Firstly, this work compared the projection-based framework with other time series classification approaches, by utilizing the Aeon-toolkit python library \cite{AeonDoc}, with models listed in Table \ref{tab:non_cv_list}. Furthermore, this work combined the proposed method with a wide variety of classification \gls{CV} models. For that, it was used the Pytorch python library \cite{PytorchDoc}, which aggregates a wide variety of neural network design architectures. Those architectures varies from simply convolutional networks to vision transformers. The Table \ref{tab:cv_list} lists all \gls{CV} models involved in the experiment. With that, these experiments submitted this thesis framework to an abrangent set of scenarios.

\input{tex/tabelas/aeon_listing}

\input{tex/tabelas/cv_listing}

However, defining the models is insufficient, as it still needed the choice of their hyperparameters. Hyparameters are high level parameters that does not change during the training procedure. For the Aeon models, the default hyperparameters provided by the library were chosen. But, for the computational vision model, while most of them were set to default, our experiments employed hyperparameter search for the learning rate hyperparameter, used by the optimizing algorithm. The Optuna python library \cite{OptunaDoc} did that search, by doing heuristic search over the set of all parameters requested dynamically in the user code. That library prunes ramifications of the search-space tree with a variety of methods, of which our experiments used the median pruning. In our case, the score that guides this heuristic is the accuracy score, the ratio of hits over the number of samples. Our experiments measured it in the validation dataset of size 2, that resulted from a simple random split. This functionality allowed us to find a near-optimal combination of parameters without testing cases exhaustively, using as heuristic the score of the model in the validation dataset.

\subsection{The training strategy}

Given the before-mentioned models, the dataset and its divisions, it was needed to establish the training method for the models parameters adjustment. Since the Aeon implementation already contained a default training procedure, our experiment only established the fitting framework for the Pytorch computational vision models and the data feeding method. Our experiment fed the models by loading the signals data and, before transforming them, applied random over sampling, since the dataset was unbalanced. After doing the projection transform, our exeperiment loaded pre-trained model weights provided by the Pytorch. This learning transferring originated from training the models in the ImageNet \cite{ImageNet} dataset. Following that, our experiment did the Pytorch models fitting using the Adam optimizing algorithm \cite{Adam} to minimize the Cross Entropy Loss function. The implementation of the training strategy did this optimization cycle with an amount of epochs depending on an median-based early stop technique. The formula bellow gives the score of the $n$-th epoch:

\begin{equation}
EarlyStopScore(n) = med([|l_{n-i} - med([l_{n-i}]_{i=0}^9)|]_{i=0}^{9})
\end{equation}

\noindent Where $l_k$ is the loss value (the Cross Entropy Loss) of the $k$-th epoch, ${med}$ is the median and $[f(i)]_{i=0}^k$ is the sequence generated by $f(i)$ when varying $i$ from $0$ to $k$. In other words, the formula calculates the median of the absolute deviation of the medians of the last 10 losses values using the median as the central value. If $EarlyStopScore(n) \leq 0.1$, the training stops in the $n$-th epoch. With that established, it is only left to determine the metrics to be measured.     

\subsection{The measurements}

Finally, we chose the metrics to evaluate objectively efficacy of the solution. For these experiments, we can separate the used metrics into two groups: prediction metrics, which measures the quality of the models signal quality assessments; and benchmarking metrics, which measures the resources usage and the model speed. As the prediction metrics, our experiments used the Cohen kappa score, the F1-score, and the precision. The Cohen kappa score, in binary classification tasks, measures the relation between the obtained accuracy $acc_o$ and the expected accuracy $acc_e$. The following equations define those accuracies and the Cohen Kappa score, in terms of confusion matrix cell values:

\begin{equation} 
acc_o(R) = \frac{TP+TN}{N}
\end{equation}

\begin{equation}
acc_e(R)  = \left(\frac{TP+FP}{N} \cdot \frac{TP+FN}{N}\right) + \left(\frac{TN+FP}{N} \cdot \frac{TN+FN}{N}\right)
\end{equation}

\begin{equation} \label{eq:Cohen Kappa}
\Mathenize{Cohen-Kappa(R)}  = \frac{acc_o(R) - acc_e(R)}{1 - acc_e(R)} 
\end{equation}  

\noindent Where $N=TP+TN+FP+FN$ is the total number of samples and R is the set of pairs $\{(f(X),y') | \forall (X,y') \in Dataset\}$, where $f$ is the predictor. For the purpose of aligning this metric with others, we can rescale that metric from $[-1,1]$ to $[0,1]$:

\begin{equation}
\Mathenize{Cohen-Kappa-Rescaled(R)} = \frac{\Mathenize{Cohen-Kappa(R)}+1}{2} 
\end{equation}  

In sequence, the Precision is a metric that measures the ratio of hits in the set of positive predictions. In our context, a higher Precision imply that the predictor approved a low amount of ``Bad'' signals, which is desirable in applications where we do not want to show to the user measurements based on unreliable signals. From the Precision and from the Recall, the ratio of hits in the set of all existing positives, we can obtain the F1-Score. Precisely, the F1-Score is the harmonic mean between those two metrics. In other words, a high F1-Score indicates a good balance between Precision and Recall scores. In our application, it measures the same as the Precision plus the Recall, which would measure the amount of ``Good'' signals that would feed the application. This is an desirable quality when we want to provide constant feedback to the user. The following equations define those metrics:

\begin{equation} \label{eq:Precision}
Precision = \frac{TP}{TP+FP}
\end{equation}


\begin{equation} \label{eq:Recall}
Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation} \label{eq:F1-Score}
\Mathenize{F1-Score}  = \Mathenize{Harmonic-Mean}(Precision,Recall) = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
\end{equation}

\noindent Therefore, the Cohen Kappa provides an overall sense of accuracy, the F1-Score suggest the usability level of the model and the Precision indicates the security of the predictor. 

Regarding the benchmarking metrics, our experiment measured the memory usage of the model in bytes and the inference time (added to the 1D-to-2D projection time for the projection-based models) in seconds. The memory measurement is important since practical applications of heart rate estimations often imposes hardware constraints that limits the allowed memory occupation. Additionally, the inference time is desirable for an almost-instant evaluation of the result, making the application more responsive.

\subsection{Overall schema}

\input{tex/figuras/framework}

The Figure \ref{fig:framework} expresses the experiment framework for the \gls{CV} models. A first observation is that the main difference from the framework applied to non-\gls{CV} models is that the 1D to 2D conversion acts as a frontier between the dataset and the rest of the components. That means the non-\gls{CV} models experiment can be expressed using almost the same schema by removing that conversion block. With that, the experiment began by the hyperparameters selection, splitting the \gls{BUTPPG} dataset using simple division for the purpose of, subsequentially, selecting the best hyperparameters of the \gls{CV}. With the best chosen hyperparameters, all models, including the non-\gls{CV} models, will be evaluated using the \gls{LOSO} strategy. For each fold, our experiments subjected the model to a training procedure that repeats epochs of training-and-validating until the early stopping interferes. Then, the model is tested, producing the metrics for that fold.

\subsection{The implementation details}

Presented the general concepts, this section discusses some details, following the practical sequence of events. It is proper to begin describing the dataset sourcing procedure. Our implementation done that sourcing by using the Pytorch multithreading data feeding solution, named Data Loader\footnote{Documentation available at \url{https://pytorch.org/docs/stable/data.html\#torch.utils.data.DataLoader}}. Our implementation configured it to load in batches of size 32. But, before loading the batches, our implementation did the training dataset balancing using the Imbalanced Learn library \cite{ImbalancedLearn} solution of the random oversampling\footnote{Documentation available at \url{https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\#imblearn.over_sampling.RandomOverSampler}}. Then, the batches are loaded transforming the 1D signal into 2D images using the projection algorithms of the PyTS Python library \cite{PyTS}. Even though the signals are now 2D, the width and height might be incompatible with the networks inputs, when considering their pre-trained weights. To solve this, our implementation applied the Pytorch resize transform\footnote{Documentation available at \url{https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html}} to adjust the width and the height. Finally, our implementation added a new convolutional layer corresponding to the \gls{PMix} method.

The \gls{CV} models were trained using a single NVIDIA RTX 3090 TI. For training, our implementation used the Pytorch implementation of the Adam optimization algorithm\footnote{Documentation available at \url{https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\#torch.optim.Adam}}, which uses the gradients evaluated by the Pytorch autograd engine \cite{Pytorch}. The loss class (which, in our case, is the torch.nn.CrossEntropyLoss\footnote{Documentation available at \url{https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\#torch.nn.CrossEntropyLoss}}) backpropagates the gradients based of the model forward pass errors. For the models testing, our implementation used the Sklearn \cite{Sklearn} metrics\footnote{Documentation at \url{https://scikit-learn.org/stable/modules/classes.html\#module-sklearn.metrics}}. For model memory measurement, our implementation counted the summation of the size of each parameter and buffer tensors in the \gls{CV} models, while for the non-\gls{CV} models, our implementation used the \texttt{asizeof} function\footnote{Documentation at \url{https://pympler.readthedocs.io/en/latest/library/asizeof.html}} of the Pympler library \cite{Pympler}. Finally, we describe the inference time measurement, for which our implementation extracted 500 measurement samples. For the non-\gls{CV} models, our implementation used the Python time method\footnote{Documentation at \url{https://docs.python.org/3/library/time.html\#time.time}}, of the time library, to measure instants of time. For the \gls{CV} models, our implementation marked the time instants by using CUDA events interface provided by Pytorch\footnote{Documentation at \url{https://pytorch.org/docs/stable/generated/torch.cuda.Event.html}}, while, before measuring, performing 500 iterations to warm-up the GPU. 

\section{Experimental results}

In this section, we analyzed the results by comparing the score metrics of the models and by assessing their trade-off with respect to the memory consumption and the inference time. Since our experiments considered a large set of models, our analysis did the comparisons in sections. In each section, our analysis considered one of the \gls{CV} model families of the Table \ref{tab:cv_list}: Transformers, Residual Nets, Mobile-Oriented, Extreme Nets, Efficiency-Oriented, and Diverse. In each of those sections, our analysis selected the best combinations of model variant and projection method. Subsequently, our analysis chose the best non-\gls{CV} models present in the Aeon toolkit library. Lastly, our analysis determined the final best choices, also discussing the differences between the projection methods.

\subsection{Per Model Family Analysis}
 
This subsection analyzes each \gls{CV} model family of the Table \ref{tab:cv_list}.
 
\subsubsection{Transformers}

Our experiments tested four transformers the \gls{ViT}, the \gls{MaxViT}, the \gls{SwinT} and its second version, \gls{SwinTV2}. The \gls{ViT} is the base model of the others. It transforms the visual input into a word where the letters are linear embeddings of subimages patches obtained by partitioning the original image in a grid-like shape. Then, the other modules increments the original by using multiple layers and by changing the attention mechanisms. For instance, the \gls{MaxViT} employs architectural blocks where each alternates between two self-attention modes: grid attention, a mode of high granularity, and block attention, a mode of low granularity. The \gls{SwinT}, on the other hand, changes its attention in layer-level, by merging patches of the antecedent layer into a new one, and in block-level, by shifting the self-attention windows into different positions. Finally, the \gls{SwinTV2} propose several specific improvements to the older version.

\textcolor{red}{
Our experiments generated one metric table for each transformer type. 
% Vision Transformer
The Table \ref{tab:Averages_of_VisionTransformer} presents the \gls{ViT} scores, where the variants can be Base (B), Large (L) or Huge (H) in parameter size and can have 14, 16 or 32 patch size. From that table, we can see that the \gls{PMix} and \gls{RP} projection methods obtained the best scores for all metrics. For most cases, the \gls{PMix} was equal or better than the \gls{RP}, except for the \mbox{H 14} variant, where \gls{RP} was superior. Among the variant plus projection combinations, the \gls{RP} and \gls{PMix} with \mbox{B 16} and \mbox{L 16}, and the \gls{PMix} with \mbox{B 32} and \mbox{L 32} led to the best scores. 
% MaxViT
The Table \ref{tab:Averages_of_Maxvit} shows the \gls{MaxViT} scores. For that model, the \gls{PMix} method achieved the best scores for the Cohen Kappa and Precision metrics, but the \gls{RP} surpassed it for the F1-Score, even though the \gls{PMix} had the smallest dispersion for that score.  
% Swin Transformer
The Table \ref{tab:Averages_of_SwinTransformer} exhibits the \gls{SwinT} scores, where the variants can be Base (B), Small (S) or Tiny (T) in parameter amount. The \gls{RP} allowed the best scores for the B and S variants, while the \gls{PMix} resulted in better scores for the T variant. Particularly, the combination \gls{PMix} with T achieved the best Cohen Kappa and Precision scores, but got the second best F1-Score, being surpassed by the combination \gls{RP} with S.
% Swin Transformer V2
The Table \ref{tab:Averages_of_SwinTransformer V2} exposes the \gls{SwinTV2} scores, where the variants can be Base (B), Small (S) or Tiny (T). The \gls{PMix} method permitted better scores for the B and S variants, but the \gls{RP} produced better scores for the T variant, although the \gls{RP} led to the largest dispersion for the F1-Score in that case. The \gls{PMix} with S combination, especially, resulted in better Cohen Kappa and F1 scores, with the second best Precision, for which \gls{RP} with T was better.
% Inter-Family Comparison
When considering all tables \ref{tab:Averages_of_VisionTransformer}, \ref{tab:Averages_of_Maxvit}, \ref{tab:Averages_of_SwinTransformer}, and \ref{tab:Averages_of_SwinTransformer V2}, the \gls{RP} and \gls{PMix} with \mbox{\gls{ViT} B 16} and \mbox{\gls{ViT} L 16}, the \gls{PMix} with \mbox{\gls{ViT} B 32} and \mbox{\gls{ViT} L 32}, and the \gls{SwinTV2} S with \gls{PMix} obtained generally better scores. For those combinations, we can observe the benchmarking metrics.
}
We see in the Table \ref{tab:Memory_of_Transformers} that the \gls{SwinT} V2 S variant uses considerably less memory than the \gls{ViT} variants, \textcolor{red}{so the combination \gls{SwinT} V2 S with \gls{PMix} can achieve high scores with few memory usage}. On the other hand, we observe in Figure \ref{fig:Time_of_Transformers} that the \gls{SwinT} V2 S variant have a slower inference speed if compared to the \gls{ViT} variants. \textcolor{red}{ Among the \gls{ViT} variants, the B 32 was the fastest, so the combination \gls{PMix} with \gls{ViT} B 32 can produce high scores with lower inference time.}

\sout{
About the results, we can see on the tables \ref{tab:Averages_of_VisionTransformer}, \ref{tab:Averages_of_Maxvit}, \ref{tab:Averages_of_SwinTransformer}, and \ref{tab:Averages_of_SwinTransformer V2} that only the Swin Transformer V2 and the Vision Transformer variants obtained a Cohen Kappa score superior than 90\%. Specifically, for the Swin Transformer V2, only the standard variant combined with the \gls{PMix} method achieved such a score. For the Vision Transformer, only the large and base variants suprassed the limit. The variants with patch size $32\times 32$ did so by using the \gls{PMix}, while the variants with patch size $16 \times 16$ not only reached the score when combined with \gls{PMix} but also with \gls{RP}. Since both of the models reached a similar score, we observe the benchmarking metrics. We see in the Table \ref{tab:Memory_of_Transformers} that the Swin Transformer V2 S uses considerably less memory than the Vision Transformer. On the other hand, we observe in Figure \ref{fig:Time_of_Transformers} that the Swin Transformer V2 S have a slower inference speed if compared to the Vision Transformer variants. However, because I consider the memory consumption as a factor of major importance, I selected for this section the Swin Transformer V2 standard variant with \gls{PMix}.
}

\input{tex/tabelas/resultados/averages/Averages_of_Transformers}

\IncludeMemoryTable{Transformers}{Transformers}

\input{tex/figuras/resultados/boxplots/Transformers}

\FloatBarrier

\subsubsection{Residual Nets}

This section analyzes the ResNet and its variations. The ResNet is a model that introduced the residual connections, links between non-adjacent layers that bypasses intermediate layers. As its variations, our analysis considered two: the Wide ResNet and the ResNeXt. The first one widens the original net by increasing the number of channels per block, with the intention of providing an alternative to increasing the layer depth. The second one employs a multipath philosophy, aggregating the paths by an additive operation. As opposed to the antecedent, it avoids increasing the width and the depth of the network by providing an additional dimension to increase. 

\textcolor{red}{
Our experiments generated three score tables. 
% ResNet
The Table \ref{tab:Averages_of_ResNet} presents the ResNet scores, where the variants could have 18, 34, 50, 101 and 152 layers. For those neural networks, the \gls{PMix} and \gls{RP} methods provided better scores than the others projections. Specially, the \gls{PMix} allowed the best scores when combined with the 50 and 101 variants.
% ResNeXt
The Table \ref{tab:Averages_of_ResNeXt} displays the ResNeXt scores, where the variants could have 50 or 101 layers, cardinality 32 or 64, and bottleneck width 4 or 8. In those cases, the \gls{PMix} and \gls{RP} methods obtained the best scores. Notably, the \gls{RP} with \mbox{ResNeXt 101 $32\times 8d$} got the best scores. 
% Wide ResNet
The Table \ref{tab:Averages_of_Wide ResNet} lists the Wide ResNet scores, where the variants could have 50 or 101 layers, with a widening factor 2. Again, the \gls{PMix} and \gls{RP} methods performed better when considering all scores. A special case was when the variant Wide \mbox{ResNet 101-2} used the \gls{PMix} method, case in which the Cohen Kappa and F1-Score were the best, and the Precision the second best.    
% Inter-Family Comparison
When we observe the tables \ref{tab:Averages_of_ResNet}, \ref{tab:Averages_of_ResNeXt} and \ref{tab:Averages_of_Wide ResNet} together, we can see that the \mbox{Wide ResNet 101-2} with \gls{PMix} was the best combination in terms of scoring. However, that combination was the largest in memory and the fourth slower in inference. An alternative with the second best scores but considerably less memory occupation and inference time would be the \mbox{ResNet 50} with \gls{PMix}. 
}

\sout{
Of those models, only the original ResNet and the Wide ResNet achieved a suficient Cohen Kappa score, as we can see in the tables \ref{tab:Averages_of_ResNet}, \ref{tab:Averages_of_ResNeXt} and \ref{tab:Averages_of_Wide ResNet}. From the ResNet variants, the ones with 50 and 101 layers reached that score. Solely the Wide ResNet with 101 layers and 2 convolutions per block sucessfully score above the limit. Those models variants got those scores by employing the \gls{PMix} projection method. Comparing these last variants, we conclude that the Wide ResNet 101-2 variant with the \gls{PMix} method obtained the best score, both for Cohen Kappa and F1 scores. However, that Wide ResNet posesses the largest memory occupation, according to the table \ref{tab:Memory_of_ResNet based}. It also spends the fourth higher inference time, as seen in the figure \ref{fig:Time_of_ResNet based}. Despite those disavantages, the Wide ResNet 101-2 variant with the \gls{PMix} was the choice of this section, because I prioritize the classification metrics over the benchmarking ones.
}

\input{tex/tabelas/resultados/averages/Averages_of_ResNet based}

\IncludeMemoryTable{Residual Nets}{Residual Nets}

\input{tex/figuras/resultados/boxplots/ResNet based}

\FloatBarrier

\subsubsection{Mobile-Oriented}

This section explores the mobile-oriented networks, that is, networks designed specifically for mobile hardware constraints. Our experiments tested three models: the MNASNet, the \mbox{Mobile Net V2} and the \mbox{Mobile Net V3}. In chronological order, the \mbox{Mobile Net V2} comes first. This network proposes several architectural changes to use less memory while maintaining accuracy, with inverted residual blocks being one of them. This proposed alteration swaps places of the high-channeled and low-channeled layers, which causes the connection between layers with a lower amount of channels, reducing the number of parameters in that block. The MNASNet select blocks to fit in a predefined architectural skeleton, optimizing the model performance on real-world mobile hardware. Finally, the \mbox{Mobile Net V3} combined both approaches, while also made changes such as adding the NetAdapt \cite{NetAdapt} algorithm in the architectural search.

\textcolor{red}{
Our experiments generated three metric tables for that family of \gls{CV} models.
%MNASNet
The Table \ref{tab:Averages_of_MNASNet} registers the scores obtained by the MNASNets, variants that could have depth multiplier of 0.5, 0.75, 1.0 or 1.3, with respect to number of channels. One case stood out: the combination of \mbox{MNASNet 1.0} with \gls{PMix}, which achieved the best score for all metrics.
%MobileNet V2
The Table \ref{tab:Averages_of_MobileNet V2} presents the scores of the MobileNet V2. The \gls{RP} projection reached the best Cohen Kappa and Precision scores, while the \gls{PMix} obtained the the best F1-Score. However, the \gls{RP} performed generally more consistently, since it produced less variable results when compared to the \gls{PMix} standard deviations. 
%MobileNet V3 
The Table \ref{tab:Averages_of_MobileNet V3} lists the MobileNet V3 variants, with one being Small and the other Large in terms of resource usage. Two cases performed notably better: the \gls{PMix} with the Large variant, for the Cohen Kappa and Precision scores, and the \gls{RP} with the Small variant, for the F1-Score metric.
% Inter-Family Comparison
From the tables \ref{tab:Averages_of_MNASNet}, \ref{tab:Averages_of_MobileNet V2} and \ref{tab:Averages_of_MobileNet V3}, we notice an overall best case: the combination of \mbox{MNASNet 1.0} with \gls{PMix}. That case, like the others has inference time median bellow to 20 ms, according to the Figure \ref{fig:Time_of_Mobile nets}, and memory consumption lower than 15 MB, according to the Table \ref{tab:Memory_of_Mobile nets}, results that competed with the other nets in the \gls{CV} family.
}

\sout{
In those set of models, only the MNASNet obtained a sufficient Cohen Kappa score, as visible in the tables \ref{tab:Averages_of_MNASNet}, \ref{tab:Averages_of_MobileNet V2} e \ref{tab:Averages_of_MobileNet V3}. Specifically, the MNASNet with depth multiplier of 1.0, being that multiplier related to the number of channels dimension. It achieved that score combined with the \gls{PMix} projection method method. According to the table \ref{tab:Memory_of_Mobile nets} and the figure \ref{fig:Time_of_Mobile nets}, all of those networks have very small memory size, with values inferior to 30 MB. They also have very fast inference time, with medians bellow to 20 ms. For those reasons and since it was the only net to achieve the minimum accuracy requirements, I chose the MNASNet 1.0 with \gls{PMix}.
}

\input{tex/tabelas/resultados/averages/Averages_of_Mobile nets}

\IncludeMemoryTable{Mobile-Oriented}{Mobile-Oriented}

\input{tex/figuras/resultados/boxplots/Mobile nets}

\FloatBarrier

\subsubsection{Extreme Nets}

In this section, our analysis discusses the results of neural models which focus on one concept, such as layer depth, model compression or residual connections, and explore its extent, such as the VGG, the DenseNet and the SqueezeNet. The VGG is a model that uses filters of size $3\times 3$ to allow adding more layers, increasing the depth model. On the other hand, the DenseNet applies skipping connections of the residual nets to all pairs of architectural blocks present in the network, to achieve the benefits of having each layer closer to the input and the output of the model. Finally, the SqueezeNet tries to reach minimum memory usage not only by applying model compression techniques but by introducing a new architectural module which reduces the number of channels of a layer before another with large convolutions filters, such as $3 \times 3$ filters. This reduces the number of parameters considerably.

\textcolor{red}{
Our experiments constructed three metric tables, one for each model in the \gls{CV} family.
% Dense Net
The Table \ref{tab:Averages_of_DenseNet} shows the DenseNet results, where the variants could have 121, 161, 169 or 201 layer depth. For the variants 169 and 201, the \gls{PMix} method performed better, and for the variant 161 the \gls{RP} was the best. As for the variant 121, the \gls{PMix} got the best Cohen Kappa score, while the \gls{RP} got the best F1-Score and a perfect Precision. Overall, the \mbox{DenseNet 161} with \gls{RP}, \mbox{DenseNet 201} with \gls{PMix}, and \mbox{DenseNet 121} with \gls{RP} obtained the best scores for, respectively, the Cohen Kappa, F1-Score, and Precision metrics. The \mbox{DenseNet 161} with \gls{RP}, specifically, seemed to have achieved a good metric balance, reaching the best Cohen Kappa score and the second best F1 and Precision scores.    
% SqueezeNet
The Table \ref{tab:Averages_of_SqueezeNet} exhibits the SqueezeNet results, with variants corresponding to the version 1.0 and the optimized version 1.1. The optimized version achieved the best scores when combined to the \gls{PMix} method, reaching the best Cohen Kappa and F1 scores, and when combined to the \gls{RP}, obtaining the best Precision score. Both of those combinations got an generally good score across all metrics. 
% VGG
The Table \ref{tab:Averages_of_VGG} unrolls the VGG scores, with possibly 11, 13, 16 or 19 layers and with or without Batch Normalization (BN). For that model, the \gls{RP} and \gls{PMix} methods allowed the best scores for all variants, despite the occurrence of the highest dispersions in some cases. The combinations \mbox{VGG 16} with \gls{RP} and \mbox{VGG 16 BN} with \gls{PMix} scored the highest, with the first combination being the best for the Cohen Kappa and Precision metrics and the second for the F1-Score. However, the \mbox{VGG 16} with \gls{RP} combination had one of the highest dispersions for the F1-score metric, so the other combination was more reliable.   
% Inter-Family Comparison
Among the best variants of each model of the tables \ref{tab:Averages_of_DenseNet}, \ref{tab:Averages_of_SqueezeNet} and \ref{tab:Averages_of_VGG}, the combinations \mbox{SqueezeNet 1.1} and \mbox{VGG 16 BN} with \gls{PMix} were the best, with the \mbox{SqueezeNet 1.1} one having the best Cohen Kappa and the \mbox{VGG 16 BN} one having the best F1-score, when we consider every model variant in the Extreme Nets \gls{CV} family. On the Figure \ref{fig:Time_of_Extreme nets} we can see that the \mbox{SqueezeNet 1.1} with the \gls{PMix} was faster than most model variants. Additionally, on the Table \ref{tab:Memory_of_Extreme models} we can also see that the same combination was the smallest.
}

\sout{
Accordingly to the tables \ref{tab:Averages_of_DenseNet}, \ref{tab:Averages_of_VGG} and \ref{tab:Averages_of_SqueezeNet}, only the SqueezeNet and the VGG achieved the minimum Cohen Kappa score. For the Squeezenet, only the version 1.1, a more economic version if compared to the 1.0, reached the score, when combined to the \gls{RP} and the \gls{PMix}, with the \gls{PMix} giving the best score. The second was the VGG with 16 weight layers without batch normalization, when combined to the \gls{RP} projection method. Of those combinations, the SqueezeNet 1.1 with \gls{PMix} performed better than the others, while having less variance. Adding to that performance, the SqueezeNet 1.1 is the smallest model in memory, according to the table \ref{tab:Memory_of_Extreme models}. Furthermore, the SqueezeNets have low inference time if compared to the other models, as the Figure \ref{fig:Time_of_Extreme nets} depicts. Therefore, I selected for this section the SqueezeNet 1.1 with \gls{PMix}.
}

\input{tex/tabelas/resultados/averages/Averages_of_Extreme models}

\IncludeMemoryTable{Extreme Models}{Extreme Nets}

\input{tex/figuras/resultados/boxplots/Extreme nets}

\FloatBarrier

\subsubsection{Efficiency-Oriented}

This section discusses the models that researchers designed to efficiently utilize resources, trying to maximize the performance with less parameters. For this work, our experiments explored the ShuffleNet V2, EfficientNet and EfficientNet V2. The first one is the ShuffleNet V2, successor of the ShuffleNet, which introduced the channel shuffle operator to allow information exchange among channels. It changes its predecessor by proposing, for each block, a channel split operation, to avoid costly operators such as grouped convolutions. Beside the shuffle nets, we have the efficient nets. The EfficientNet researchers studied the model scaling and created a compound resizing method that proportionally increase multiple dimensions, such as depth, number of channels and resolution. This allowed to create a very efficient base model that the researchers scaled up to bigger variants that maintain the advantages of the original. Finally, the EfficientNet V2 reformulated the predecessor by proposing a non-proportional scaling and by employing network architectural search. Additionally, it employed progressive learning by gradually increasing the dataset regularization.  

\textcolor{red}{
Our experiments built three score tables for the Efficiency-Oriented \gls{CV} family.
% EfficientNet
The Table \ref{tab:Averages_of_EfficientNet} lists the EfficientNet results, where the variants can vary from B0, the smallest, to B4, the biggest, when considering parameter scaling. The \gls{PMix} projection achieved the best score for the variants B0, B1 and B2. For the variant B3, the \gls{MTF} method reached the best F1-score, while the \gls{RP} was superior for the other metrics. When we consider all variants, the \mbox{Efficient Net B1} with \gls{PMix} was the best combination.
% EfficientNetV2
The Table \ref{tab:Averages_of_EfficientNetV2} shows the \mbox{EfficientNet V2} scores, where the \gls{PMix} projection performed better than all other methods.
% ShuffleNet V2
The Table \ref{tab:Averages_of_ShuffleNet V2} displays the \mbox{ShuffleNet V2} variants metrics. Those variants could have $\times 0.5$, $\times 1.0$, $\times 1.5$, or $\times 2.0$ multipliers on the number or channels in each architectural block. For all variants, the \gls{PMix} surpassed all other projection methods. Specially, the best scores along all \mbox{ShuffleNet V2} results combine the $\times 0.5$ and $\times 1.0$ variants with the \gls{PMix} method.
% Inter-Family Comparison
When considering all the results from the tables \ref{tab:Averages_of_EfficientNet}, \ref{tab:Averages_of_EfficientNetV2} and \ref{tab:Averages_of_ShuffleNet V2}, we see that the \mbox{EfficientNet V2}, \mbox{ShuffleNet V2 $\times 0.5$}, and \mbox{ShuffleNet V2 $\times 1.0$} combined with the \gls{PMix} achieved a broadly high score for all metrics, with the best Cohen Kappa and F1 scores, and the second best Precision. Among those combinations, the \mbox{ShuffleNet V2 $\times 0.5$} with \gls{PMix} was the smallest in memory usage, according to the Table \ref{tab:Memory_of_Efficiency Oriented}, while being one of the fastest in inference, as visible in the Figure \ref{fig:Time_of_Efficiency Oriented}.
}

\sout{
We observe in the tables \ref{tab:Averages_of_ShuffleNet V2}, \ref{tab:Averages_of_EfficientNet}, and \ref{tab:Averages_of_EfficientNetV2} that none of the EfficientNet variants accomplished the minimum Cohen Kappa Score. Only a few variants of the other models remained. For the ShuffleNet V2, we see that the variants with $\times 1$ and $\times 0.5$ channels were the remainders, when combined with the \gls{PMix} method. For the EfficientNet V2, the unique variant that I tested, we observe that only the \gls{PMix} method allowed the minimum score. The scores that the ShuffleNet V2 variants reached are similar to the ones of the EfficientNet V2. However, the ShuffleNet V2 variants are smaller than the EfficientNet V2, as we can visualize in the Table \ref{tab:Memory_of_Efficiency Oriented}. Furthermore, the Figure \ref{fig:Time_of_Efficiency Oriented} shows that the ShuffleNet V2 variants are generally faster than the EfficientNet V2. For those reasons, I chose the smallest variant, ShuffleNet V2 $\times 0.5$, combined with the \gls{PMix} projection method. 
}

\input{tex/tabelas/resultados/averages/Averages_of_Efficiency Oriented}

\input{tex/figuras/resultados/boxplots/Efficiency Oriented}

\IncludeMemoryTable{Efficiency-Oriented}{Efficiency-Oriented}

\FloatBarrier


\subsubsection{Diverse}

This section analyzes the remaining models. They are: AlexNet, ConvNeXt and RegNet. Researchers introduced the first one in the old year of 2012. That net tested the idea of deep learning trained on multiple GPUs, allowing a faster training. Furthermore, it applied dropout to reduce overfitting. In contrast, the ConvNeXt is a modern model of 2022, which reunites several convolutional techniques of the past years, such as patchified convolutions, inverted bottlenecks and grouped convolutions. The researchers proposed that model to push the state of traditional convolutional networks to the limit. Differently of the antecedents, the RegNet employs the idea of designing network spaces rather than individual nets. Those populations are characterized by the restriction of the parameter space to a linear function. This created populations of networks that are more appropriate for random architectural search. In this manner, our experiments tested networks with considerable differences among them.

\textcolor{red}{
Our experiments led to three score tables for the remaining \gls{CV} models.
% AlexNet
The Table \ref{tab:Averages_of_AlexNet} contains the scores of AlexNet. Notably, the \gls{PMix} projection earned the best scores for all metrics in that table.
% ConvNeXt
The Table \ref{tab:Averages_of_ConvNeXt} shows the results of the ConvNeXt, where it could be Tiny, Small, Base or Large in parameter size. For those variants, the \gls{RP} and \gls{PMix} methods were the best in all scores, even though some cases were the most dispersed, such as \gls{RP} with the Tiny variant for the Cohen Kappa score. The combination \gls{PMix} with \mbox{ConvNeXt Tiny} obtained the best F1-score, while the \gls{PMix} with \mbox{ConvNeXt Small} obtained the best Cohen Kappa and Precision scores. That last case also achieved the second best F1-score.
% RegNet 
The Table \ref{tab:Averages_of_RegNet} exhibits the RegNet variants and its scores. Those variants could be either X or Y when, respectively, they belonged to the RegNetX or RegNetY design spaces \cite{RegNet}, and could have different float operations per second rates, such as 400 Mega Flops (MF) or 16 Giga Flops (GF). Among those variants, some cases reached an above-third-quartile scores in all metrics. For the X space, they were: \gls{RP} with \mbox{400 MF} and \mbox{800 MF} variants; and \gls{PMix} with the \mbox{800 MF}, \mbox{3.2 GF}, \mbox{8 GF}, and \mbox{16 GF} variants. For the Y space, they were: \gls{RP} with the 400 MF, 1.6 GF, 16 GF, and 32 GF variants; and \gls{PMix} with the 800 MF, 3.2 GF, and 16 GF variants. Amid those cases, the \gls{PMix} with the \mbox{RegNet X 3.2 GF}, \mbox{RegNet X 800 MF}, \mbox{RegNet Y 400 MF}, and \mbox{RegNet Y 800 MF} variants obtained the best Cohen Kappa and F1 scores, and the third best Precision score. When we consider those high scoring variants, the \mbox{RegNet Y 400 MF} with \gls{PMix} used the lower amount of memory, as seen in Table \ref{tab:Memory_of_Diverse}, while the \mbox{RegNet X 800 MF} with \gls{PMix} had the fastest inferences of the model variants, as seen in Figure \ref{fig:Time_of_Diverse}.        	
% Inter-Family Comparison
When we consider the best models in each model type in the Diverse \gls{CV} family, we notice that \mbox{RegNet Y 400 MF} with \gls{RP}, and \mbox{RegNet X 800 MF} and AlexNet with \gls{PMix}, achieved the highest scores. Specially, the \mbox{RegNet Y 400 MF} with \gls{RP} had the least memory occupation, as visible in Table \ref{tab:Memory_of_Diverse}, while the AlexNet had the lowest inference time for all projections, as the Figure \ref{fig:Time_of_Diverse} shows.
}

\sout{
Amid those models, only the RegNet and the AlexNet passed the minimum Cohen Kappa test, as we can infer from the tables \ref{tab:Averages_of_AlexNet}, \ref{tab:Averages_of_ConvNeXt}, and \ref{tab:Averages_of_RegNet}. The AlexNet did so by applying the \gls{PMix} method for its only variant. The RegNet, on the other hand, has a plenty of variants, that originate from two network design spaces: X, the base space, and Y, which adds a squeeze-and-excitation operation after the blocks of X. From those variants, only the 3.2 GF (Giga FLOPs) and 800 MF (Mega FLOPs) variants of the X network space and only the 400 MF and 800 MF variants of the Y network space reached the score. Most of them did so by applying the \gls{PMix}, with only one exception: The RegNet Y 400 MF did by using the \gls{RP}. Since the variants of both models scored similarly, we observe the Figure \ref{fig:Time_of_Diverse} and the Table \ref{tab:Memory_of_Diverse}. The first observation is that the the AlexNet occupies, at least, two times more memory than most of the RegNet variants, including the ones with good score. The second one is that the AlexNet has faster inference speed than every model in this section. But, since the memory occupation is priority, I chose the RegNet over the AlexNet. Specifically, the RegNet Y 400 MF variant (with \gls{RP}), because it is the smallest.  
}

\input{tex/tabelas/resultados/averages/Averages_of_Diverse}

\input{tex/figuras/resultados/boxplots/Diverse}

\IncludeMemoryTable{Diverse}{Diverse}

\FloatBarrier

\subsection{Non-CV models comparison}

\textcolor{red}{
This section evaluate non-computationally-visual baseline models. The Table \ref{tab:Averages_of_Non CV} shows their respective scores. In that table, the models Individual \mbox{Ordinal TDE}, Random Interval Classifier, \gls{RISEC}, \mbox{TS Fresh} Classifier, \gls{TDE}, and \mbox{WEASEL V2} achieved high scores for all metrics. Particularly, the \gls{RISEC} and the \gls{TDE} surpassed all models for every score metric. Between them, the first model had faster inference, as seen in Figure \ref{fig:Time_of_Non CV}, while the second had lower memory consumption, according to the Table \ref{tab:Memory_of_Non CV}.
}

\sout{
This section evaluate non-computationally-visual models, which I presented in the chapter \ref{Revisao_bibliografica}. Firstly, we observe in the Table \ref{tab:Averages_of_Non CV} that seven models achieved the minimum Cohen Kappa score. Among them only two achieved the greatest scores: the Random Interval Spectral Ensemble Classifier and the Temporal Dictionary Ensemble. But they got similar scores, so I refer to the Table \ref{tab:Memory_of_Non CV} and Figure \ref{fig:Time_of_Non CV} to decide which to choose. It is visible that the Random Interval Spectral Ensemble Classifier is faster in inference time than the Temporal Dictionary Ensemble, while the later is smaller in memory size. Due to the memory being a factor of major priority, I chose the Temporal Dictionary Ensemble. 	 
}

\input{tex/tabelas/resultados/averages/Averages_of_Non CV}

\IncludeMemoryTable{Non-CV}{Non-CV}

\input{tex/figuras/resultados/boxplots/Non CV}

\FloatBarrier

\subsection{Best Models Found Comparison}

This section aggregates the best combinations between models and projections of the previous sections. Observing the Table \ref{tab:Averages_of_best models}, we see that the three models obtained the best scores: the \gls{TDE}, the \gls{RISEC}, and the \mbox{Wide ResNet 100-2} with \gls{PMix}, even though the other models obtained comparable scores. That \mbox{Wide ResNet} obtained the highest Cohen Kappa score, but there are some considerations. Firstly, that model was the second largest among the models of this section, as we can read on Table \ref{tab:Memory_of_Best Models}. Secondly, it did not obtained the best F1-Score and Precision. Lastly, the \mbox{Wide ResNet} was the second slowest model, as we can refer to the Figure \ref{fig:Time_of_Best Models}. On the other hand, the \gls{TDE} and the \gls{RISEC} were the best in the usability and security scores, F1-Score and Precision. They also were the fastest models in inference and the smallest in memory space. Therefore, we can conclude that the \gls{CV} approach led to a better accuracy, represented by the \mbox{Wide ResNet 100-2} with \gls{PMix}, but the non-\gls{CV} approach could be more useful, since they used less resources, were faster and had the advantages of the F1-Score and Precision metrics.


\input{tex/tabelas/resultados/averages/Averages_of_Best Models}

\IncludeMemoryTable{Best Models}{best models}

\input{tex/figuras/resultados/boxplots/Best Models}

\section{Limitations}
\label{sec:Limitations}

Even though the results are promising, we need to consider a series of limitations present in our experiments. One main observation is that our experiments used only the BUTPPG dataset for testing. This has a series of implications in our context. Firstly, even though the proposed method allowed the use of \gls{CV} models with a good performance in the BUTPPG dataset, the same could not necessarily be concluded for different datasets. This is because different methods of measurement, sensor qualities, signal lengths, and individual medical conditions could lead to alterations on the obtained performance. One evidence of that is the absence of confirmed \gls{CA} cases in the \gls{BUTPPG} dataset, which could be present in external data. Furthermore, the small size of the dataset resulted in a reduced testing dataset, which makes the obtained results less general, that is, unreliable when we consider the possible variability of data that is external to the dataset. A small dataset size also implies that the deep learning models had less data samples to effectively learn. This contrasts with the usual treatment for deep learning, where usually large amounts of data feed the training of the model, allowing the proper adjustment of the large set of parameters. Therefore, our experiments would be more complete if our experiments tested on different and larger datasets.

Other limit is that our experiments did not explored all the options available, in terms of models and hyperparameters. For instance, our experiments left out some of the models of the Pytorch and the Aeon libraries. Examples of them are the GoogLeNet \cite{GoogLeNet}, from the Pytorch, and the Hydra Classifier \cite{HydraClassifier}, from the Aeon. Additionally, our experiments did not tested models external to those libraries, such as the Xception \cite{Xception}, available at the Keras library \cite{Keras}. Also, for some of the tested models, our experiments did not evaluated all the variants, such as the Efficient Net B7. Furthermore, our experiments did not search over the hypeparameters of the projection methods, as the number of dimensions of the \gls{RP}. Thus, various options could be explored, as the different libraries that implement machine learning models, while the currently used ones could be better availed. Moreover, our experiments could do Optuna hyperparameter search over the projection hyperparameters.

Finally, some other limitations were evident at the implementation level. Firstly, our implementation only did data random oversampling for the purpose of balancing the dataset. However, there are other options designed specifically for time series, such as presented in the work of Wen et al. \cite{TimeSeriesAugmentation}. With that, our implementation could not only balance the dataset but enlarge it as well. Secondly, our implementation used a resize transform to adapt the projection image to the model input. This could cause a considerable loss of information for matrix images that encode relationships in each pixel. As a consequence, the \gls{CV} models probably did not performed as good as they could. Thirdly, there was no research for the early stop method that our implementation used. There is a chance that this method prematurely stopped the training for models that required more epochs. Lastly, our implementation did the measurements of the benchmarking metrics by using the Python standard API, which is not a direct method of measuring them, but is limited to the interpreter. Additionally, our measurements did not control the environment where measured the inference time. This could imply that external users have scheduled tasks that competed with mine measurements. Therefore, much could have done much improvements at the implementation level, such as applying time series augmentation techniques, resizing without distorting the image by using an integer multiplier and padding the remaining space, researching early stopping methods, and measuring the benchmarking metrics in more controlled environment and using low-level interfaces.        
