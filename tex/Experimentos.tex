%The tests were performed using the \gls{BUTPPG}~\cite{butppg}. In such a database, it was utilized a smartphone to record 48 \gls{ppg} signals of 12 \gls{sbjs} (\gls{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. This database can be consumed through Physionet~\cite{physionet} interface via \texttt{wfdb} package. The proposed method was implemented in Python. Implementations of \gls{gaf}, \gls{mtf} and \gls{rp} were provided by PyTS \cite{pyts} library. The PyTorch library~\cite{pytorch} was used to perform the training and the classification operations. The models used in this study are listed in Table~\ref{tab:results} and provided by TorchVision. Hyper-parameters optimization was performed using the Optuna library~\cite{optuna}. For each model, 50 Optuna trials were performed for fitting and validating the \gls{ml} model with median pruner to avoid excessive computation of trial epochs that do not show hope of better results.

%After the optimization, the metrics of the best trial were evaluated in the testing dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \gls{ml} model in hands, the dataset was divided into folds using the cross-validation \gls{loso} re-sampling method, into pieces matching each of the 12 \gls{sbjs}. For each fold, the smaller split was used as the testing dataset for the evaluation of the model's metrics, while the bigger split was subdivided into the training dataset, of size 7 \gls{sbjs}; and into the validation dataset, of size 4 \gls{sbjs}. Applying such an experimental setup allowed the generation of results concerning the metrics present in Table~\ref{tab:results}, where, for each projection method, all models were seen as samples of a statistics population possessing 5 values corresponding to the mean of all 12 folds of each metric.

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \gls{sbjs}; validation, with 2 \gls{sbjs}; and test, with 3 \gls{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \gls{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%In order to evaluate those projection methods, the following metrics were considered accuracy score, the proportion of hits $Accuracy = \frac{(TP+TN)}{TP+TN+FP+FN}$; precision score, the proportion of correct positive guesses, $Precision = \frac{TP}{TP+FP}$; recall score, the proportion of found positive samples, $Recall = \frac{TP}{TP+FN}$; and F1 score, the harmonic mean between the precision score and the recall score, $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$. Additionally, it was measured the Cohen Kappa score, the degree of agreement of annotators for a classification problem, $CohenKappa = 1-\frac{1-p_o}{1-p_e}$, where $p_o$ is the proportion of observed concordance and $p_e$ is the probability of concordance between all annotators. In the supervised binary classification domain, only two annotators, corresponding to the predicted and the real labels, and two classes, corresponding to positive or negative, are considered, in a manner that the confusion matrix can be used directly to evaluate the score, by the formula $CohenKappa=\frac{2\cdot (TP\cdot TN - FP \cdot FN)}{(TP+FP)\cdot(FP+TN)+(TP+FN)\cdot(FN+TN)}$.

% Those metrics were evaluated for 70 \gls{ml} models implemented in \href{https://pytorch.org/}{PyTorch}, the ones listed on Table~ \ref{tab:results}
% For such networks, the same experiment framework was applied, highlighting three major process blocks: dataset construction, when the database was loaded and transformed; hyperparameters selection, when, for each \gls{ml} model, a search was done to try to find the set of its best hyperparameters for the built dataset; and projection metrics evaluation, when each model, in conjunction with its set of best hyper-parameters found, was tested, generating the metrics present in this article. %(the framework is shown in figure \ref{figure:framework})
% Also, hyper-parameters selection and projections evaluation apply training and testing cycles.  To clarify those processes, they will be described in detail in the following sections.


%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\linewidth]{imgs/framework2.pdf}
%    \caption{Flowchart representing the experimental framework. Gray elements are entities, while blue elements are processes.}
%    \label{figure:framework}
%\end{figure*}

% \subsection{Training and Testing}

% Machine learning tasks involve 2 main steps: training and testing. Such a format was applied several times on the hyper-parameters selection process, for testing certain sets of hyper-parameters; and the projections evaluation process, for evaluating the model and its set of hyper-parameters efficiency. That core task was done using \href{https://pytorch.org/}{PyTorch} python library, which automatically computes the gradients based on the user's implementation \cite{pytorch}. Also, it allows parallel GPU processing to be done, by applying tensor-based operations to batches of the dataset, which size used in this experiment was the whole dataset. 

% The training process parameters also involve an optimizer and and loss function. The optimizer algorithm to be used was Adam\cite{adam}, while the Cross-Entropy Loss function was used, %\cite{cross-entropy-loss} 
% both implemented in \href{https://pytorch.org/}{PyTorch}. Moreover, the training process needs to stop at some point, fact that was done using an early stopping approach, computing the median absolute deviation, deviation metric robust to outliers, %\cite{?}
% of a window of the last 10 loss function values on the validation dataset, stopping the training process when such windows converge to, at least, a deviation of value 1. When the training process stopped, the best result found was chosen.

% \subsection{Dataset construction}

% In order to provide data to train and test \gls{ml} models, the Brno University of Technology Smartphone PPG Database was used as a starting point. In such a database, it was utilized a smartphone to record 48 \gls{ppg} signals of 12 \gls{sbjs} (\gls{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. Additionally, other refinement procedures were done to perfect that database, such as cropping 20 seconds of the recording, leaving the middle 10 seconds intact\cite{butppg}. And, finally, the database is publicly available on Physionet through the link \url{https://physionet.org/content/butppg/1.0.0/}. 

% However, the database is in \gls{wfdb} format \cite{butppg}, one dimensional signal format in function of time, %\cite{wfdb}
% letting yet to be done the application of projection methods. For such purpose, PyTS was used, a Python package developed for time series classification, containing the desired projection algorithms, \gls{gaf}, \gls{mtf} and \gls{rp} \cite{pyts}, with its implementations of PyTS version 0.13.0 stored in \href{https://zenodo.org/}{Zenodo} repository \cite{pyts-v0.13.0}.
    
% With such a package, it was possible to produce a dataset containing all projections as 2D images with 3 channels with shape $3 \times siglen^2 \times siglen^2$, where $siglen$ is the signal length. For each projection, its matrix was repeated on every channel of the image, while the \gls{PM} filled every channel with each of the 3 mentioned projections. Finally, it was necessary to resize every image for each \gls{ml} model input shape, for such purpose that it was used \href{https://pytorch.org/}{PyTorch} resize transform with bi-linear interpolation. After those procedures, the dataset was ready for the following processes, as the hyper-parameters selection.

% \subsection{Hyper-parameters selection}

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \gls{sbjs}; validation, with 2 \gls{sbjs}; and test, with 3 \gls{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \gls{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%\subsection{Projection Metrics Evaluation}

%After the optimization, the metrics of the best trial were evaluated in the test dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \gls{ml} model in hands, the dataset was divided into folds using the cross-validation \gls{loso} re-sampling method, into pieces matching each of the 12 \gls{sbjs}. For each fold, the left split was used as the test dataset for the evaluation of the model's metrics, while the other split was subdivided into the train dataset, of size 7 \gls{sbjs}; and into the valid dataset, of size 4 \gls{sbjs}. Applying such an experimental setup allowed to generate results with respect to the before-mentioned metrics, where, for each projection method, all models are seen as samples of a statistics population possessing 5 values corresponding to the metrics.

%Figure~\ref{fig:boxplots} depicts presents the boxplot of Accuracy, F1-score, Precision, Recall, and Cohen's Kappa distributions for different \glspl{CV} models tested on the \gls{BUTPPG} database. From these graphs, we can notice that, for the recall score, all projections are very equated. Moreover, it is noticeable that \gls{rp} and the proposed \gls{PM} have improved performances when compared with other \gls{mtf} and \gls{gaf} projections, especially when we observe the minimum, maximum, and the first quartiles of distributions, presented in Table~\ref{tab:boxplots}. From this table, we can perceive, at first glance, that the \gls{rp} and the \gls{PM} are generally superior methods if compared with the other two. In fact, \gls{rp} and \gls{PM} first quartiles are greater than \gls{gaf} and \gls{mtf} third quartiles considering all metrics except Recall Score, which shows that 75\% of samples in \gls{rp} and \gls{PM} are generally greater than 75\% of samples in \gls{gaf} and \gls{mtf}. The detailed performance of the points in the distributions considered in Figure~\ref{fig:boxplots} is depicted per model in Table~\ref{tab:results}.

%\input{tables/results}

% Each of the 4 populations produced box plots shown in figure \ref{fig:boxplots}, allowing, at first glance, to see the Recurrence Plot and the \gls{PM} as generally superior methods if compared with the other two. In fact, \gls{rp} and P\gls{PM} first quartiles are greater than \gls{gaf} and \gls{mtf} third quartiles considering all metrics except Recall Score, what shows that 75\% of samples in \gls{rp} and \gls{PM} are generally greater than 75\% of samples in \gls{gaf} and \gls{mtf}.    %TODO: falar do Cohen Cappa Score 

This chapter first presents the experimental setup. Then, it exposes and discusses the experimental results. Finally, it investigates the experiments limitations. 

\section{Experimental Setup}

This section discusses the experimental setup, analyzing elements used in the experiments, such as datasets, programming libraries and predictive models. Additionally, it clarifies metrics to be evaluated and approaches to measure them.  

\subsection{The dataset}

As for every machine learning task, we need a dataset to provide data to feed the predictive models for their parameters fitting, molding them to the domain of the specific task. In this work, the task of assessing the quality of the signal is a supervised classification problem, that is, can be described as the problem of finding a function that best defines a predefined set of pairs of variables and label, $(X,y)$. In this scope, the pair corresponds to the signal itself mapped to its quality label, ``Good'' or ``Bad''. For the purpose of training the predictive methods and evaluating their ability to fit to the problem of classifying the quality of heartbeat time series, the \gls{BUTPPG}~\cite{butppg} dataset was employed.

%\input{tex/figuras/samples}

The \gls{BUTPPG} is a publicly available database produced by the Department of Biomedical Engineering of the the Brno University of Technology. It contains samples of \gls{PPG} signals, its quality labels and its heat rate estimations. Those signals were extracted using a low-cost method: recording with the camera of a smartphone. To be more precise, the researchers recorded the subject's index finger, covering the lens of the camera and its \gls{LED} light. Then, they measured, for each video frame, the average of the intensities of the red channel of every image pixel, resulting in a time series of averages. Finally, they inverted the signal. %The Figure \ref{fig:butppg_samples} shows examples of the results of such a sequence of procedures.  

They done this method of obtaining \gls{PPG} signals  48 times, ammount distributed equally between 12 subjects. That is, for each subject, they did 4 measurements. Moreover, they did those recordings in two possible situations: one which the subject sat down and stayed static, case which the quality label ``Good'' was probable; and other in which the subject walked, hence, case likely to be a ``Bad'' recording. That distinction is relevant, since the walking situation occurred only 1 time for each subject, biasing the labels proportion to be nearly 25\% of ``Bad'' ones. Therefore, this dataset is imbalanced, factor that we need to handle in our experiment.

As for the definition of the signal quality labels, they designated specialists to estimate the heart rate associated with the \gls{PPG} signals, with the help of a software specialized made by the researchers for the analysis. Then, the organizers compared the number they gave with the one given by a gold standard method that, instead of using the \gls{PPG} signal, used an ECG recording as reference. The measurer syncronized the ECG mannually. If the specialist measured with an error less or equal than 5 bpm, then the organizers considered the estimative correct. Finally, if 3 specialists of 5 gave correct estimations, the organizers considered the quality of the \gls{PPG} signal as correct. Thus, the dataset ``Good'' labels, in essence, identify if a signal is human-readable. 

\subsection{The dataset split}

Machine learning tasks also requires the separation the dataset into fragments. One of them is the training dataset, used for the models parameters fitting. Another is the test dataset, used for evaluating the models efficiency. An optional one is the validation dataset, used to choose the set of best hyperparameters of a trained models. In our experiment case, to define the training-test splits, it was used a cross validation method named \gls{LOSO}, which partitions the dataset into $K$ train-test splits. It makes the $k$-th train-test split assigning the $k$-th piece as the test dataset, lefting the other $K-1$ pieces as the training dataset. In the \gls{BUTPPG} case, the $K$ value is 12, the number of subjects. Notice that the smaller unity of division is the subject, not the signals that are associated with it. It has the advantage of increasing the difference between training samples and testing samples. Since the dataset is small, such a split method allows the maximum provect of the avaliable resources, because it uses every sample in the dataset as a test object at least once, without biasing the results. Additionally, the training datset was divided producing a validation dataset of size 3, with usage that will be explained later.

\subsection{The models}

To evaluate the proposed projection-based framework and find particular overperforming cases, it is necessary to involve a big ammount of machine learning models. Firstly, this work compared the projection-based framework with other time series classification approaches, by utilizing the Aeon-toolkit python library \cite{AeonDoc}, with models listed in Table \ref{tab:non_cv_list}. Furthermore, this work combined the proposed method with a wide variet of classification \gls{CV} models. For that, it was used the Pytorch python library \cite{PytorchDoc}, which aggregates a wide variety of neural network design achitectures. Those achitectures varies from simply convolutional networks to vision transformers. The Table \ref{tab:cv_list} lists all \gls{CV} models involved in the experiment. With that, these experiments submited this thesis framework to an abrangent set of scenarios.

\input{tex/tabelas/aeon_listing}

\input{tex/tabelas/cv_listing}

However, defining the models is insuficient, as it still needed the choice of their hyperparameters. Hyparameters are high level parameters that does not change during the training procedure. For the Aeon models, the default hyperparameters provided by the library were chosen. But, for the computational vision model, while most of them were set to default, our experiments employed hyperparameter search for the learning rate hyperparameter, used by the optimizing algorithm. The Optuna python library \cite{OptunaDoc} did that search, by doing heuristic search over the set of all parameters requested dynamically in the user code. That libray prunes ramifications of the search-space tree with a variety of methods, of which our experiments used the median prunning. In our case, the score that guides this heuristic is the accuracy score, the ratio of hits over the number of samples. Our experiments measured it in the validation dataset of size 2, that resulted from a simple random split. This functionallity allowed us to find a near-optimal combination of parameters without testing cases exhaustively, using as heuristic the score of the model in the validation dataset.

\subsection{The training strategy}

Given the before-mentioned models, the dataset and its divisions, It is needed to establish the training method for the models parameters adjustment. Since the Aeon implementation already contained a default training procedure, our experiment only established the fitting framework for the Pytorch computational vision models and the data feeding method. Our experiment feed the models by loading the signal data and, before transforming them, applied random over sampling, since the dataset was unbalanced. After doing the projection transform, our exeperiment loaded pre-trained model weights provided by the Pytorch. This learn transferring originated from training the models in the ImageNet \cite{ImageNet} dataset. Following that, our experiment did the pytorch models fitting using the Adam optimizing algorithm \cite{Adam} to minimize the Cross Entropy Loss function. The implementation of the training strategy did this optimization cycle with an amount of epochs depending on an median-based early stop technique. The formula bellow gives the score of the $n$-th epoch:

\begin{equation}
EarlyStopScore(n) = med([|l_{n-i} - med([l_{n-i}]_{i=0}^9)|]_{i=0}^{9})
\end{equation}

\noindent Where $l_k$ is the loss value (the Cross Entropy Loss) of the $k$-th epoch, ${med}$ is the median and $[f(i)]_{i=0}^k$ is the sequence generated by $f(i)$ when varying $i$ from $0$ to $k$. In other words, the formula calculates the median of the absolute deviation of the medians of the last 10 losses values using the median as the central value. If $EarlyStopScore(n) \leq 0.1$, the training stops in the $n$-th epoch. With that established, it is only left to determine the metrics to be measured.     

\subsection{The measurements}

Finally, we chose the metrics to evaluate objectively efficacy of the solution. For these experiments, we can separate the used metrics into two groups: prediction metrics, which measures the quality of the models signal quality assessments; and benchmarking metrics, which measures the resources usage and the model speed. As the prediction metrics, our experiments used the Cohen kappa score, the F1-score, and the precision. The Cohen kappa score, in binary classification tasks, measures the relation between the obtained accuracy $acc_o$ and the expected accuracy $acc_e$. The following equations define those accuracies and the Cohen Kappa score, in terms of confusion matrix cell values:

\begin{equation} 
acc_o(R) = \frac{TP+TN}{N}
\end{equation}

\begin{equation}
acc_e(R)  = \left(\frac{TP+FP}{N} \cdot \frac{TP+FN}{N}\right) + \left(\frac{TN+FP}{N} \cdot \frac{TN+FN}{N}\right)
\end{equation}

\begin{equation} \label{eq:Cohen Kappa}
\Mathenize{Cohen-Kappa(R)}  = \frac{acc_o(R) - acc_e(R)}{1 - acc_e(R)} 
\end{equation}  

\noindent Where $N=TP+TN+FP+FN$ is the total number of samples and R is the set of pairs $\{(f(X),y') | \forall (X,y') \in Dataset\}$, where $f$ is the predictor. For the purpose of aligning this metric with others, we can rescale that metric from $[-1,1]$ to $[0,1]$:

\begin{equation}
\Mathenize{Cohen-Kappa-Rescaled(R)} = \frac{\Mathenize{Cohen-Kappa(R)}+1}{2} 
\end{equation}  

In sequence, the Precision is a metric that measures the ratio of hits in the set of positive predictions. In our context, a higher Precision imply that the predictor approved a low amount of ``Bad'' signals, which is desirable in applications where we do not want to show to the user measurements based on unreliable signals. From the Precision and from the Recall, the ratio of hits in the set of all existing positives, we can obtain the F1-Score. Precisely, the F1-Score is the harmonic mean between those two metrics. In other words, a high F1-Score indicates a good balance between Precision and Recall scores. In our application, it measures the same as the Precision plus the Recall, which would measure the amount of ``Good'' signals that would feed the application. This is an desirable quality when we want to provide constant feedback to the user. The following equations define those metrics:

\begin{equation} \label{eq:Precision}
Precision = \frac{TP}{TP+FP}
\end{equation}


\begin{equation} \label{eq:Recall}
Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation} \label{eq:F1-Score}
\Mathenize{F1-Score}  = \Mathenize{Harmonic-Mean}(Precision,Recall) = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
\end{equation}

\noindent Therefore, the Cohen Kappa provides an overall sense of accuracy, the F1-Score suggest the usability level of the model and the Precision indicates the security of the predictor. 

Regarding the benchmarking metrics, our experiment measured the memory usage of the model in bytes and the inference time (added to the 1d-to-2d projection time for the projection-based models) in seconds. The memory measurement is important since practical applications of heart rate estimations often imposes hardware constraints that limits the allowed memory occupation. Additionally, the inference time is desirable for an almost-instant evaluation of the result, making the application more responsive.

\subsection{Overall schema}

\input{tex/figuras/framework}

The Figure \ref{fig:framework} expresses the experiment framework for the \gls{CV} models. A first observation is that the main difference from the framework applied to non-\gls{CV} models is that the 1d to 2d conversion acts as a frontier between the dataset and the rest of the components. That means the non-\gls{CV} models experiment can be expressed using almost the same schema by removing that conversion block. With that, the experiment began by the hyperparameters selection, splitting the \gls{BUTPPG} dataset using simple division for the purpose of, subsequentially, selecting the best hyperparameters of the \gls{CV}. With the best chosen hyperparameters, all models, including the non-\gls{CV} models, will be evaluated using the \gls{LOSO} strategy. For each fold, our experiments subjected the model to a training procedure that repeats epochs of training-and-validating until the early stopping interferes. Then, the model is tested, producing the metrics for that fold.

\subsection{The implementation details}

Presented the general concepts, this section discusses some details, following the practical sequence of events. It is proper to begin describing the dataset sourcing procedure. Our implementation done that sourcing by using the Pytorch multithreading data feeding solution, named Data Loader\footnote{Documentation available at \url{https://pytorch.org/docs/stable/data.html\#torch.utils.data.DataLoader}}. Our implementation configured it to load in batches of size 32. But, before loading the batches, our implementation did the training dataset balancing using the Imbalanced Learn library \cite{ImbalancedLearn} solution of the random oversampling\footnote{Documentation available at \url{https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\#imblearn.over_sampling.RandomOverSampler}}. Then, the batches are loaded transforming the signal into its images using the projection algorithms of the PyTS Python library \cite{PyTS}. Even though the signals are now 2d, the width, height and number of channels dimensions are incompatible with the networks inputs. To solve this, our implementation applied the Pytorch resize transform\footnote{Documentation available at \url{https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html}} to adjust the width and the height. Finally, our implementation added a new convolutional layer corresponding to the \gls{Mix} method.

The \gls{CV} models were trained using a single NVIDIA RTX 3090 TI. For training, I used the Pytorch implementation of the Adam optmization algorithm\footnote{Documentation avaliable at \url{https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\#torch.optim.Adam}}, which uses the gradients evaluated by the Pytorch autograd engine \cite{Pytorch}. The loss class (which, in our case, is the Cross Entropy Loss\footnote{Documentation avaliable at \url{https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\#torch.nn.CrossEntropyLoss}}) backpropagates the gradients based of the model foward pass errors. For the models testing, I used the Sklearn \cite{Sklearn} implementation of the metrics\footnote{Documentation at \url{https://scikit-learn.org/stable/modules/classes.html\#module-sklearn.metrics}}. For model memory measurement, I counted the summation of the size of each parameter and buffer tensors in the \gls{CV} models, while for the non-\gls{CV} models, I used the asizeof function\footnote{Documentation at \url{https://pympler.readthedocs.io/en/latest/library/asizeof.html}} of the Pympler library \cite{Pympler}. Finally, I descibe the inference time measurement, for which I extracted 500 measurement samples.  For the non-\gls{CV} models, I used the Python time method\footnote{Documentation at \url{https://docs.python.org/3/library/time.html\#time.time}}, of the time library, to measure instants of time. For the \gls{CV} models, I marked the time instants by using CUDA events interface provided by Pytorch\footnote{Documentation at \url{https://pytorch.org/docs/stable/generated/torch.cuda.Event.html}}, while, before measuring, performing 500 iterations to warm-up the GPU. 

\section{Experimental results}

In this section, I analyze the results will by comparing the metrics in the following order: firstly, I verify if the reescaled Cohen Kappa Score value is superior o equal to 90\%. Secondly, the F1-Score; and, lastly, the Precision score. Moreover, I considered the trade-off between models with respect to the memory consumption and the inference time, in that order. Since I considered a large set of models, I did the comparisions in sections. In each section, I consider one of the six before-presented computer vision model classifications: Transformers, Residual Nets, Mobile Oriented, Extreme Nets, Efficiency Oriented, and Diverse. In each of those sections, I selected the best combinatons of model variant and projection method, if at least one of them is sufficiently good. Subsequently, I will choose the best non-\gls{CV} models present in the Aeon toolkit. Lastly, I will determine the final best choices, also discussing the differences between the projection methods.

\subsection{Categories analysis}

\subsubsection{Transformers}

I tested four transformers the Vision Transformer, the Multi-axis Vision Transformer, the Shifted Windows Transformer and its second version. The Vision Transformer is the base model of the others. It transforms the visual input into into a word where the letters are linear embedings of subimages obtained by particioning the original image in a grid-like shape. Then, the other modules increments the original by using multiple layers and by changing the attention mechanisms. For instance, the MaxViT employs achitectural blocks where each alternates between two self-atention modes: grid attention, a mode of high granularity, and block attention, a mode of low granularity. The Swin Transformer, on the other hand, changes its attention in layer-level, by merging patches of the antecedent layer into a new one, and in block-level, by shifting the self-attention windows into different positions. Finally, the Swin Transformer V2 propose several specific improvements to the older version.

About the results, we can see on the tables \ref{tab:Averages_of_VisionTransformer}, \ref{tab:Averages_of_Maxvit}, \ref{tab:Averages_of_SwinTransformer}, and \ref{tab:Averages_of_SwinTransformer V2} that only the Swin Transformer V2 and the Vision Transformer variants obtained a Cohen Kappa score superior than 90\%. Specifically, for the Swin Transformer V2, only the standard variant combined with the \gls{Mix} method achieved such a score. For the Vision Transformer, only the large and base variants suprassed the limit. The variants with patch size $32\times 32$ did so by using the \gls{Mix}, while the variants with patch size $16 \times 16$ not only reached the score when combined with \gls{Mix} but also with \gls{RP}. Since both of the models reached a similar score, we observe the benchmarking metrics. We see in the Table \ref{tab:Memory_of_Transformers} that the Swin Transformer V2 S uses considerably less memory than the Vision Transformer. On the other hand, we observe in Figure \ref{fig:Time_of_Transformers} that the Swin Transformer V2 S have a slower inference speed if compared to the Vision Transformer variants. However, because I consider the memory consumption as a factor of major importance, I selected for this section the Swin Transformer V2 standard variant with \gls{Mix}.

\input{tex/tabelas/resultados/averages/Averages_of_Transformers}

\IncludeMemoryTable{Transformers}{Transformers}

\input{tex/figuras/resultados/boxplots/Transformers}

\FloatBarrier

\subsubsection{Residual Nets}

This section analyzes the ResNet itself and its variations. The ResNet is a model that introduced the residual conections, links between non-adjacent layers that bypasses intermediate layers. As its variations, I analyzed two: the Wide ResNet and the ResNeXt. The first one widens the original net by increasing the number of channels per block, with the intention of providing an alternative to increasing the layer depth. The second one employs a multipath philosophy, agreggating the paths by an additive operation. As oposed to the antecedent, it avoids increasing the width and the depth of the network by providing an additional dimension to increase. 

Of those models, only the original ResNet and the Wide ResNet achieved a suficient Cohen Kappa score, as we can see in the tables \ref{tab:Averages_of_ResNet}, \ref{tab:Averages_of_ResNeXt} and \ref{tab:Averages_of_Wide ResNet}. From the ResNet variants, the ones with 50 and 101 layers reached that score. Solely the Wide ResNet with 101 layers and 2 convolutions per block sucessfully score above the limit. Those models variants got those scores by employing the \gls{Mix} projection method. Comparing these last variants, we conclude that the Wide ResNet 101-2 variant with the \gls{Mix} method obtained the best score, both for Cohen Kappa and F1 scores. However, that Wide ResNet posesses the largest memory occupation, according to the table \ref{tab:Memory_of_ResNet based}. It also spends the fourth higher inference time, as seen in the figure \ref{fig:Time_of_ResNet based}. Despite those disavantages, the Wide ResNet 101-2 variant with the \gls{Mix} was the choice of this section, because I prioritize the classification metrics over the benchmarking ones.

\input{tex/tabelas/resultados/averages/Averages_of_ResNet based}

\IncludeMemoryTable{ResNet based}{Residual Nets}

\input{tex/figuras/resultados/boxplots/ResNet based}

\FloatBarrier

\subsubsection{Mobile Oriented}

This section explores the mobile oriented networks, that is, networks designed specifically for mobile hardware constraints. I tested three models: the MNASNet, the Mobile Net V2 and the Mobile Net V3. In cronologial order, the Mobile Net V2 comes first. This newtork proposes several achitectural changes to use less memory while mantaining accuracy, with inverted residual blocks being one of them. This proposed alteration swaps places of the high-channeled and low-channeled layers, which causes the connection between layers with a lower ammount of channels, reducing the number of parameters in that block. After the antecedent, researchers introduced the MNASNet. Its name stands for Mobile Neural Architecture Search. It select blocks to fit in a predefined achitectural skeleton, optimizing the model performance on real-world mobile hardware. Finally, the Mobile Net V3 combined both approaches, while also made changes such as adding the NetAdapt \cite{NetAdapt} algorithm in the achitectural search.

In those set of models, only the MNASNet obtained a sufficient Cohen Kappa score, as visible in the tables \ref{tab:Averages_of_MNASNet}, \ref{tab:Averages_of_MobileNet V2} e \ref{tab:Averages_of_MobileNet V3}. Specifically, the MNASNet with depth multiplier of 1.0, being that multiplier related to the number of channels dimension. It achieved that score combined with the \gls{Mix} projection method method. According to the table \ref{tab:Memory_of_Mobile nets} and the figure \ref{fig:Time_of_Mobile nets}, all of those networks have very small memory size, with values inferior to 30 MB. They also have very fast inference time, with medians bellow to 20 ms. For those reasons and since it was the only net to achieve the minimum accuracy requirements, I chose the MNASNet 1.0 with \gls{Mix}. 

\input{tex/tabelas/resultados/averages/Averages_of_Mobile nets}

\IncludeMemoryTable{Mobile nets}{Mobile Oriented}

\input{tex/figuras/resultados/boxplots/Mobile nets}

\FloatBarrier

\subsubsection{Extreme Nets}

In this section, I discuss the results of neural models which focus on one concept and push it to the limit, such as the VGG, the DenseNet and the SqueezeNet. The VGG is a model that uses filters of size $3\times 3$ to allow adding more layers, increasing the depth model. On the other hand, the DenseNet applies skipping connections of the residual nets to all pairs of achitectural blocks present in the network, to achieve the benefits of having each layer closer to the input and the output of the model. Finally, the SqueezeNet tries to reach minimum memory usage not only by applying model compression techiniques but by introducing a new achitectural module which reduces the number of channels of a layer before another one with large convolutions filters, such as $3 \times 3$ filters. This reduces the number of parameters considerably.

Accordingly to the tables \ref{tab:Averages_of_DenseNet}, \ref{tab:Averages_of_VGG} and \ref{tab:Averages_of_AlexNet}, only the SqueezeNet and the VGG achieved the minimum Cohen Kappa score. For the Squeezenet, only the version 1.1, a more economic version if compared to the 1.0, reached the score, when combined to the \gls{RP} and the \gls{Mix}, with the \gls{Mix} giving the best score. The second was the VGG with 16 weight layers without batch normalization, when combined to the \gls{RP} projection method. Of those combinations, the SqueezeNet 1.1 with \gls{Mix} performed better than the others, while having less variance. Adding to that performance, the SqueezeNet 1.1 is the smalest model in memory, according to the table \ref{tab:Memory_of_Extreme models}. Furthermore, the SqueezeNets have low inference time if copared to the other models, as the Figure \ref{fig:Time_of_Extreme nets} depicts. Therefore, I selected for this section the SqueezeNet 1.1 with \gls{Mix}.

\input{tex/tabelas/resultados/averages/Averages_of_Extreme models}

\IncludeMemoryTable{Extreme models}{Extreme Nets}

\input{tex/figuras/resultados/boxplots/Extreme nets}

\FloatBarrier

\subsubsection{Efficiency Oriented}

In this setion I discuss the models that researchers designed to efficiently utilize resources. For this work, I elaborate on the ShuffleNet V2, EfficientNet and EfficientNet V2. The first one is the ShuffleNet V2, sucessor of the ShuffleNet, which introduced the channel shuffle operator to allow information exchange among channels. It changes its predecessor by proposing, for each block, a channel split operation, to avoid costly operators such as grouped convolutions. Beside the shuffle nets, we have the efficient nets. The EfficientNet researchers studied the model scaling and created a compound resizing method that proportionally increase multiple dimensions, such as depth, number of channels and resolution. This allowed to create a very efficient base model that the researchers scaled up to bigger variants that mantain the advantages of the original. Finnaly, the EfficientNet V2 reformulated the antecessor by proposing a non-proportional scaling and by employing network architectural search. Additionally, it employed progressive learning by gradually increasing the dataset regularization.  

We observe in the tables \ref{tab:Averages_of_ShuffleNet V2}, \ref{tab:Averages_of_EfficientNet}, and \ref{tab:Averages_of_EfficientNetV2} that none of the EfficientNet variants accomplished the minimum Cohen Kappa Score. Only a few variants of the other models remained. For the ShuffleNet V2, we see that the variants with $\times 1$ and $\times 0.5$ channels were the remainders, when combined with the \gls{Mix} method. For the EfficientNet V2, the unique variant that I tested, we observe that only the \gls{Mix} method allowed the minimum score. The scores that the ShuffleNet V2 variants reached are similar to the ones of the EfficientNet V2. However, the ShuffleNet V2 variants are smaller than the EfficientNet V2, as we can visualize in the Table \ref{tab:Memory_of_Efficiency Oriented}. Furthermore, the Figure \ref{fig:Time_of_Efficiency Oriented} shows that the ShuffleNet V2 variants are generally faster than the EfficientNet V2. For those reasons, I chose the smallest variant, ShuffleNet V2 $\times 0.5$, combined with the \gls{Mix} projection method. 

\input{tex/tabelas/resultados/averages/Averages_of_Efficiency Oriented}

\input{tex/figuras/resultados/boxplots/Efficiency Oriented}

\IncludeMemoryTable{Efficiency Oriented}{Efficiency Oriented}

\FloatBarrier


\subsubsection{Diverse}

In this section, I analyze the remaining models. They are: AlexNet, ConvNeXt and RegNet. Researchers introduced the first one in the old year of 2012. That net tested the idea of deep learning trained on multiple GPUs, allowing a faster training. Furthermore, it applyied dropout to reduce overfitting. In contrast, the ConvNeXt is a modern model of 2022, which reunites several convolutional techiniques of the past years, such as patchfied convolutions, inverted bottlenecks and grouped convolutions. The researchers proposed that model to push the state of traditional convolutional networks to the limit. Differently of the antecedents, the RegNet employs the ideia of designing network spaces rather than individual nets. Those populations are caracterized by the restriction of the parameter space to a linear function. This created populations of networks that are more apropriate for random achitectural search. In this manner, I am testing networks with big differences among them.

Amid those models, only the RegNet and the AlexNet passed the minimum Cohen Kappa test, as we can infer from the tables \ref{tab:Averages_of_AlexNet}, \ref{tab:Averages_of_ConvNeXt}, and \ref{tab:Averages_of_RegNet}. The AlexNet did so by applying the \gls{Mix} method for its only variant. The RegNet, on the other hand, has a plenty of variants, that originate from two network design spaces: X, the base space, and Y, which adds a squeeze-and-excitation operation after the blocks of X. From those variants, only the 3.2 GF (Giga FLOPs) and 800 MF (Mega FLOPs) variants of the X network space and only the 400 MF and 800 MF variants of the Y network space reached the score. Most of them did so by applying the \gls{Mix}, with only one exception: The RegNet Y 400 MF did by using the \gls{RP}. Since the variants of both models scored similarly, we observe the Figure \ref{fig:Time_of_Diverse} and the Table \ref{tab:Memory_of_Diverse}. The first observation is that the the AlexNet occupies, at least, two times more memory than most of the RegNet variants, including the ones with good score. The second one is that the AlexNet has faster inference speed than every model in this section. But, since the memory occupation is priority, I chose the RegNet over the AlexNet. Specifically, the RegNet Y 400 MF variant (with \gls{RP}), because it is the smallest.  

\input{tex/tabelas/resultados/averages/Averages_of_Diverse}

\input{tex/figuras/resultados/boxplots/Diverse}

\IncludeMemoryTable{Diverse}{Diverse}

\FloatBarrier

\subsection{Non-CV models comparison}

This section evaluate non-computationally-visual models, which I presented in the chapter \ref{Revisao_bibliografica}. Firstly, we observe in the Table \ref{tab:Averages_of_Non CV} that seven models achieved the minimum Cohen Kappa score. Among them only two achieved the greatest scores: the Random Interval Spectral Ensemble Classifier and the Temporal Dictionary Ensemble. But they got similar scores, so I refer to the Table \ref{tab:Memory_of_Non CV} and Figure \ref{fig:Time_of_Non CV} to decide which to choose. It is visible that the Random Interval Spectral Ensemble Classifier is faster in inference time than the Temporal Dictionary Ensemble, while the later is smaller in memory size. Due to the memory being a factor of major priority, I chose the Temporal Dictionary Ensemble. 	 

\input{tex/tabelas/resultados/averages/Averages_of_Non CV}

\IncludeMemoryTable{Non CV}{Non CV}

\input{tex/figuras/resultados/boxplots/Non CV}

\FloatBarrier

\subsection{Best models comparison}

This section aggregates all choices that I made through all past sections. Observing their Table \ref{tab:Averages_of_Best Models}, we see that the two models obtained the best Cohen Kappa scores: the Temporal Dictionary Ensemble and the Wide ResNet 100-2 with \gls{Mix}, even though the other models obtained comparable score. That Wide ResNet obtained the highest Cohen Kappa score, but there are some considerations. Firstly, that model is the largest of this set, as we can read on table \ref{tab:Memory_of_Best Models}. Secondly, it did not obtained the best F1 score and Precision. Lastly, the Wide ResNet is the second slowest model as we can refer to the Figure \ref{fig:Time_of_Best Models}. On the other hand, the Temporal Dictionary Ensemble was the best in the usabiliy and security scores, F1-Score and Precision. It also is the fastest in inference and the smallest in memory. Therefore, I conclude that the \gls{CV} approach lead to a better accuracy, represented by the Wide ResNet 100-2 with \gls{Mix}, but the non-\gls{CV} approach can be more useful, since it uses less resources and is less inconvenient to the user.


\input{tex/tabelas/resultados/averages/Averages_of_Best Models}

\IncludeMemoryTable{Best Models}{Best Models}

\input{tex/figuras/resultados/boxplots/Best Models}

\section{Limitations}

Even though the results are promising, we need to consider a series of limitations present in my experiment. One main observation is that I used only the BUTPPG dataset for testing. This has a series of implications in our context. Firstly, even though the proposed method allowed the use of \gls{CV} models with a good performance in the BUTPPG dataset, the same could not necessarily be concluded for different datasets. This is because different methods of measurement, sensor qualities, and signal lengths could lead to alterations on the obtained performance. Furthermore, the small size of the dataset resulted in a low ammount of testing, which makes the obtained results less trustable. A small dataset size also implies that the deep learning models had less resources to effectively learn. This contasts with the usual treatment for deep learning, where usually large ammounts of data feed the training of the model, allowing the proper adjustment of the large set of parameters. Therefore, my experiment would be more complete if I tested on different and larger datasets.

Other limit is that I did not explored all the options avaliable, in terms of models and hyperparameters. For instance, I did left out some of the models of the Pytorch and the Aeon libraries. Examples of them are the GoogLeNet \cite{GoogLeNet}, from the Pytorch, and the Hydra Classifier \cite{HydraClassifier}, from the Aeon. Additionally, I did not tested models external to those libraries, such as the Xception \cite{Xception}, avaliable at the Keras library \cite{Keras}. Also, for some of the tested models, I did not evaluated all the variants, such as the Efficient Net B7. Furthermore, I did not search over the hypeparameters of the projection methods, as the number of dimensions of the \gls{RP}. Thus, various options could be explored, as the different libraries that implement machine learning models, while the currenly used ones could be better availed. Moreover, I could do Optuna hyperparameter search over the projection hyperparameters.

Finally, some other limitations were evident at the implementation level. Firstly, I only did data random oversampling for the purpose of balacing the dataset. However, there are other options designed specifically for time series, such as presented in \cite{TimeSeriesAugmentation}. With that, I could not only balance the dataset but enlarge it as well. Secondly, I used a resize transform to adapt the projection image to the model input. This could cause a considerable loss of information for matrix images that encode relationships in each pixel. As a consequence, the \gls{CV} models probably did not performed as good as they could. Thirdly, there was no research for the early stop method that I proposed. There is a chance that this method prematurely stopped the training for models that required more epochs. Lastly, I did the measurements of the benchmarking metrics by using the Python standard API, which is not a direct method of measuring them, but is limited to the interpreter. Additionally, I did not control the environment where measured the inference time. This could imply that external users have scheduled tasks that competed with mine measurements. Therefore, I could have done much improvements at the implementation level, such as applying time series augmentation techiniques, resizing without distorting the image by using an integer multiplier and padding the remainig space, researching early stopping methods, and measuring the benchmarking metrics in more controled environment and using low-level interfaces.        
