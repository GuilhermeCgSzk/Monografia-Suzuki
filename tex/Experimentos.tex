%The tests were performed using the \gls{BUTPPG}~\cite{butppg}. In such a database, it was utilized a smartphone to record 48 \acrshort{ppg} signals of 12 \acrlong{sbjs} (\acrshort{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. This database can be consumed through Physionet~\cite{physionet} interface via \texttt{wfdb} package. The proposed method was implemented in Python. Implementations of \acrshort{gaf}, \acrshort{mtf} and \acrshort{rp} were provided by PyTS \cite{pyts} library. The PyTorch library~\cite{pytorch} was used to perform the training and the classification operations. The models used in this study are listed in Table~\ref{tab:results} and provided by TorchVision. Hyper-parameters optimization was performed using the Optuna library~\cite{optuna}. For each model, 50 Optuna trials were performed for fitting and validating the \acrshort{ml} model with median pruner to avoid excessive computation of trial epochs that do not show hope of better results.

%After the optimization, the metrics of the best trial were evaluated in the testing dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \acrshort{ml} model in hands, the dataset was divided into folds using the cross-validation \acrfull{loso} re-sampling method, into pieces matching each of the 12 \acrshort{sbjs}. For each fold, the smaller split was used as the testing dataset for the evaluation of the model's metrics, while the bigger split was subdivided into the training dataset, of size 7 \acrshort{sbjs}; and into the validation dataset, of size 4 \acrshort{sbjs}. Applying such an experimental setup allowed the generation of results concerning the metrics present in Table~\ref{tab:results}, where, for each projection method, all models were seen as samples of a statistics population possessing 5 values corresponding to the mean of all 12 folds of each metric.

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \acrshort{sbjs}; validation, with 2 \acrshort{sbjs}; and test, with 3 \acrshort{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \acrshort{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%In order to evaluate those projection methods, the following metrics were considered accuracy score, the proportion of hits $Accuracy = \frac{(TP+TN)}{TP+TN+FP+FN}$; precision score, the proportion of correct positive guesses, $Precision = \frac{TP}{TP+FP}$; recall score, the proportion of found positive samples, $Recall = \frac{TP}{TP+FN}$; and F1 score, the harmonic mean between the precision score and the recall score, $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$. Additionally, it was measured the Cohen Kappa score, the degree of agreement of annotators for a classification problem, $CohenKappa = 1-\frac{1-p_o}{1-p_e}$, where $p_o$ is the proportion of observed concordance and $p_e$ is the probability of concordance between all annotators. In the supervised binary classification domain, only two annotators, corresponding to the predicted and the real labels, and two classes, corresponding to positive or negative, are considered, in a manner that the confusion matrix can be used directly to evaluate the score, by the formula $CohenKappa=\frac{2\cdot (TP\cdot TN - FP \cdot FN)}{(TP+FP)\cdot(FP+TN)+(TP+FN)\cdot(FN+TN)}$.

% Those metrics were evaluated for 70 \acrshort{ml} models implemented in \href{https://pytorch.org/}{PyTorch}, the ones listed on Table~ \ref{tab:results}
% For such networks, the same experiment framework was applied, highlighting three major process blocks: dataset construction, when the database was loaded and transformed; hyperparameters selection, when, for each \acrshort{ml} model, a search was done to try to find the set of its best hyperparameters for the built dataset; and projection metrics evaluation, when each model, in conjunction with its set of best hyper-parameters found, was tested, generating the metrics present in this article. %(the framework is shown in figure \ref{figure:framework})
% Also, hyper-parameters selection and projections evaluation apply training and testing cycles.  To clarify those processes, they will be described in detail in the following sections.


%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\linewidth]{imgs/framework2.pdf}
%    \caption{Flowchart representing the experimental framework. Gray elements are entities, while blue elements are processes.}
%    \label{figure:framework}
%\end{figure*}

% \subsection{Training and Testing}

% Machine learning tasks involve 2 main steps: training and testing. Such a format was applied several times on the hyper-parameters selection process, for testing certain sets of hyper-parameters; and the projections evaluation process, for evaluating the model and its set of hyper-parameters efficiency. That core task was done using \href{https://pytorch.org/}{PyTorch} python library, which automatically computes the gradients based on the user's implementation \cite{pytorch}. Also, it allows parallel GPU processing to be done, by applying tensor-based operations to batches of the dataset, which size used in this experiment was the whole dataset. 

% The training process parameters also involve an optimizer and and loss function. The optimizer algorithm to be used was Adam\cite{adam}, while the Cross-Entropy Loss function was used, %\cite{cross-entropy-loss} 
% both implemented in \href{https://pytorch.org/}{PyTorch}. Moreover, the training process needs to stop at some point, fact that was done using an early stopping approach, computing the median absolute deviation, deviation metric robust to outliers, %\cite{?}
% of a window of the last 10 loss function values on the validation dataset, stopping the training process when such windows converge to, at least, a deviation of value 1. When the training process stopped, the best result found was chosen.

% \subsection{Dataset construction}

% In order to provide data to train and test \acrshort{ml} models, the Brno University of Technology Smartphone PPG Database was used as a starting point. In such a database, it was utilized a smartphone to record 48 \acrshort{ppg} signals of 12 \acrlong{sbjs} (\acrshort{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. Additionally, other refinement procedures were done to perfect that database, such as cropping 20 seconds of the recording, leaving the middle 10 seconds intact\cite{butppg}. And, finally, the database is publicly available on Physionet through the link \url{https://physionet.org/content/butppg/1.0.0/}. 

% However, the database is in \acrfull{wfdb} format \cite{butppg}, one dimensional signal format in function of time, %\cite{wfdb}
% letting yet to be done the application of projection methods. For such purpose, PyTS was used, a Python package developed for time series classification, containing the desired projection algorithms, \acrshort{gaf}, \acrshort{mtf} and \acrshort{rp} \cite{pyts}, with its implementations of PyTS version 0.13.0 stored in \href{https://zenodo.org/}{Zenodo} repository \cite{pyts-v0.13.0}.
    
% With such a package, it was possible to produce a dataset containing all projections as 2D images with 3 channels with shape $3 \times siglen^2 \times siglen^2$, where $siglen$ is the signal length. For each projection, its matrix was repeated on every channel of the image, while the \gls{PM} filled every channel with each of the 3 mentioned projections. Finally, it was necessary to resize every image for each \acrshort{ml} model input shape, for such purpose that it was used \href{https://pytorch.org/}{PyTorch} resize transform with bi-linear interpolation. After those procedures, the dataset was ready for the following processes, as the hyper-parameters selection.

% \subsection{Hyper-parameters selection}

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \acrshort{sbjs}; validation, with 2 \acrshort{sbjs}; and test, with 3 \acrshort{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \acrshort{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%\subsection{Projection Metrics Evaluation}

%After the optimization, the metrics of the best trial were evaluated in the test dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \acrshort{ml} model in hands, the dataset was divided into folds using the cross-validation \acrfull{loso} re-sampling method, into pieces matching each of the 12 \acrshort{sbjs}. For each fold, the left split was used as the test dataset for the evaluation of the model's metrics, while the other split was subdivided into the train dataset, of size 7 \acrshort{sbjs}; and into the valid dataset, of size 4 \acrshort{sbjs}. Applying such an experimental setup allowed to generate results with respect to the before-mentioned metrics, where, for each projection method, all models are seen as samples of a statistics population possessing 5 values corresponding to the metrics.

%Figure~\ref{fig:boxplots} depicts presents the boxplot of Accuracy, F1-score, Precision, Recall, and Cohen's Kappa distributions for different \glspl{CV} models tested on the \gls{BUTPPG} database. From these graphs, we can notice that, for the recall score, all projections are very equated. Moreover, it is noticeable that \acrshort{rp} and the proposed \gls{PM} have improved performances when compared with other \gls{mtf} and \gls{gaf} projections, especially when we observe the minimum, maximum, and the first quartiles of distributions, presented in Table~\ref{tab:boxplots}. From this table, we can perceive, at first glance, that the \gls{rp} and the \gls{PM} are generally superior methods if compared with the other two. In fact, \acrshort{rp} and \gls{PM} first quartiles are greater than \acrshort{gaf} and \acrshort{mtf} third quartiles considering all metrics except Recall Score, which shows that 75\% of samples in \acrshort{rp} and \gls{PM} are generally greater than 75\% of samples in \acrshort{gaf} and \acrshort{mtf}. The detailed performance of the points in the distributions considered in Figure~\ref{fig:boxplots} is depicted per model in Table~\ref{tab:results}.

%\input{tables/results}

% Each of the 4 populations produced box plots shown in figure \ref{fig:boxplots}, allowing, at first glance, to see the Recurrence Plot and the \gls{PM} as generally superior methods if compared with the other two. In fact, \acrshort{rp} and P\gls{PM} first quartiles are greater than \acrshort{gaf} and \acrshort{mtf} third quartiles considering all metrics except Recall Score, what shows that 75\% of samples in \acrshort{rp} and \gls{PM} are generally greater than 75\% of samples in \acrshort{gaf} and \acrshort{mtf}.    %TODO: falar do Cohen Cappa Score 

\section{Experimental Setup}

In this section, the experimental setup will be discussed, analyzing elements used in the experiment, such as datasets, programming libraries and predictive models, and clarifying metrics to be evaluated and approaches to measure them.  

\subsection{Experiment components}

\subsubsection{The dataset}

As for every machine learning task, a dataset is needed to provide data to feed the predictive models for their parameters fitting, molding them to the domain of the specific task. In this work, the task of assessing the quality of the signal is cleary a supervised classification problem, that is, can be described as the problem of finding a function that best defines a set of pairs of variables and label, $(X,y)$, which, in this scope, corresponds to the signal itself mapped to its quality label, ``Good'' or ``Bad''. For the purpose of training the predictive methods and evaluating their ability to fit to the problem of classifying the quality of heartbeat time series, it was employed the \acrshort{BUTPPG}~\cite{butppg} dataset.

The \acrfull{BUTPPG}, in essence, is a publicly available database produced by the Department of Biomedical Engineering of the the Brno University of Technology containing samples of \acrshort{PPG} signals, its quality labels and its heat rate estimations. Those signals were extracted using a low-cost method: recording the subject finger with the camera of a smartphone. To be more precise, it was placed the index finger in a way that the camera and its led were covered; it was recorded; it measured, for each video frame, the average of the red channel of every image pixel, resulting in a time series of averages; and, finally, the signal was inverted.  

This method of obtaining \acrshort{PPG} signals was done 48 times, ammount distributed equally between 12 subjects, that is, for each subject, 4 measurements were done. Moreover, those recordings were done in two possible situations: one which the subject was sat down and static, case which the quality label ``Good'' is probable; and other which the subject was walking, hence, likely to be a ``Bad'' recording. That distinction is relevant, since the walking situation occurred only 1 time for each subject, biasing the labels proportion to be nearly 25\% of ``Bad'' ones. Therefore, this dataset is imbalanced, factor that need to be handled in the monography experiment.

As for the definition of the signal quality labels, specialists were designated to estimate the heart rate associated with the \acrshort{PPG} signals, with the help of a software specialized for the analysis. Then, the number they gave were compared with the one given by a gold standard method that, instead of using the \acrshort{PPG} signal, used an ECG recording, which was syncronized mannually. If their measurement error was less or equal than 5 bpm, then their estimation were considered correct. Finally, if 3 specialists of 5 gave correct estimations, the quality of the \acrshort{PPG} signal was considered correct. Thus, the quality labels assigned to the signals were obtained by a low-biased method. 

\subsubsection{The models}

In order to evaluate the proposed projection-based framework, it's necessary to combine it with various classification models which, in that case, are computational vision based. Such a kind of models were implemented in the pytorch python library, in which a wide variety of neural network design strategies were used, as pure convolutional networks, which use the convolution operation to extract features reducing 2d images into smaller ones; residual networks, which uses bypass links between further layers; transformers, that combines the convolution operation with language processing self attention mecanisms; parameter optmized networks, which focuses on having the best parameters selection; models optimized for mobile applications; and more. A complete list of all models involved in the experiment follows:

\begin{itemize}
	\item Diverse
		\begin{itemize}
		    	\item AlexNet
		    	\item ConvNeXt
		    	\item ShuffleNet V2  
			\item VGG	
		\end{itemize}

	\item  Transformers
		\begin{itemize}
			\item VisionTransformer
			\item MaxVit
			\item SwinTransformer
		\end{itemize}
	
	\item  Residual nets
		\begin{itemize}
			\item ResNet
			\item ResNeXt
			\item Wide ResNet
			\item DenseNet
		\end{itemize}
		
	\item Parameter efficiency
		\begin{itemize}
		    	\item EfficientNet
		    	\item EfficientNetV2
		    	\item RegNet
    			\item SqueezeNet  
		\end{itemize}
	
	\item Mobile nets
		\begin{itemize}
			\item MNASNet
			\item MobileNet V2
			\item MobileNet V3
		\end{itemize}
	
\end{itemize}

\subsubsection{The training strategy}




\input{tex/tabelas/resultados/averages}

\input{tex/figuras/resultados/violinplots}

\input{tex/figuras/resultados/boxplots/time}

\input{tex/tabelas/resultados/memory}
