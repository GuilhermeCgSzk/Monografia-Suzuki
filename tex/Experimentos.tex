%The tests were performed using the \gls{BUTPPG}~\cite{butppg}. In such a database, it was utilized a smartphone to record 48 \gls{ppg} signals of 12 \gls{sbjs} (\gls{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking~\cite{butppg}. This database can be consumed through Physionet~\cite{physionet} interface via \texttt{wfdb} package. The proposed method was implemented in Python. Implementations of \gls{gaf}, \gls{mtf} and \gls{rp} were provided by PyTS~\cite{pyts} library. The PyTorch library~\cite{pytorch} was used to perform the training and the classification operations. The models used in this study are listed in Table~\ref{tab:results} and provided by TorchVision. Hyper-parameters optimization was performed using the Optuna library~\cite{optuna}. For each model, 50 Optuna trials were performed for fitting and validating the \gls{ml} model with median pruner to avoid excessive computation of trial epochs that do not show hope of better results.

%After the optimization, the metrics of the best trial were evaluated in the testing dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \gls{ml} model in hands, the dataset was divided into folds using the cross-validation \gls{loso} re-sampling method, into pieces matching each of the 12 \gls{sbjs}. For each fold, the smaller split was used as the testing dataset for the evaluation of the model's metrics, while the bigger split was subdivided into the training dataset, of size 7 \gls{sbjs}; and into the validation dataset, of size 4 \gls{sbjs}. Applying such an experimental setup allowed the generation of results concerning the metrics present in Table~\ref{tab:results}, where, for each projection method, all models were seen as samples of a statistics population possessing 5 values corresponding to the mean of all 12 folds of each metric.

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning~\cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \gls{sbjs}; validation, with 2 \gls{sbjs}; and test, with 3 \gls{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \gls{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%In order to evaluate those projection methods, the following metrics were considered accuracy score, the proportion of hits $Accuracy = \frac{(TP+TN)}{TP+TN+FP+FN}$; precision score, the proportion of correct positive guesses, $Precision = \frac{TP}{TP+FP}$; recall score, the proportion of found positive samples, $Recall = \frac{TP}{TP+FN}$; and F1 score, the harmonic mean between the precision score and the recall score, $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$. Additionally, it was measured the Cohen kappa score, the degree of agreement of annotators for a classification problem, $Cohenkappa = 1-\frac{1-p_o}{1-p_e}$, where $p_o$ is the proportion of observed concordance and $p_e$ is the probability of concordance between all annotators. In the supervised binary classification domain, only two annotators, corresponding to the predicted and the real labels, and two classes, corresponding to positive or negative, are considered, in a manner that the confusion matrix can be used directly to evaluate the score, by the formula $Cohenkappa=\frac{2\cdot (TP\cdot TN - FP \cdot FN)}{(TP+FP)\cdot(FP+TN)+(TP+FN)\cdot(FN+TN)}$.

% Those metrics were evaluated for 70 \gls{ml} models implemented in \href{https://pytorch.org/}{PyTorch}, the ones listed on Table~ \ref{tab:results}
% For such networks, the same experiment framework was applied, highlighting three major process blocks: dataset construction, when the database was loaded and transformed; hyperparameters selection, when, for each \gls{ml} model, a search was done to try to find the set of its best hyperparameters for the built dataset; and projection metrics evaluation, when each model, in conjunction with its set of best hyper-parameters found, was tested, generating the metrics present in this article. %(the framework is shown in figure \ref{figure:framework})
% Also, hyper-parameters selection and projections evaluation apply training and testing cycles.  To clarify those processes, they will be described in detail in the following sections.


%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\linewidth]{imgs/framework2.pdf}
%    \caption{Flowchart representing the experimental framework. Gray elements are entities, while blue elements are processes.}
%    \label{figure:framework}
%\end{figure*}

% \subsection{Training and Testing}

% Machine learning tasks involve 2 main steps: training and testing. Such a format was applied several times on the hyper-parameters selection process, for testing certain sets of hyper-parameters; and the projections evaluation process, for evaluating the model and its set of hyper-parameters efficiency. That core task was done using \href{https://pytorch.org/}{PyTorch} python library, which automatically computes the gradients based on the user's implementation~\cite{pytorch}. Also, it allows parallel GPU processing to be done, by applying tensor-based operations to batches of the dataset, which size used in this experiment was the whole dataset. 

% The training process parameters also involve an optimizer and and loss function. The optimizer algorithm to be used was Adam\cite{adam}, while the Cross-Entropy Loss function was used, %\cite{cross-entropy-loss} 
% both implemented in \href{https://pytorch.org/}{PyTorch}. Moreover, the training process needs to stop at some point, fact that was done using an early stopping approach, computing the median absolute deviation, deviation metric robust to outliers, %\cite{?}
% of a window of the last 10 loss function values on the validation dataset, stopping the training process when such windows converge to, at least, a deviation of value 1. When the training process stopped, the best result found was chosen.

% \subsection{Dataset construction}

% In order to provide data to train and test \gls{ml} models, the Brno University of Technology Smartphone PPG Database was used as a starting point. In such a database, it was utilized a smartphone to record 48 \gls{ppg} signals of 12 \gls{sbjs} (\gls{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking~\cite{butppg}. Additionally, other refinement procedures were done to perfect that database, such as cropping 20 seconds of the recording, leaving the middle 10 seconds intact\cite{butppg}. And, finally, the database is publicly available on Physionet through the link \url{https://physionet.org/content/butppg/1.0.0/}. 

% However, the database is in \gls{wfdb} format~\cite{butppg}, one dimensional signal format in function of time, %\cite{wfdb}
% letting yet to be done the application of projection methods. For such purpose, PyTS was used, a Python package developed for time series classification, containing the desired projection algorithms, \gls{gaf}, \gls{mtf} and \gls{rp}~\cite{pyts}, with its implementations of PyTS version 0.13.0 stored in \href{https://zenodo.org/}{Zenodo} repository~\cite{pyts-v0.13.0}.
    
% With such a package, it was possible to produce a dataset containing all projections as 2D images with 3 channels with shape $3 \times siglen^2 \times siglen^2$, where $siglen$ is the signal length. For each projection, its matrix was repeated on every channel of the image, while the \gls{PM} filled every channel with each of the 3 mentioned projections. Finally, it was necessary to resize every image for each \gls{ml} model input shape, for such purpose that it was used \href{https://pytorch.org/}{PyTorch} resize transform with bi-linear interpolation. After those procedures, the dataset was ready for the following processes, as the hyper-parameters selection.

% \subsection{Hyper-parameters selection}

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning~\cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \gls{sbjs}; validation, with 2 \gls{sbjs}; and test, with 3 \gls{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \gls{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%\subsection{Projection Metrics Evaluation}

%After the optimization, the metrics of the best trial were evaluated in the test dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \gls{ml} model in hands, the dataset was divided into folds using the cross-validation \gls{loso} re-sampling method, into pieces matching each of the 12 \gls{sbjs}. For each fold, the left split was used as the test dataset for the evaluation of the model's metrics, while the other split was subdivided into the train dataset, of size 7 \gls{sbjs}; and into the valid dataset, of size 4 \gls{sbjs}. Applying such an experimental setup allowed to generate results with respect to the before-mentioned metrics, where, for each projection method, all models are seen as samples of a statistics population possessing 5 values corresponding to the metrics.

%Figure~\ref{fig:boxplots} depicts presents the boxplot of Accuracy, F1-score, Precision, Recall, and Cohen's kappa distributions for different \glspl{CV} models tested on the \gls{BUTPPG} database. From these graphs, we can notice that, for the recall score, all projections are very equated. Moreover, it is noticeable that \gls{rp} and the proposed \gls{PM} have improved performances when compared with other \gls{mtf} and \gls{gaf} projections, especially when we observe the minimum, maximum, and the first quartiles of distributions, presented in Table~\ref{tab:boxplots}. From this table, we can perceive, at first glance, that the \gls{rp} and the \gls{PM} are generally superior methods if compared with the other two. In fact, \gls{rp} and \gls{PM} first quartiles are greater than \gls{gaf} and \gls{mtf} third quartiles considering all metrics except Recall Score, which shows that 75\% of samples in \gls{rp} and \gls{PM} are generally greater than 75\% of samples in \gls{gaf} and \gls{mtf}. The detailed performance of the points in the distributions considered in Figure~\ref{fig:boxplots} is depicted per model in Table~\ref{tab:results}.

%\input{tables/results}

% Each of the 4 populations produced box plots shown in figure \ref{fig:boxplots}, allowing, at first glance, to see the Recurrence Plot and the \gls{PM} as generally superior methods if compared with the other two. In fact, \gls{rp} and P\gls{PM} first quartiles are greater than \gls{gaf} and \gls{mtf} third quartiles considering all metrics except Recall Score, what shows that 75\% of samples in \gls{rp} and \gls{PM} are generally greater than 75\% of samples in \gls{gaf} and \gls{mtf}.    %TODO: falar do Cohen Cappa Score 


In Chapter~\ref{Metodologia}, we introduced the proposed technique for assessing \gls{PPG} signals. In line with the goals outlined in Chapter~\ref{Introducao}, the aim of the current chapter is to investigate the effects of different methods on predicting whether a given input \gls{PPG} signal is reliable or not. To achieve this aim, the chapter first presents the experimental setup, then discusses the experimental results, and finally addresses the limitations of the experiments.



\section{Experimental setup}

This section discusses the experimental setup, analyzing the elements used in the experiments, such as datasets, programming libraries, and predictive models. Additionally, it clarifies the metrics to be evaluated and the approaches used to measure them.

\subsection{The dataset}

As with any machine learning task, we require a dataset to supply data for feeding the predictive models during parameter fitting, shaping them to the specific task's domain. In this work, assessing the quality of the signal is framed as a supervised classification problem, which can be described as the task of finding a function that best fits a predefined set of pairs of variables and labels, $(X, y)$. In this context, the pair corresponds to the signal mapped to its quality label, either `Good' or `Bad'. To train the predictive methods and evaluate their performance in classifying the quality of heartbeat time series, the \gls{BUTPPG}~\cite{butppg} dataset was employed.

%\input{tex/figuras/samples}

The \gls{BUTPPG} is a publicly available database produced by the Department of Biomedical Engineering at Brno University of Technology. It contains samples of \gls{PPG} signals, their quality labels, and heart rate estimations. These signals were extracted using a low-cost method: recording with a smartphone camera. Specifically, the researchers recorded the subject's index finger while it covered the camera lens and its \gls{LED} light. For each video frame, they measured the average intensity of the red channel across all image pixels, resulting in a time series of averages. Finally, the signal was inverted. %The Figure \ref{fig:butppg_samples} shows examples of the results of such a sequence of procedures.  

They performed this method of obtaining \gls{PPG} signals 48 times, with the samples distributed equally among 12 subjects—4 measurements per subject. Moreover, the recordings were taken in two situations: one where the subject was seated and remained static, a case in which the quality label `Good' was likely; and another where the subject was walking, a case likely to result in a `Bad' recording. This distinction is relevant because the walking condition occurred only once for each subject, resulting in approximately 25\% of the recordings being labeled as `Bad'. Therefore, this dataset is imbalanced, a factor that was addressed in our experiment.

For the definition of signal quality labels, specialists were designated to estimate the heart rate associated with the \gls{PPG} signals using specialized software developed by the researchers. The organizers then compared the specialists' estimates with those provided by a gold standard method, which used an ECG recording as a reference instead of the \gls{PPG} signal. The ECG was manually synchronized by the measurer. If the specialist's measurement had an error of 5 bpm or less, the estimate was considered correct. Finally, if 3 out of 5 specialists provided correct estimates, the \gls{PPG} signal quality was labeled as `Good.' Thus, the `Good' labels in the dataset essentially indicate whether a signal is human-readable.

\subsection{The dataset split}

Machine learning tasks also require the dataset to be split into fragments. One of these is the training dataset, used for fitting the model's parameters. Another is the test dataset, used for evaluating the model's efficiency. An optional split is the validation dataset, used to select the best set of hyperparameters for the trained model. In our experiment, the training-test splits were defined using a cross-validation method called \gls{LOSO}, which partitions the dataset into $K$ train-test splits. The $k$-th train-test split assigns the $k$-th segment as the test dataset, leaving the remaining $K-1$ segments as the training dataset. In the case of \gls{BUTPPG}, $K$ equals 12, which corresponds to the number of subjects. Notably, the smallest unit of division is the subject, not the individual signals associated with each subject. This approach has the advantage of increasing the distinction between training and testing samples. Since the dataset is small, this splitting method maximizes the use of available resources by ensuring every sample is used as a test case at least once, without introducing bias into the results. Additionally, the training dataset was further divided to produce a validation dataset of size 3, the usage of which will be explained later.

\subsection{The models}

To evaluate the proposed projection-based framework and identify specific cases of superior performance, it was necessary to involve a large number of machine learning models. Firstly, this work compared the projection-based framework with other time series classification approaches using the Aeon-toolkit Python library~\cite{AeonDoc}, with the models listed in Table~\ref{tab:non_cv_list}. Furthermore, the proposed method was combined with a wide variety of classification \gls{CV} models, utilizing the PyTorch Python library~\cite{PytorchDoc}, which supports a diverse range of neural network architectures. These architectures vary from simple convolutional networks to vision transformers. Table~\ref{tab:cv_list} lists all the \gls{CV} models involved in the experiment. This approach subjected the framework to an extensive set of experimental scenarios.


\input{tex/tabelas/aeon_listing}

\input{tex/tabelas/cv_listing}

However, defining the models alone is insufficient, as the selection of their hyperparameters is also required. Hyperparameters are high-level parameters that do not change during the training process. For the Aeon models, the default hyperparameters provided by the library were used. For the computational vision models, while most hyperparameters were set to their defaults, our experiments employed hyperparameter search for the learning rate, used by the optimization algorithm. The Optuna Python library~\cite{OptunaDoc} conducted this search by heuristically exploring the parameter space dynamically defined in the user code. Optuna prunes the search-space tree using various methods, and in our experiments, the median pruning method was applied. In this case, the guiding metric for the heuristic search was the accuracy score, defined as the ratio of correct predictions to the total number of samples. This score was measured on a validation dataset of size 2, created through a simple random split. This functionality allowed us to find a near-optimal combination of parameters without exhaustively testing all possible cases, using the model's validation dataset score as a heuristic.

\subsection{Training strategy}

Given the aforementioned models, dataset, and its divisions, it was necessary to establish the training method for adjusting the models' parameters. Since the Aeon implementation already contained a default training procedure, our experiment only established the fitting framework for the PyTorch computer vision models and the data feeding method. Our experiment involved feeding the models by loading the signals data, applying random oversampling before transforming them, as the dataset was unbalanced. After performing the projection transform, our experiment loaded pre-trained model weights provided by PyTorch, which were originally trained on the ImageNet~\cite{ImageNet} dataset. Following that, we fitted the PyTorch models using the Adam optimization algorithm~\cite{Adam} to minimize the cross-entropy loss function. The implementation of the training strategy performed this optimization cycle with a number of epochs determined by a median-based early stopping technique. The formula below gives the score of the $n$-th epoch:
\begin{equation}
EarlyStopScore(n) = med([|l_{n-i} - med([l_{n-i}]_{i=0}^9)|]_{i=0}^{9})
\end{equation}
\noindent Where $l_k$ is the loss value (i.e., cross-entropy loss) of the $k$-th epoch, ${med}$ is the median and $[f(i)]_{i=0}^k$ is the sequence generated by $f(i)$ when varying $i$ from $0$ to $k$. In other words, the formula calculates the median of the absolute deviations of the medians of the last 10 loss values using the central value. If $EarlyStopScore(n) \leq 0.1$, the training stops in the $n$-th epoch. With that established, it remains to determine the metrics to be measured for smoother readability.

\subsection{Performance measurements}

We chose the metrics to evaluate the efficacy of the solution. For these experiments, we can categorize the metrics into two groups: prediction metrics, which measure the quality of the model's signal quality assessments, and benchmarking metrics, which measure resource usage and the model speed. As the prediction metrics, our experiments used the Cohen kappa score, the F1-score, and the precision. The Cohen kappa score, in binary classification tasks, measures the agreement between the obtained accuracy $acc_o$ and the expected accuracy $acc_e$. The following equations define those accuracies and the Cohen kappa score, in terms of confusion matrix cell values:
\begin{equation} 
acc_o(R) = \frac{TP+TN}{N},
\end{equation}
\begin{equation}
acc_e(R)  = \left(\frac{TP+FP}{N} \cdot \frac{TP+FN}{N}\right) + \left(\frac{TN+FP}{N} \cdot \frac{TN+FN}{N}\right),
\end{equation}
and
\begin{equation} \label{eq:Cohen kappa}
\Mathenize{Cohen-kappa(R)}  = \frac{acc_o(R) - acc_e(R)}{1 - acc_e(R)} 
\end{equation}  
\noindent Where $N=TP+TN+FP+FN$ is the total number of samples and R is the set of pairs $\{(f(X),y') | \forall (X,y') \in Dataset\}$, where $f$ is the predictor. For the purpose of aligning this metric with others, we can rescale that metric from $[-1,1]$ to $[0,1]$:
\begin{equation}
\Mathenize{Cohen-kappa-Rescaled(R)} = \frac{\Mathenize{Cohen-kappa(R)}+1}{2} 
\end{equation}  
In sequence, the Precision is a metric that measures the ratio of hits in the set of positive predictions. In our context, a higher Precision imply that the predictor approved a low amount of ``Bad'' signals, which is desirable in applications where we do not want to show to the user measurements based on unreliable signals. From the Precision and from the Recall, the ratio of hits in the set of all existing positives, we can obtain the F1-Score. Precisely, the F1-Score is the harmonic mean between those two metrics. In other words, a high F1-Score indicates a good balance between Precision and Recall scores. In our application, it measures the same as the Precision plus the Recall, which would measure the amount of ``Good'' signals that would feed the application. This is an desirable quality when we want to provide constant feedback to the user. The following equations define those metrics:
\begin{equation} \label{eq:Precision}
Precision = \frac{TP}{TP+FP},
\end{equation}
\begin{equation} \label{eq:Recall}
Recall = \frac{TP}{TP+FN},
\end{equation}
and
\begin{equation} \label{eq:F1-Score}
\Mathenize{F1-Score}  = \Mathenize{Harmonic-Mean}(Precision,Recall) = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}.
\end{equation}
\noindent Therefore, the Cohen kappa score provides an overall sense of accuracy, the F1-Score suggests the model's usability level, and precision indicates the predictor's reliability.

Regarding the benchmarking metrics, our experiment measured the memory usage of the model in bytes and the inference time (including the 1D-to-2D projection time for projection-based models) in seconds. Memory usage is crucial because practical applications for heart rate estimation often impose hardware constraints that limit allowable memory usage. Additionally, inference time is important for achieving near-instantaneous evaluations, which enhances the application's responsiveness.

\subsection{Overall schema}

\input{tex/figuras/framework}

The Figure~\ref{fig:framework} illustrates the experiment framework for the \gls{CV} models. One notable difference from the framework used for non-\gls{CV} models is that the 1D-to-2D conversion acts as a boundary between the dataset and the other components. This means that the experiment for non-\gls{CV} models can be represented using a similar schema by omitting the conversion block. The experiment began with hyperparameter selection, involving the splitting of the \gls{BUTPPG} dataset through a simple division method to subsequently select the optimal hyperparameters for the \gls{CV} models. With the best hyperparameters chosen, all models, including non-\gls{CV} models, will be evaluated using the \gls{LOSO} strategy. For each fold, our experiments subjected the model to a training procedure that iterates through epochs of training and validation until early stopping is triggered. The model is then tested to produce the metrics for that fold.
\subsection{The implementation details}

The dataset sourcing procedure was carried out using the PyTorch multithreading data feeding solution, Data Loader\footnote{Documentation available at \url{https://pytorch.org/docs/stable/data.html\#torch.utils.data.DataLoader}}. This was configured to load batches of size 32. Prior to loading the batches, the training dataset was balanced using the Imbalanced Learn library~\cite{ImbalancedLearn} and its random oversampling method\footnote{Documentation available at \url{https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\#imblearn.over_sampling.RandomOverSampler}}. The batches were then transformed from 1D signals into 2D images using the projection algorithms of the PyTS library~\cite{PyTS}. Although the signals are now 2D, their width and height might not be compatible with the original network's input dimensions, especially considering pre-trained weights. To address this issue, the PyTorch resize transform\footnote{Documentation available at \url{https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html}} was applied to adjust the width and height. Additionally, a new convolutional layer corresponding to the \gls{PMix} method was incorporated.

The \gls{CV} models were trained using a single NVIDIA RTX 3090 TI. For training, our implementation used the Pytorch implementation of the Adam optimization algorithm\footnote{Documentation available at \url{https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\#torch.optim.Adam}}, which uses the gradients evaluated by the Pytorch autograd engine~\cite{Pytorch}. The loss class (which, in our case, is the torch.nn.CrossEntropyLoss\footnote{Documentation available at \url{https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\#torch.nn.CrossEntropyLoss}}) backpropagates the gradients based on the model forward pass errors. For the models testing, our implementation used the Sklearn~\cite{Sklearn} metrics\footnote{Documentation at \url{https://scikit-learn.org/stable/modules/classes.html\#module-sklearn.metrics}}. For model memory measurement, our implementation counted the summation of the size of each parameter and buffer tensors in the \gls{CV} models, while for the non-\gls{CV} models, our implementation used the \texttt{asizeof} function\footnote{Documentation at \url{https://pympler.readthedocs.io/en/latest/library/asizeof.html}} of the Pympler library~\cite{Pympler}. Finally, we describe the inference time measurement, for which our implementation extracted 500 measurement samples. For the non-\gls{CV} models, our implementation used the Python time method\footnote{Documentation at \url{https://docs.python.org/3/library/time.html\#time.time}}, of the time library, to measure instants of time. For the \gls{CV} models, our implementation marked the time instants by using CUDA events interface provided by Pytorch\footnote{Documentation at \url{https://pytorch.org/docs/stable/generated/torch.cuda.Event.html}}, while, before measuring, performing 500 iterations to warm-up the GPU. 

\section{Experimental results}

The results were analyzed by comparing the score metrics of the models and assessing their trade-offs with respect to memory consumption and inference time. Given the large number of models considered, the analysis was organized into sections. First, each section focused on one of the \gls{CV} model families listed in Table~\ref{tab:cv_list}: Transformers, Residual Nets, Mobile-Oriented, Extreme Nets, Efficiency-Oriented, and Diverse. Within each category, the analysis identified the best combinations of model variants and projection methods. Subsequently, the top non-\gls{CV} models from the Aeon toolkit library were selected. Finally, the overall best choices were determined, and differences between the projection methods were discussed.


\subsection{Analysis by computer vision model family}
 
This analysis covers each \gls{CV} model family listed in Table~\ref{tab:cv_list}.
 
\subsubsection{Transformers}

The experiments tested four transformers: \gls{ViT}, \gls{MaxViT}, \gls{SwinT}, and its second version, \gls{SwinTV2}. The \gls{ViT} serves as the base model for the others, transforming visual input into a sequence where each element is a linear embedding of subimage patches obtained by partitioning the original image into a grid-like pattern. Subsequent models build on this base by incorporating additional layers and altering attention mechanisms. For example, the \gls{MaxViT} utilizes architectural blocks that alternate between two self-attention modes: grid attention, which operates with high granularity, and block attention, which operates with low granularity. The \gls{SwinT} modifies attention at both the layer level—by merging patches from the previous layer—and at the block level—by shifting self-attention windows to different positions. The \gls{SwinTV2} introduces several specific improvements over the earlier version.


One metric table was generated for each type of transformer.
% Vision Transformer
The Table \ref{tab:Averages_of_VisionTransformer} presents the \gls{ViT} scores, with variants categorized as Base (B), Large (L), or Huge (H) in parameter size and patch sizes of 14, 16, or 32. The table shows that the \gls{PMix} and \gls{RP} projection methods achieved the best scores across all metrics. In most cases, \gls{PMix} was equal to or better than \gls{RP}, except for the \mbox{H 14} variant, where \gls{RP} was superior. Among the combinations of variants and projections, \gls{RP} and \gls{PMix} with \mbox{B 16} and \mbox{L 16}, as well as \gls{PMix} with \mbox{B 32} and \mbox{L 32}, yielded the best scores.
% MaxViT
Table \ref{tab:Averages_of_Maxvit} shows the \gls{MaxViT} scores.  For this model, the \gls{PMix} method achieved the highest scores for the Cohen Kappa and Precision metrics, while the \gls{RP} surpassed it for the F1-Score, despite \gls{PMix} having the smallest dispersion for that metric.
% Swin Transformer
Table \ref{tab:Averages_of_SwinTransformer} exhibits the \gls{SwinT} scores, with variants categorized as Base (B), Small (S), or Tiny (T) based on parameter count. The \gls{RP} method achieved the best scores for the B and S variants, while the \gls{PMix} method resulted in better scores for the T variant. Specifically, the \gls{PMix} method with the T variant attained the highest Cohen Kappa and Precision scores, but ranked second for the F1-Score, which was surpassed by the \gls{RP} method with the S variant.


% Swin Transformer V2
Table \ref{tab:Averages_of_SwinTransformer V2} displays the \gls{SwinTV2} scores, with variants categorized as Base (B), Small (S), or Tiny (T). The \gls{PMix} method achieved better scores for the B and S variants, while the \gls{RP} method performed better for the T variant, despite \gls{RP} having the largest dispersion for the F1-Score in this case. Specifically, the \gls{PMix} method with the S variant resulted in the highest Cohen Kappa and F1 scores, and the second-best Precision, where the \gls{RP} method with the T variant was superior.
% Inter-Family Comparison
When considering all tables \ref{tab:Averages_of_VisionTransformer}, \ref{tab:Averages_of_Maxvit}, \ref{tab:Averages_of_SwinTransformer}, and \ref{tab:Averages_of_SwinTransformer V2}, the \gls{RP} and \gls{PMix} methods with \mbox{\gls{ViT} B 16} and \mbox{\gls{ViT} L 16}, as well as \gls{PMix} with \mbox{\gls{ViT} B 32} and \mbox{\gls{ViT} L 32}, and the \gls{SwinTV2} S with \gls{PMix}, generally achieved better scores. The benchmarking metrics for these combinations are summarized next.

Table~\ref{tab:Memory_of_Transformers} shows that the \gls{SwinT} V2 S variant uses considerably less memory than the \gls{ViT} variants. Therefore, the \gls{SwinT} V2 S with \gls{PMix} can achieve high scores while utilizing less memory. However, Figure~\ref{fig:Time_of_Transformers} indicates that the \gls{SwinT} V2 S variant has a slower inference speed compared to the \gls{ViT} variants. Among the \gls{ViT} variants, the \mbox{B 32} variant was the fastest, suggesting that the combination of \gls{PMix} with \gls{ViT} B 32 can produce high scores with lower inference time.


% \sout{
% About the results, we can see on the tables \ref{tab:Averages_of_VisionTransformer}, \ref{tab:Averages_of_Maxvit}, \ref{tab:Averages_of_SwinTransformer}, and \ref{tab:Averages_of_SwinTransformer V2} that only the Swin Transformer V2 and the Vision Transformer variants obtained a Cohen kappa score superior than 90\%. Specifically, for the Swin Transformer V2, only the standard variant combined with the \gls{PMix} method achieved such a score. For the Vision Transformer, only the large and base variants suprassed the limit. The variants with patch size $32\times 32$ did so by using the \gls{PMix}, while the variants with patch size $16 \times 16$ not only reached the score when combined with \gls{PMix} but also with \gls{RP}. Since both of the models reached a similar score, we observe the benchmarking metrics. We see in the Table \ref{tab:Memory_of_Transformers} that the Swin Transformer V2 S uses considerably less memory than the Vision Transformer. On the other hand, we observe in Figure \ref{fig:Time_of_Transformers} that the Swin Transformer V2 S have a slower inference speed if compared to the Vision Transformer variants. However, because I consider the memory consumption as a factor of major importance, I selected for this section the Swin Transformer V2 standard variant with \gls{PMix}.
% }

\input{tex/tabelas/resultados/averages/Averages_of_Transformers}

\IncludeMemoryTable{Transformers}{Transformers}

\input{tex/figuras/resultados/boxplots/Transformers}

\FloatBarrier

\subsubsection{Residual nets}

The ResNet model and its variations are analyzed. ResNet introduced residual connections, which are links between non-adjacent layers that bypass intermediate layers. The two variations considered are Wide ResNet and ResNeXt. Wide ResNet expands the original network by increasing the number of channels per block, offering an alternative to increasing layer depth. In contrast, ResNeXt employs a multipath approach, aggregating paths through an additive operation. Instead of increasing width and depth, ResNeXt introduces an additional dimension for enhancement.


The experiments produced three score tables.
% ResNet
Table~\ref{tab:Averages_of_ResNet} presents the ResNet scores for variants with 18, 34, 50, 101, and 152 layers. Among these, the \gls{PMix} and \gls{RP} methods outperformed the other projection methods. Specifically, the \gls{PMix} method achieved the best scores when combined with the 50 and 101-layer variants.
% ResNeXt
Table~\ref{tab:Averages_of_ResNeXt} displays the ResNeXt scores for variants with 50 or 101 layers, cardinality of 32 or 64, and bottleneck width of 4 or 8. Among these, the \gls{PMix} and \gls{RP} methods achieved the best scores. Notably, the \gls{RP} method with the \mbox{ResNeXt 101 $32\times 8d$} variant achieved the highest scores.
% Wide ResNet
Table \ref{tab:Averages_of_Wide ResNet} lists the Wide ResNet scores for variants with 50 or 101 layers and a widening factor of 2. The \gls{PMix} and \gls{RP} methods consistently performed better across all metrics. Notably, the \gls{PMix} method with the Wide \mbox{ResNet 101-2} variant achieved the best scores for Cohen kappa and F1-Score, and the second-best score for Precision.
% Inter-Family Comparison
Observing Tables~\ref{tab:Averages_of_ResNet}, \ref{tab:Averages_of_ResNeXt}, and \ref{tab:Averages_of_Wide ResNet} together reveals that the \mbox{Wide ResNet 101-2} with \gls{PMix} was the top-performing combination in terms of scoring. However, this combination had the highest memory usage and was the fourth slowest in inference time. An alternative with nearly the second-best scores but significantly lower memory usage and inference time is the \mbox{ResNet 50} with \gls{PMix}.


% \sout{
% Of those models, only the original ResNet and the Wide ResNet achieved a suficient Cohen kappa score, as we can see in the tables \ref{tab:Averages_of_ResNet}, \ref{tab:Averages_of_ResNeXt} and \ref{tab:Averages_of_Wide ResNet}. From the ResNet variants, the ones with 50 and 101 layers reached that score. Solely the Wide ResNet with 101 layers and 2 convolutions per block sucessfully score above the limit. Those models variants got those scores by employing the \gls{PMix} projection method. Comparing these last variants, we conclude that the Wide ResNet 101-2 variant with the \gls{PMix} method obtained the best score, both for Cohen kappa and F1 scores. However, that Wide ResNet posesses the largest memory occupation, according to the table \ref{tab:Memory_of_ResNet based}. It also spends the fourth higher inference time, as seen in the figure \ref{fig:Time_of_ResNet based}. Despite those disavantages, the Wide ResNet 101-2 variant with the \gls{PMix} was the choice of this section, because I prioritize the classification metrics over the benchmarking ones.
% }

\input{tex/tabelas/resultados/averages/Averages_of_ResNet based}

\IncludeMemoryTable{Residual Nets}{Residual nets}

\input{tex/figuras/resultados/boxplots/ResNet based}

\FloatBarrier

\subsubsection{Mobile-oriented}

Mobile-oriented networks are designed specifically to address mobile hardware constraints. Three models were tested: MNASNet, MobileNet V2, and MobileNet V3. MobileNet V2, introduced first, incorporates architectural changes to reduce memory usage while maintaining accuracy, including inverted residual blocks. This alteration swaps high- and low-channel layers, connecting layers with fewer channels and thus reducing the number of parameters in the block. MNASNet selects blocks to fit a predefined architectural skeleton, optimizing model performance on real-world mobile hardware. MobileNet V3 combines these approaches and introduces additional changes, such as incorporating the NetAdapt~\cite{NetAdapt} algorithm into the architectural search.


Three metric tables were generated for mobile-oriented family of \gls{CV} models.
%MNASNet
Table~\ref{tab:Averages_of_MNASNet} records the scores obtained by the MNASNet variants, which can have depth multipliers of 0.5, 0.75, 1.0, or 1.3, affecting the number of channels. Notably, the combination of \mbox{MNASNet 1.0} with \gls{PMix} achieved the best scores across all metrics.
%MobileNet V2
Table~\ref{tab:Averages_of_MobileNet V2} presents the scores for MobileNet V2. The \gls{RP} projection achieved the best Cohen kappa and Precision scores, while \gls{PMix} obtained the highest F1-Score. However, \gls{RP} demonstrated greater consistency, with lower variability in results compared to the standard deviations of \gls{PMix}.
%MobileNet V3 
Table~\ref{tab:Averages_of_MobileNet V3} lists the MobileNet V3 variants, which include Small and Large configurations in terms of resource usage. Notably, \gls{PMix} with the Large variant achieved the best Cohen kappa and Precision scores, while \gls{RP} with the Small variant excelled in the F1-Score metric.
% Inter-Family Comparison
From the tables \ref{tab:Averages_of_MNASNet}, \ref{tab:Averages_of_MobileNet V2} and \ref{tab:Averages_of_MobileNet V3}, the combination of \mbox{MNASNet 1.0} with \gls{PMix} emerges as the overall best case. This combination demonstrates a median inference time below 20 ms, as shown in Figure~\ref{fig:Time_of_Mobile nets}, and memory consumption under 15 MB, according to Table~\ref{tab:Memory_of_Mobile-Oriented}.

% \sout{
% In those set of models, only the MNASNet obtained a sufficient Cohen kappa score, as visible in the tables \ref{tab:Averages_of_MNASNet}, \ref{tab:Averages_of_MobileNet V2} e \ref{tab:Averages_of_MobileNet V3}. Specifically, the MNASNet with depth multiplier of 1.0, being that multiplier related to the number of channels dimension. It achieved that score combined with the \gls{PMix} projection method method. According to the table \ref{tab:Memory_of_Mobile nets} and the figure \ref{fig:Time_of_Mobile nets}, all of those networks have very small memory size, with values inferior to 30 MB. They also have very fast inference time, with medians bellow to 20 ms. For those reasons and since it was the only net to achieve the minimum accuracy requirements, I chose the MNASNet 1.0 with \gls{PMix}.}

\input{tex/tabelas/resultados/averages/Averages_of_Mobile nets}

\IncludeMemoryTable{Mobile-Oriented}{Mobile-Oriented}

\input{tex/figuras/resultados/boxplots/Mobile nets}

\FloatBarrier

\subsubsection{Extreme nets}

Neural models that focus on specific concepts, such as layer depth, model compression, or residual connections, include VGG, DenseNet, and SqueezeNet. VGG employs $3\times 3$ filters to allow for deeper network architectures by adding more layers, thus increasing model depth. DenseNet uses skipping connections among all pairs of architectural blocks in the network, which brings each layer closer to both the input and output, enhancing model performance. SqueezeNet focuses on minimizing memory usage through model compression techniques and by introducing a new architectural module that reduces the number of channels in a layer before applying large convolution filters, such as $3 \times 3$ filters. This approach significantly reduces the number of parameters.



Three metric tables were constructed, each corresponding to a model within the \gls{CV} family.
% Dense Net
Table \ref{tab:Averages_of_DenseNet} presents the DenseNet results, where the variants include depths of 121, 161, 169, or 201 layers. For the 169 and 201 layer variants, the \gls{PMix} method achieved superior performance, whereas the \gls{RP} method was the best for the 161 layer variant. For the 121 layer variant, the \gls{PMix} method obtained the highest Cohen kappa score, while the \gls{RP} method excelled in the F1-Score and achieved perfect Precision. Overall, the DenseNet 161 with \gls{RP}, DenseNet 201 with \gls{PMix}, and DenseNet 121 with \gls{RP} achieved the best scores in terms of Cohen kappa, F1-Score, and Precision metrics, respectively. Notably, the DenseNet 161 with \gls{RP} demonstrated a good balance across metrics, attaining the best Cohen kappa score and the second-best F1 and Precision scores.
% SqueezeNet
Table~\ref{tab:Averages_of_SqueezeNet} exhibits the SqueezeNet results for versions 1.0 and 1.1. The optimized version 1.1 achieved the highest scores when paired with the \gls{PMix} method, attaining the best Cohen kappa and F1 scores. When combined with the \gls{RP} method, the optimized version 1.1 achieved the best Precision score. Both combinations demonstrated generally strong performance across all metrics.
% VGG
Table \ref{tab:Averages_of_VGG} details the VGG scores across variants with 11, 13, 16, or 19 layers, with or without Batch Normalization (BN). The \gls{RP} and \gls{PMix} methods achieved the best scores for all variants, though some cases exhibited higher dispersion. Notably, the combination of \mbox{VGG 16} with \gls{RP} excelled in Cohen kappa and Precision metrics, while \mbox{VGG 16 BN} with \gls{PMix} achieved the highest F1-Score. However, the \mbox{VGG 16} with \gls{RP} combination showed considerable dispersion in the F1-Score metric, making \mbox{VGG 16 BN} with \gls{PMix} a more reliable choice.
% Inter-Family Comparison
the combinations \mbox{SqueezeNet 1.1} with \gls{PMix} and \mbox{VGG 16 BN} with \gls{PMix} stand out. Specifically, \mbox{SqueezeNet 1.1} with \gls{PMix} achieved the best Cohen kappa, while \mbox{VGG 16 BN} with \gls{PMix} attained the highest F1-score among all Extreme Nets \gls{CV} family models. The Figure~\ref{fig:Time_of_Extreme nets} illustrates that \mbox{SqueezeNet 1.1} with \gls{PMix} outperforms most other variants in terms of inference speed. Additionally, Table~\ref{tab:Memory_of_Extreme Models} shows that this combination also ranks as the smallest in memory consumption.


% \sout{
% Accordingly to the tables \ref{tab:Averages_of_DenseNet}, \ref{tab:Averages_of_VGG} and \ref{tab:Averages_of_SqueezeNet}, only the SqueezeNet and the VGG achieved the minimum Cohen kappa score. For the Squeezenet, only the version 1.1, a more economic version if compared to the 1.0, reached the score, when combined to the \gls{RP} and the \gls{PMix}, with the \gls{PMix} giving the best score. The second was the VGG with 16 weight layers without batch normalization, when combined to the \gls{RP} projection method. Of those combinations, the SqueezeNet 1.1 with \gls{PMix} performed better than the others, while having less variance. Adding to that performance, the SqueezeNet 1.1 is the smallest model in memory, according to the table \ref{tab:Memory_of_Extreme models}. Furthermore, the SqueezeNets have low inference time if compared to the other models, as the Figure \ref{fig:Time_of_Extreme nets} depicts. Therefore, I selected for this section the SqueezeNet 1.1 with \gls{PMix}.
% }

\input{tex/tabelas/resultados/averages/Averages_of_Extreme models}

\IncludeMemoryTable{Extreme Models}{Extreme nets}

\input{tex/figuras/resultados/boxplots/Extreme nets}

\FloatBarrier

\subsubsection{Efficiency-oriented}

The models designed for efficient resource utilization aim to maximize performance with fewer parameters. This work examined ShuffleNet V2, EfficientNet, and EfficientNet V2. ShuffleNet V2 is an advancement of ShuffleNet, introducing the channel shuffle operator to facilitate information exchange among channels. It improves upon its predecessor by incorporating a channel split operation within each block, which avoids the use of costly grouped convolutions. EfficientNet focuses on model scaling through a compound resizing method that proportionally increases multiple dimensions, such as depth, number of channels, and resolution. This approach creates a highly efficient base model that can be scaled up to larger variants while preserving the original model's advantages. EfficientNet V2 builds on the original EfficientNet by proposing non-proportional scaling and utilizing network architecture search. It also introduces progressive learning, which involves gradually increasing dataset regularization. 

Three score tables were constructed for the Efficiency-Oriented \gls{CV} family.
% EfficientNet
Table~\ref{tab:Averages_of_EfficientNet} lists the results for EfficientNet variants ranging from B0, the smallest, to B4, the largest, in terms of parameter scaling. The \gls{PMix} projection achieved the highest scores for variants B0, B1, and B2. For the B3 variant, the \gls{MTF} method excelled in the F1-score, while the \gls{RP} method performed better for the other metrics. Overall, the combination of \mbox{EfficientNet B1} with \gls{PMix} emerged as the top performer.
% EfficientNetV2
Table~\ref{tab:Averages_of_EfficientNetV2} presents the scores for EfficientNet V2, with the \gls{PMix} projection outperforming all other methods.
% ShuffleNet V2
Table~\ref{tab:Averages_of_ShuffleNet V2} displays the metrics for ShuffleNet V2 variants, which include multipliers of $\times 0.5$, $\times 1.0$, $\times 1.5$, and $\times 2.0$ on the number of channels in each architectural block. Across all variants, the \gls{PMix} projection method outperformed all others. Specifically, the best scores for ShuffleNet V2 were achieved with the $\times 0.5$ and $\times 1.0$ variants combined with the \gls{PMix} method.
% Inter-Family Comparison
When analyzing the results from Tables  \ref{tab:Averages_of_EfficientNet}, \ref{tab:Averages_of_EfficientNetV2}, and \ref{tab:Averages_of_ShuffleNet V2}, it is evident that the usage of \mbox{EfficientNet V2}, \mbox{ShuffleNet V2 $\times 0.5$}, and \mbox{ShuffleNet V2 $\times 1.0$}  with \gls{PMix} achieved high scores across all metrics, including the best Cohen kappa and F1 scores, as well as the second-best Precision. Among these, the \mbox{ShuffleNet V2 $\times 0.5$} with \gls{PMix} was the most efficient in terms of memory usage, as shown in Table~\ref{tab:Memory_of_Efficiency-Oriented}, while being one of the fastest in inference, as visible in the Figure \ref{fig:Time_of_Efficiency Oriented}.


% \sout{
% We observe in the tables \ref{tab:Averages_of_ShuffleNet V2}, \ref{tab:Averages_of_EfficientNet}, and \ref{tab:Averages_of_EfficientNetV2} that none of the EfficientNet variants accomplished the minimum Cohen kappa Score. Only a few variants of the other models remained. For the ShuffleNet V2, we see that the variants with $\times 1$ and $\times 0.5$ channels were the remainders, when combined with the \gls{PMix} method. For the EfficientNet V2, the unique variant that I tested, we observe that only the \gls{PMix} method allowed the minimum score. The scores that the ShuffleNet V2 variants reached are similar to the ones of the EfficientNet V2. However, the ShuffleNet V2 variants are smaller than the EfficientNet V2, as we can visualize in the Table \ref{tab:Memory_of_Efficiency Oriented}. Furthermore, the Figure \ref{fig:Time_of_Efficiency Oriented} shows that the ShuffleNet V2 variants are generally faster than the EfficientNet V2. For those reasons, I chose the smallest variant, ShuffleNet V2 $\times 0.5$, combined with the \gls{PMix} projection method. }

\input{tex/tabelas/resultados/averages/Averages_of_Efficiency Oriented}

\input{tex/figuras/resultados/boxplots/Efficiency Oriented}

\IncludeMemoryTable{Efficiency-Oriented}{Efficiency-oriented}

\FloatBarrier


\subsubsection{Further architectures}

The analysis includes the following models: AlexNet, ConvNeXt, and RegNet. Introduced in 2012, AlexNet was one of the earliest deep learning models designed to be trained across multiple GPUs, which accelerated the training process and utilized dropout to mitigate overfitting. In contrast, ConvNeXt, a modern model from 2022, integrates various convolutional techniques from recent years, such as patchified convolutions, inverted bottlenecks, and grouped convolutions, with the goal of advancing traditional convolutional networks. On the other hand, RegNet departs from designing individual networks by focusing on creating network families defined by linear parameter spaces, facilitating architectural search within these defined populations. The experiments encompassed networks with significant variations among them.


Three score tables were generated for the remaining computer vision models.
% AlexNet
The Table \ref{tab:Averages_of_AlexNet} contains the scores of AlexNet. Notably, the \gls{PMix} projection earned the best scores for all metrics in that table.
% ConvNeXt
Table~\ref{tab:Averages_of_ConvNeXt} presents the results for the ConvNeXt model, with variants including Tiny, Small, Base, and Large in terms of parameter size. Among these variants, the \gls{RP} and \gls{PMix} methods achieved the best scores overall, though some instances, such as \gls{RP} with the Tiny variant, exhibited higher dispersion, particularly for the Cohen kappa score. Specifically, the \gls{PMix} method with the \mbox{ConvNeXt Tiny} variant achieved the highest F1-score, while the \gls{PMix} method with the \mbox{ConvNeXt Small} variant obtained the best Cohen kappa and Precision scores, and also secured the second best F1-score.
% RegNet 
Table \ref{tab:Averages_of_RegNet} exhibits the results for RegNet variants, categorized into RegNetX and RegNetY design spaces~\cite{RegNet}, with varying float operations per second rates such as 400 Mega Flops (MF) or 16 Giga Flops (GF). Among these variants, several achieved scores above the third quartile across all metrics. For the X space, notable cases include \gls{RP} with the 400 MF and 800 MF variants, and \gls{PMix} with the 800 MF, 3.2 GF, 8 GF, and 16 GF variants. For the Y space, noteworthy cases are \gls{RP} with the 400 MF, 1.6 GF, 16 GF, and 32 GF variants, and \gls{PMix} with the 800 MF, 3.2 GF, and 16 GF variants. Among these high-scoring variants, the \gls{PMix} method with the \mbox{RegNet X 3.2 GF}, \mbox{RegNet X 800 MF}, \mbox{RegNet Y 400 MF}, and \mbox{RegNet Y 800 MF} variants achieved the best Cohen kappa and F1 scores, and the third best Precision score. Of these top-performing combinations, the \mbox{RegNet Y 400 MF} with \gls{PMix} had the lowest memory usage, as shown in Table \ref{tab:Memory_of_Diverse}, while the \mbox{RegNet X 800 MF} with \gls{PMix} had the fastest inferences of the model variants, as seen in Figure \ref{fig:Time_of_Diverse}.
% Inter-Family Comparison
When evaluating the top-performing models from each type within the Diverse \gls{CV} family, the \mbox{RegNet Y 400 MF} with \gls{RP}, and the \mbox{RegNet X 800 MF} and AlexNet with \gls{PMix} achieved the highest scores. Notably, the \mbox{RegNet Y 400 MF} with \gls{RP} exhibited the lowest memory usage, as shown in Table~\ref{tab:Memory_of_Diverse}, while AlexNet had the fastest inference times across all projections, as illustrated in Figure~\ref{fig:Time_of_Diverse}.

%\sout{
%Amid those models, only the RegNet and the AlexNet passed the minimum Cohen kappa test, as we can infer from the tables \ref{tab:Averages_of_AlexNet}, \ref{tab:Averages_of_ConvNeXt}, and \ref{tab:Averages_of_RegNet}. The AlexNet did so by applying the \gls{PMix} method for its only variant. The RegNet, on the other hand, has a plenty of variants, that originate from two network design spaces: X, the base space, and Y, which adds a squeeze-and-excitation operation after the blocks of X. From those variants, only the 3.2 GF (Giga FLOPs) and 800 MF (Mega FLOPs) variants of the X network space and only the 400 MF and 800 MF variants of the Y network space reached the score. Most of them did so by applying the \gls{PMix}, with only one exception: The RegNet Y 400 MF did by using the \gls{RP}. Since the variants of both models scored similarly, we observe the Figure \ref{fig:Time_of_Diverse} and the Table \ref{tab:Memory_of_Diverse}. The first observation is that the the AlexNet occupies, at least, two times more memory than most of the RegNet variants, including the ones with good score. The second one is that the AlexNet has faster inference speed than every model in this section. But, since the memory occupation is priority, I chose the RegNet over the AlexNet. Specifically, the RegNet Y 400 MF variant (with \gls{RP}), because it is the smallest.  }

\input{tex/tabelas/resultados/averages/Averages_of_Diverse}

\input{tex/figuras/resultados/boxplots/Diverse}

\IncludeMemoryTable{Diverse}{Diverse}

\FloatBarrier

\subsection{Non-computer vision models comparison}


Table~\ref{tab:Averages_of_Non-CV} presents the scores for non-computationally-visual baseline models. The models Individual \mbox{Ordinal TDE}, Random Interval Classifier, \gls{RISEC}, \mbox{TS Fresh} Classifier, \gls{TDE}, and \mbox{WEASEL V2} achieved high scores across all metrics. Among these, \gls{RISEC} and \gls{TDE} surpassed the other models in every score metric. Specifically, \gls{RISEC} demonstrated faster inference, as shown in Figure \ref{fig:Time_of_Non CV},while \gls{TDE} had lower memory consumption, according to Table~\ref{tab:Memory_of_Non-CV}.


% \sout{
% This section evaluate non-computationally-visual models, which I presented in the chapter \ref{Revisao_bibliografica}. Firstly, we observe in the Table \ref{tab:Averages_of_Non CV} that seven models achieved the minimum Cohen kappa score. Among them only two achieved the greatest scores: the Random Interval Spectral Ensemble Classifier and the Temporal Dictionary Ensemble. But they got similar scores, so I refer to the Table \ref{tab:Memory_of_Non CV} and Figure \ref{fig:Time_of_Non CV} to decide which to choose. It is visible that the Random Interval Spectral Ensemble Classifier is faster in inference time than the Temporal Dictionary Ensemble, while the later is smaller in memory size. Due to the memory being a factor of major priority, I chose the Temporal Dictionary Ensemble.}

\input{tex/tabelas/resultados/averages/Averages_of_Non CV}

\IncludeMemoryTable{Non-CV}{Non-CV}

\input{tex/figuras/resultados/boxplots/Non CV}

\FloatBarrier

\subsection{Comparison of the top-performing models}


The best combinations between models and projections from previous analyses are aggregated in Table~\ref{tab:Averages_of_best models}. The top-performing models include the \gls{TDE}, the \gls{RISEC}, and the \mbox{Wide ResNet 100-2} with \gls{PMix}. While the \mbox{Wide ResNet} achieved the highest Cohen kappa score, it is notable that this model was the second largest in terms of memory usage, as indicated in Table~\ref{tab:Memory_of_Best Models}. Additionally, it did not achieve the highest F1-Score or Precision and was the second slowest in terms of inference speed, as shown in Figure~\ref{fig:Time_of_Best Models}. In contrast, the \gls{TDE} and \gls{RISEC} models excelled in usability and security metrics, including F1-Score and Precision. They also demonstrated superior performance in terms of inference speed and memory efficiency. Consequently, while the \gls{CV} approach, represented by the \mbox{Wide ResNet 100-2} with \gls{PMix}, achieved higher accuracy, the non-\gls{CV} approach, embodied by the \gls{TDE} and \gls{RISEC}, offers better resource efficiency and speed, making it a more practical choice for applications requiring lower resource consumption and faster performance.


\input{tex/tabelas/resultados/averages/Averages_of_Best Models}

\IncludeMemoryTable{Best Models}{best models}

\input{tex/figuras/resultados/boxplots/Best Models}

\section{Limitations}
\label{sec:Limitations}

Despite the promising results, some limitations can be considered. The experiments utilized only the BUTPPG dataset for testing. This has a series of implications in our context. Firstly, even though the proposed method allowed the use of \gls{CV} models with a good performance in the BUTPPG dataset, the same could not necessarily be concluded for different datasets. This is because different methods of measurement, sensor qualities, signal lengths, and individual medical conditions could lead to alterations on the obtained performance. One evidence of that is the absence of confirmed \gls{CA} cases in the \gls{BUTPPG} dataset, which could be present in external data. Furthermore, the small size of the dataset resulted in a reduced testing dataset, which makes the obtained results less general, that is, unreliable when we consider the possible variability of data that is external to the dataset. A small dataset size also implies that the deep learning models had less data samples to effectively learn. This contrasts with the usual treatment for deep learning, where usually large amounts of data feed the training of the model, allowing the proper adjustment of the large set of parameters. Therefore, our experiments would be more complete if our experiments tested on different and larger datasets.

Another constraint is that the experiments did not explore all available options in terms of models. For instance, our experiments left out some of the models of the Pytorch and the Aeon libraries. Examples of them are the GoogLeNet~\cite{GoogLeNet}, from the Pytorch, and the Hydra Classifier~\cite{HydraClassifier}, from the Aeon.  Additionally, models external to these libraries, such as Xception~\cite{Xception} available in the Keras library~\cite{Keras}, were not tested. Furthermore, not all variants of the tested models were evaluated, such as EfficientNet B7. Additionally, the hyperparameters of the projection methods, such as the number of dimensions in \gls{RP}, were not optimized. Exploring a broader range of options, including different libraries and hyperparameter search techniques like Optuna, could yield more comprehensive results.




Finally, additional limitations were identified at the implementation level.  Firstly, the implementation utilized random oversampling to balance the dataset. However, alternative methods specifically designed for time series data, such as those described in~\cite{TimeSeriesAugmentation}, could have been employed.  These methods not only balance the dataset but also can augment it. Secondly, resizing transforms were used to adapt projection images to model inputs, potentially leading to significant loss of information for matrix images that encode pixel relationships. As a consequence, the \gls{CV} models probably did not performed as good as they could. Thirdly, there was no research for the early stop method that our implementation used. Consequently, there is a chance that this method prematurely stopped the training for models that required more epochs.  Lastly, benchmarking metrics were measured using the Python standard API, which may be limited by the interpreter. Additionally, our measurements did not control the environment where measured the inference time. This could imply that external users have scheduled tasks that competed with mine measurements. Therefore, improvements at the implementation level could include applying time series augmentation techniques, resizing images without distortion by using integer multipliers and padding, researching and optimizing early stopping methods, and conducting measurements in a more controlled environment using low-level interfaces.

