%The tests were performed using the \gls{BUTPPG}~\cite{butppg}. In such a database, it was utilized a smartphone to record 48 \acrshort{ppg} signals of 12 \acrlong{sbjs} (\acrshort{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. This database can be consumed through Physionet~\cite{physionet} interface via \texttt{wfdb} package. The proposed method was implemented in Python. Implementations of \acrshort{gaf}, \acrshort{mtf} and \acrshort{rp} were provided by PyTS \cite{pyts} library. The PyTorch library~\cite{pytorch} was used to perform the training and the classification operations. The models used in this study are listed in Table~\ref{tab:results} and provided by TorchVision. Hyper-parameters optimization was performed using the Optuna library~\cite{optuna}. For each model, 50 Optuna trials were performed for fitting and validating the \acrshort{ml} model with median pruner to avoid excessive computation of trial epochs that do not show hope of better results.

%After the optimization, the metrics of the best trial were evaluated in the testing dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \acrshort{ml} model in hands, the dataset was divided into folds using the cross-validation \acrfull{loso} re-sampling method, into pieces matching each of the 12 \acrshort{sbjs}. For each fold, the smaller split was used as the testing dataset for the evaluation of the model's metrics, while the bigger split was subdivided into the training dataset, of size 7 \acrshort{sbjs}; and into the validation dataset, of size 4 \acrshort{sbjs}. Applying such an experimental setup allowed the generation of results concerning the metrics present in Table~\ref{tab:results}, where, for each projection method, all models were seen as samples of a statistics population possessing 5 values corresponding to the mean of all 12 folds of each metric.

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \acrshort{sbjs}; validation, with 2 \acrshort{sbjs}; and test, with 3 \acrshort{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \acrshort{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%In order to evaluate those projection methods, the following metrics were considered accuracy score, the proportion of hits $Accuracy = \frac{(TP+TN)}{TP+TN+FP+FN}$; precision score, the proportion of correct positive guesses, $Precision = \frac{TP}{TP+FP}$; recall score, the proportion of found positive samples, $Recall = \frac{TP}{TP+FN}$; and F1 score, the harmonic mean between the precision score and the recall score, $F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$. Additionally, it was measured the Cohen Kappa score, the degree of agreement of annotators for a classification problem, $CohenKappa = 1-\frac{1-p_o}{1-p_e}$, where $p_o$ is the proportion of observed concordance and $p_e$ is the probability of concordance between all annotators. In the supervised binary classification domain, only two annotators, corresponding to the predicted and the real labels, and two classes, corresponding to positive or negative, are considered, in a manner that the confusion matrix can be used directly to evaluate the score, by the formula $CohenKappa=\frac{2\cdot (TP\cdot TN - FP \cdot FN)}{(TP+FP)\cdot(FP+TN)+(TP+FN)\cdot(FN+TN)}$.

% Those metrics were evaluated for 70 \acrshort{ml} models implemented in \href{https://pytorch.org/}{PyTorch}, the ones listed on Table~ \ref{tab:results}
% For such networks, the same experiment framework was applied, highlighting three major process blocks: dataset construction, when the database was loaded and transformed; hyperparameters selection, when, for each \acrshort{ml} model, a search was done to try to find the set of its best hyperparameters for the built dataset; and projection metrics evaluation, when each model, in conjunction with its set of best hyper-parameters found, was tested, generating the metrics present in this article. %(the framework is shown in figure \ref{figure:framework})
% Also, hyper-parameters selection and projections evaluation apply training and testing cycles.  To clarify those processes, they will be described in detail in the following sections.


%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\linewidth]{imgs/framework2.pdf}
%    \caption{Flowchart representing the experimental framework. Gray elements are entities, while blue elements are processes.}
%    \label{figure:framework}
%\end{figure*}

% \subsection{Training and Testing}

% Machine learning tasks involve 2 main steps: training and testing. Such a format was applied several times on the hyper-parameters selection process, for testing certain sets of hyper-parameters; and the projections evaluation process, for evaluating the model and its set of hyper-parameters efficiency. That core task was done using \href{https://pytorch.org/}{PyTorch} python library, which automatically computes the gradients based on the user's implementation \cite{pytorch}. Also, it allows parallel GPU processing to be done, by applying tensor-based operations to batches of the dataset, which size used in this experiment was the whole dataset. 

% The training process parameters also involve an optimizer and and loss function. The optimizer algorithm to be used was Adam\cite{adam}, while the Cross-Entropy Loss function was used, %\cite{cross-entropy-loss} 
% both implemented in \href{https://pytorch.org/}{PyTorch}. Moreover, the training process needs to stop at some point, fact that was done using an early stopping approach, computing the median absolute deviation, deviation metric robust to outliers, %\cite{?}
% of a window of the last 10 loss function values on the validation dataset, stopping the training process when such windows converge to, at least, a deviation of value 1. When the training process stopped, the best result found was chosen.

% \subsection{Dataset construction}

% In order to provide data to train and test \acrshort{ml} models, the Brno University of Technology Smartphone PPG Database was used as a starting point. In such a database, it was utilized a smartphone to record 48 \acrshort{ppg} signals of 12 \acrlong{sbjs} (\acrshort{sbjs}) index fingers, in such a manner that 3 records were extracted when the subject was sat down and a single record was extracted when he or she was walking \cite{butppg}. Additionally, other refinement procedures were done to perfect that database, such as cropping 20 seconds of the recording, leaving the middle 10 seconds intact\cite{butppg}. And, finally, the database is publicly available on Physionet through the link \url{https://physionet.org/content/butppg/1.0.0/}. 

% However, the database is in \acrfull{wfdb} format \cite{butppg}, one dimensional signal format in function of time, %\cite{wfdb}
% letting yet to be done the application of projection methods. For such purpose, PyTS was used, a Python package developed for time series classification, containing the desired projection algorithms, \acrshort{gaf}, \acrshort{mtf} and \acrshort{rp} \cite{pyts}, with its implementations of PyTS version 0.13.0 stored in \href{https://zenodo.org/}{Zenodo} repository \cite{pyts-v0.13.0}.
    
% With such a package, it was possible to produce a dataset containing all projections as 2D images with 3 channels with shape $3 \times siglen^2 \times siglen^2$, where $siglen$ is the signal length. For each projection, its matrix was repeated on every channel of the image, while the \gls{PM} filled every channel with each of the 3 mentioned projections. Finally, it was necessary to resize every image for each \acrshort{ml} model input shape, for such purpose that it was used \href{https://pytorch.org/}{PyTorch} resize transform with bi-linear interpolation. After those procedures, the dataset was ready for the following processes, as the hyper-parameters selection.

% \subsection{Hyper-parameters selection}

% The hyper-parameters selection can be a computationally expensive process, considering large sets of possible parameters to be searched exhaustively. Hence, Optuna, a hyper-parameter optimization framework based on heuristic search with pruning \cite{optuna}, was used. But, before selecting such parameters, it was needed to separate the dataset in train, with 7 \acrshort{sbjs}; validation, with 2 \acrshort{sbjs}; and test, with 3 \acrshort{sbjs}; splits.  Later, it was optimized, in 50 Optuna trials, using the train dataset for fitting the \acrshort{ml} model and the validation dataset for evaluating the objective function value. Furthermore, it was used Optuna's median pruner to avoid excessive computation of trial epochs that do not show hope of better results, if compared to previous trials. Such an optimization process allowed to find the set of best hyper-parameters for each model, which, in this case, contain only the learning rate. 

%\subsection{Projection Metrics Evaluation}

%After the optimization, the metrics of the best trial were evaluated in the test dataset to assess the selected hyperparameters' process quality. With selected hyper-parameters for each \acrshort{ml} model in hands, the dataset was divided into folds using the cross-validation \acrfull{loso} re-sampling method, into pieces matching each of the 12 \acrshort{sbjs}. For each fold, the left split was used as the test dataset for the evaluation of the model's metrics, while the other split was subdivided into the train dataset, of size 7 \acrshort{sbjs}; and into the valid dataset, of size 4 \acrshort{sbjs}. Applying such an experimental setup allowed to generate results with respect to the before-mentioned metrics, where, for each projection method, all models are seen as samples of a statistics population possessing 5 values corresponding to the metrics.

%Figure~\ref{fig:boxplots} depicts presents the boxplot of Accuracy, F1-score, Precision, Recall, and Cohen's Kappa distributions for different \glspl{CV} models tested on the \gls{BUTPPG} database. From these graphs, we can notice that, for the recall score, all projections are very equated. Moreover, it is noticeable that \acrshort{rp} and the proposed \gls{PM} have improved performances when compared with other \gls{mtf} and \gls{gaf} projections, especially when we observe the minimum, maximum, and the first quartiles of distributions, presented in Table~\ref{tab:boxplots}. From this table, we can perceive, at first glance, that the \gls{rp} and the \gls{PM} are generally superior methods if compared with the other two. In fact, \acrshort{rp} and \gls{PM} first quartiles are greater than \acrshort{gaf} and \acrshort{mtf} third quartiles considering all metrics except Recall Score, which shows that 75\% of samples in \acrshort{rp} and \gls{PM} are generally greater than 75\% of samples in \acrshort{gaf} and \acrshort{mtf}. The detailed performance of the points in the distributions considered in Figure~\ref{fig:boxplots} is depicted per model in Table~\ref{tab:results}.

%\input{tables/results}

% Each of the 4 populations produced box plots shown in figure \ref{fig:boxplots}, allowing, at first glance, to see the Recurrence Plot and the \gls{PM} as generally superior methods if compared with the other two. In fact, \acrshort{rp} and P\gls{PM} first quartiles are greater than \acrshort{gaf} and \acrshort{mtf} third quartiles considering all metrics except Recall Score, what shows that 75\% of samples in \acrshort{rp} and \gls{PM} are generally greater than 75\% of samples in \acrshort{gaf} and \acrshort{mtf}.    %TODO: falar do Cohen Cappa Score 

\section{Experimental Setup}

In this section, the experimental setup will be discussed, analyzing elements used in the experiment, such as datasets, programming libraries and predictive models, and clarifying metrics to be evaluated and approaches to measure them.  

\subsection{The dataset}

As for every machine learning task, a dataset is needed to provide data to feed the predictive models for their parameters fitting, molding them to the domain of the specific task. In this work, the task of assessing the quality of the signal is cleary a supervised classification problem, that is, can be described as the problem of finding a function that best defines a set of pairs of variables and label, $(X,y)$, which, in this scope, corresponds to the signal itself mapped to its quality label, ``Good'' or ``Bad''. For the purpose of training the predictive methods and evaluating their ability to fit to the problem of classifying the quality of heartbeat time series, it was employed the \acrshort{BUTPPG}~\cite{butppg} dataset.

The \acrfull{BUTPPG}, in essence, is a publicly available database produced by the Department of Biomedical Engineering of the the Brno University of Technology containing samples of \acrshort{PPG} signals, its quality labels and its heat rate estimations. Those signals were extracted using a low-cost method: recording the subject finger with the camera of a smartphone. To be more precise, it was placed the index finger in a way that the camera and its led were covered; it was recorded; it measured, for each video frame, the average of the red channel of every image pixel, resulting in a time series of averages; and, finally, the signal was inverted.  

This method of obtaining \acrshort{PPG} signals was done 48 times, ammount distributed equally between 12 subjects, that is, for each subject, 4 measurements were done. Moreover, those recordings were done in two possible situations: one which the subject was sat down and static, case which the quality label ``Good'' is probable; and other which the subject was walking, hence, likely to be a ``Bad'' recording. That distinction is relevant, since the walking situation occurred only 1 time for each subject, biasing the labels proportion to be nearly 25\% of ``Bad'' ones. Therefore, this dataset is imbalanced, factor that need to be handled in the monography experiment.

As for the definition of the signal quality labels, specialists were designated to estimate the heart rate associated with the \acrshort{PPG} signals, with the help of a software specialized for the analysis. Then, the number they gave were compared with the one given by a gold standard method that, instead of using the \acrshort{PPG} signal, used an ECG recording, which was syncronized mannually. If their measurement error was less or equal than 5 bpm, then their estimation were considered correct. Finally, if 3 specialists of 5 gave correct estimations, the quality of the \acrshort{PPG} signal was considered correct. Thus, the quality labels assigned to the signals were obtained by a low-biased method. 

\subsection{The dataset split}

With the dataset in hands and considering the fact that, for machine learning tasks, the dataset need to be separated into a training dataset, used for the models parameters adjustment, a test dataset, used for evaluating the models efficiency, and, optionally, a validation dataset, used to choose the set of best hyperparameters of a trained models. In this experiment case, to define the training-test splits, it was used a cross validation method named \acrfull{LOSO}, in which the dataset is particioned into k pieces and k train-test splits are produced, where the i-th train-test split is made assigning the i-th piece as the test dataset, lefting the other k-1 pieces to be used as the training dataset. In the \acrshort{BUTPPG} case, the k value is 12. In conclusion, a more robust testing method was used in this experiment. Such robustness comes from the fact that it uses every sample in the dataset as a test object at least once, without biasing the results, what would not occurr if a simple split was made, where the train-test separation is unique. Finally the training datset was divided producing a validation dataset of size 3.

\subsection{The models}

In order to evaluate the proposed projection-based framework, besides comparing it with other existing approaches, which was done using the Aeon toolkit python library, it's necessary to combine it with various classification models which, in that case, are computational vision based. Such a kind of models were implemented in the pytorch python library, in which a wide variety of neural network design strategies were used, as pure convolutional networks, which use the convolution operation to extract features reducing 2d images into smaller ones; residual networks, which uses bypass links between further layers; transformers, that combines the convolution operation with language processing self attention mecanisms; parameter optmized networks, which focuses on having the best parameters selection; models optimized for mobile applications; and more. A complete list of all models involved in the experiment follows:

\begin{itemize}
	\item Diverse
		\begin{itemize}
		    	\item AlexNet
		    	\item ConvNeXt
		    	\item ShuffleNet V2  
			\item VGG	
		\end{itemize}

	\item  Transformers
		\begin{itemize}
			\item VisionTransformer
			\item MaxVit
			\item SwinTransformer
		\end{itemize}
	
	\item  Residual nets
		\begin{itemize}
			\item ResNet
			\item ResNeXt
			\item Wide ResNet
			\item DenseNet
		\end{itemize}
		
	\item Parameter efficiency
		\begin{itemize}
		    	\item EfficientNet
		    	\item EfficientNetV2
		    	\item RegNet
    			\item SqueezeNet  
		\end{itemize}
	
	\item Mobile nets
		\begin{itemize}
			\item MNASNet
			\item MobileNet V2
			\item MobileNet V3
		\end{itemize}
	
\end{itemize}

Even though the models were estabilished, it was still missing the choice of their hyperparameters, that is, high level parameters that doesn't change during the training procedure. For the Aeon models, default hyperparameters were set. But, for the computational vision model, while most of them were set to default, hyperparameter search was done for the learning rate hyperparameter, used in the training. That seach was done applying the Optuna python library, which does heuristic search over the values of the model performance on the validation dataset, in terms of accuracy, to try to find the optimal combination of parameters without testing cases exhaustively. With the finallity of reducing the search space, ramifications the search-space tree can be prunned with a variety of methods, of which median prunning was chosen. Therefore, it was possible to search for the best setting of hyperparameters in a reasonable ammount of time.

\subsection{The training strategy}

Given the before-mentioned models, the dataset and its divisions, it is needed to estabilish a training method for the models parameters adjustment. Since the Aeon implementation already contained a default training procedure, it was only necessary to estabilish the fitting framework for the pytorch computational vision models and the data feeding method. The data feeding was implement using the Data Loader pytorch solution, configuring it to load in batches of size 32. However, before constructing those batches, it was applied a random over sampling, since, as already mentioned, the dataset was unbalanced. As for the pytorch models fitting, the Adam optimizer was used to minimize the Cross Entropy loss function, with and ammount of epochs limited by an early stop technique. In such an early stopping, it is evaluated the median of the absolute deviation values of the last 10 epochs centered in their median. If the obtained number absolute value was less or equal than $0.1$, the training stops. With that estabilished, it is only left to determine the metrics to be measured.     

\subsection{The measurements}

Finally, it's necessary to chose the metrics, used to evaluate objectivelly the solution efficacy. The metrics to be used, in this work, can be separated into two groups: prediction metrics, which measures the quality of the models signal quality assessments; and benchmarking metrics, which measures resources usage and viability. The prediction metrics used were the accuracy, which measure the proportion of correct predictions over all that were made; the precision and the recall, which, similarly to the accuracy, measures the right answers, but for the subsets of all ``Good'' predictions and of all ``Good'' samples, respectively; the f1-score, which combine the precision and the recall into one metric by calculating the harmonice mean between them; and, lastly, the Cohen Kappa score, which measures the simillarity between the observed agreement proportion and the expected agreement probability. Those metrics are summarized in ?. Regarding the benchmarking metrics, it was measured the memory usage in bytes of the model and the inference time (added to the 1d-to-2d projection time for the projection-based models) in seconds.

\subsection{The overall schema}

Reviewing what was presented, the experiment will begin by splitting the \acrshort{BUTPPG} dataset using simple division for the purpose of, subsequentially, selecting the best hyperparameters of the \acrshort{CV} models. With the best hyperparameters chosen, all models, including the non \acrshort{CV} models, were evaluated using the \acrshort{LOSO} strattegy, producing the metrics for each fold.


\section{Experimental results}

\subsection{Transformers}

\input{tex/tabelas/resultados/averages/Averages_of_Transformers}


\subsection{ResNet based}

\input{tex/tabelas/resultados/averages/Averages_of_ResNet based}

\subsection{Mobile nets}

\input{tex/tabelas/resultados/averages/Averages_of_Mobile nets}

\subsection{Diverse}

\input{tex/tabelas/resultados/averages/Averages_of_Diverse}


