@article{AlexNet,
	author = {Krizhevsky, A. and Sutskever, I. and Hinton, G. E.},
	title = {ImageNet classification with deep convolutional neural networks},
	year = {2017},
	issue_date = {June 2017},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {60},
	number = {6},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	journal = {Commun. ACM},
	month = {may},
	pages = {84–90},
	numpages = {7}
}

@INPROCEEDINGS{ConvNeXt,
  author={Liu, Z. and Mao, H. and Wu, C. Y. and Feichtenhofer, C. and Darrell, T. and Xie, S.},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A ConvNet for the 2020s}, 
  year={2022},
  volume={},
  number={},
  pages={11966-11976},
  keywords={Computer vision;Image segmentation;Visualization;Computational modeling;Scalability;Semantics;Transformers;Deep learning architectures and techniques; Recognition: detection;categorization;retrieval; Representation learning},
  doi={10.1109/CVPR52688.2022.01167}}
  
@InProceedings{ShuffleNetV2,
author="Ma, N.
and Zhang, X.
and Zheng, H. T.
and Sun, J.",
editor="Ferrari, V.
and Hebert, M.
and Sminchisescu, C.
and Weiss, Y.",
title="ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="122--138",
abstract="Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.",
isbn="978-3-030-01264-9"
}

@inproceedings{VGG,
  author       = {K. Simonyan and
                  A. Zisserman},
  editor       = {Y. Bengio and
                  Y. LeCun},
  title        = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.1556},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{VisionTransformer,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={A. Dosovitskiy and L. Beyer and A. Kolesnikov and D. Weissenborn and X. Zhai and T. Unterthiner and M. Dehghani and M. Minderer and G. Heigold and S. Gelly and J. Uszkoreit and N. Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@InProceedings{MaxViT,
author="Tu, Z.
and Talebi, H.
and Zhang, H.
and Yang, F.
and Milanfar, P.
and Bovik, A.
and Li, Y.",
editor="Avidan, S.
and Brostow, G.
and Ciss{\'e}, M.
and Farinella, G. M.
and Hassner, T.",
title="MaxViT: Multi-axis Vision Transformer",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="459--479",
abstract="Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to ``see'' globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5{\%} ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7{\%} top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. The source code and trained models will be available at https://github.com/google-research/maxvit.",
isbn="978-3-031-20053-3"
}

@INPROCEEDINGS{SwinTransformer,
  author={Liu, Z. and Lin, Y. and Cao, Y. and Hu, H. and Wei, Y. and Zhang, Z. and Lin, S. and Guo, B.},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
  year={2021},
  volume={},
  number={},
  pages={9992-10002},
  keywords={Image segmentation;Computer vision;Visualization;Computational modeling;Semantics;Object detection;Computer architecture;Representation learning;Detection and localization in 2D and 3D;Recognition and classification;Segmentation;grouping and shape},
  doi={10.1109/ICCV48922.2021.00986}}

@INPROCEEDINGS{SwinTransformerV2,
  author={Liu, Z. and Hu, H. and Lin, Y. and Yao, Z. and Xie, Z. and Wei, Y. and Ning, J. and Cao, Y. and Zhang, Z. and Dong, L. and Wei, F. and Guo, B.},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Swin Transformer V2: Scaling Up Capacity and Resolution}, 
  year={2022},
  volume={},
  number={},
  pages={11999-12009},
  keywords={Training;Representation learning;Adaptation models;Image resolution;Computational modeling;Semantics;Benchmark testing;Deep learning architectures and techniques; Representation learning},
  doi={10.1109/CVPR52688.2022.01170}}

@INPROCEEDINGS{ResNet,
  author={He, K. and Zhang, X. and Ren, S. and Sun, J.},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}}

@INPROCEEDINGS{ResNeXt,
  author={Xie, S. and Girshick, R. and Dollár, P. and Tu, Z. and He, K.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Aggregated Residual Transformations for Deep Neural Networks}, 
  year={2017},
  volume={},
  number={},
  pages={5987-5995},
  keywords={Complexity theory;Neurons;Topology;Computer architecture;Neural networks;Network topology},
  doi={10.1109/CVPR.2017.634}}

@inproceedings{WideResNet,
	title={Wide Residual Networks},
	author={S. Zagoruyko and N. Komodakis},
	year={2016},
	month={September},
	pages={87.1-87.12},
	articleno={87},
	numpages={12},
	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
	publisher={BMVA Press},
	editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith},
	doi={10.5244/C.30.87},
	isbn={1-901725-59-6},
	url={https://dx.doi.org/10.5244/C.30.87}
}

@INPROCEEDINGS{DenseNet,
  author={Huang, G. and Liu, Z. and Van Der Maaten, L. and Weinberger, K. Q.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Densely Connected Convolutional Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2261-2269},
  keywords={Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation},
  doi={10.1109/CVPR.2017.243}}

@inproceedings{EfficientNet,
  author       = {M. Tan and
                  Q. V. Le},
  editor       = {K. Chaudhuri and
                  R. Salakhutdinov},
  title        = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning,
                  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  pages        = {6105--6114},
  publisher    = {{PMLR}},
  year         = {2019},
  url          = {http://proceedings.mlr.press/v97/tan19a.html},
  timestamp    = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/TanL19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{EfficientNetV2,
  author       = {M. Tan and
                  Q. V. Le},
  editor       = {M. Meila and
                  T. Zhang},
  title        = {EfficientNetV2: Smaller Models and Faster Training},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning,
                  {ICML} 2021, 18-24 July 2021, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {10096--10106},
  publisher    = {{PMLR}},
  year         = {2021},
  url          = {http://proceedings.mlr.press/v139/tan21a.html},
  timestamp    = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/TanL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{RegNet,
  author       = {I. Radosavovic and
                  R. P. Kosaraju and
                  R. B. Girshick and
                  K. He and
                  P. Doll{\'{a}}r},
  title        = {Designing Network Design Spaces},
  booktitle    = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
  pages        = {10425--10433},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2020},
  url          = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Radosavovic\_Designing\_Network\_Design\_Spaces\_CVPR\_2020\_paper.html},
  doi          = {10.1109/CVPR42600.2020.01044},
  timestamp    = {Tue, 31 Aug 2021 14:00:04 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/RadosavovicKGHD20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{
SqueezeNet,
title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \ensuremath{<}0.5{MB} model size},
author={F. N. Iandola and S. Han and M. W. Moskewicz and K. Ashraf and W. J. Dally and K. Keutzer},
year={2017},
url={https://openreview.net/forum?id=S1xh5sYgx}
}

@INPROCEEDINGS {MNASNet,
author = {M. Tan and B. Chen and R. Pang and V. Vasudevan and M. Sandler and A. Howard and Q. V. Le},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {MnasNet: Platform-Aware Neural Architecture Search for Mobile},
year = {2019},
volume = {},
issn = {},
pages = {2815-2823},
abstract = {Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8× faster than MobileNetV2 with 0.5% higher accuracy and 2.3× faster than NASNet with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet.},
keywords = {},
doi = {10.1109/CVPR.2019.00293},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2019.00293},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@INPROCEEDINGS {MobileNetV2,
author = {M. Sandler and A. Howard and M. Zhu and A. Zhmoginov and L. Chen},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
year = {2018},
volume = {},
issn = {},
pages = {4510-4520},
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
keywords = {manifolds;neural networks;computer architecture;standards;computational modeling;task analysis},
doi = {10.1109/CVPR.2018.00474},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00474},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@INPROCEEDINGS{MobileNetV3,
  author={Howard, A. and Sandler, M. and Chen, B. and Wang, W. and Chen, L. C. and Tan, M. and Chu, G. and Vasudevan, V. and Zhu, Y. and Pang, R. and Adam, H. and Le, Q.},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Searching for MobileNetV3}, 
  year={2019},
  volume={},
  number={},
  pages={1314-1324},
  keywords={Computer architecture;Proposals;Computational modeling;Image segmentation;Neural networks;Next generation networking;Mobile handsets},
  doi={10.1109/ICCV.2019.00140}}







