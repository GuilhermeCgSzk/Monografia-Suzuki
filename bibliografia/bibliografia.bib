@inproceedings{lucafo2022,
    title={Signal Quality Assessment of Photoplethysmogram Signals Using Hybrid Rule- and Learning-Based Models},
    author={Lucafo, Giovani and Freitas, Pedro Garcia and Lima, Rafael and Luz, Gustavo and Bispo, Ruan and Rodrigues, Paula and Cabello, Frank and Penatti, Otavio},
    booktitle={CBIS 2022},
    year = {2022},
}

@inproceedings{1d-to-2d-freitas,
author = {Freitas, Pedro and Lima, Rafael and Lucafo, Giovani and Penatti, Otávio},
year = {2023},
month = {07},
pages = {},
title = {Photoplethysmogram Signal Quality Assessment via 1D-to-2D Projections and Vision Transformers},
doi = {10.1109/QoMEX58391.2023.10178569}
}

%Nemcova A, Vargova E, Smisek R, Marsanova L, Smital L, Vitek M. Brno University of Technology Smartphone PPG Database (BUT PPG): Annotated Dataset for PPG Quality Assessment and Heart Rate Estimation. BioMed Research International. 2021 Sep 7;2021. https://doi.org/10.1155/2021/3453007
@article{butppg,
    author = {Nemcova,Andrea and Vargova,Enikö and Smisek,Radovan and Marsanova,Lucie and Smital,Lukas and Vitek,Martin},
    title = {Brno University of Technology Smartphone PPG Database (BUT PPG): Annotated Dataset for PPG Quality Assessment and Heart Rate Estimation},
    journal = {BioMed Research International},
    year = {2021},
    url = {https://doi.org/10.1155/2021/3453007},
}

%Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220.
@article{physionet,
    author = {Ary L. Goldberger  and Luis A. N. Amaral  and Leon Glass  and Jeffrey M. Hausdorff  and Plamen Ch. Ivanov  and Roger G. Mark  and Joseph E. Mietus  and George B. Moody  and Chung-Kang Peng  and H. Eugene Stanley },
    title = {PhysioBank, PhysioToolkit, and PhysioNet  },
    journal = {Circulation},
    volume = {101},
    number = {23},
    pages = {e215-e220},
    year = {2000},
    doi = {10.1161/01.CIR.101.23.e215},
    
    URL = {https://www.ahajournals.org/doi/abs/10.1161/01.CIR.101.23.e215},
    eprint = {https://www.ahajournals.org/doi/pdf/10.1161/01.CIR.101.23.e215}
    ,
        abstract = { Abstract—The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet.org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise. }
}

@article{pyts,
  author  = {Johann Faouzi and Hicham Janati},
  title   = {pyts: A Python Package for Time Series Classification},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {46},
  pages   = {1-6},
  url     = {http://jmlr.org/papers/v21/19-763.html}
}

@misc{pyts-v0.13.0,
author    = {Johann Faouzi and Hicham Janati and Ken Keong LEE and Tobias Carryer and Roman Yurchak and AvisP},
title     = {johannfaouzi/pyts: Release of version 0.13.0},
DOI       = {10.5281/zenodo.1244151},
publisher = {Zenodo},
year      = {2023},
month     = {June}
}

@inproceedings{optuna,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-Generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {machine learning system, Bayesian optimization, black-box optimization, hyperparameter optimization},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inbook{pytorch,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12}
}

@inproceedings{gaf-and-mtf,
author = {Wang, Zhiguang and Oates, Tim},
year = {2015},
month = {01},
pages = {},
title = {Encoding Time Series as Images for Visual Inspection and Classification Using Tiled Convolutional Neural Networks}
}

@inproceedings{Adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{NetAdapt,
  author       = {Tien{-}Ju Yang and
                  Andrew G. Howard and
                  Bo Chen and
                  Xiao Zhang and
                  Alec Go and
                  Mark Sandler and
                  Vivienne Sze and
                  Hartwig Adam},
  editor       = {Vittorio Ferrari and
                  Martial Hebert and
                  Cristian Sminchisescu and
                  Yair Weiss},
  title        = {NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications},
  booktitle    = {Computer Vision - {ECCV} 2018 - 15th European Conference, Munich,
                  Germany, September 8-14, 2018, Proceedings, Part {X}},
  series       = {Lecture Notes in Computer Science},
  volume       = {11214},
  pages        = {289--304},
  publisher    = {Springer},
  year         = {2018},
  url          = {https://doi.org/10.1007/978-3-030-01249-6\_18},
  doi          = {10.1007/978-3-030-01249-6\_18},
  timestamp    = {Thu, 27 May 2021 16:20:51 +0200},
  biburl       = {https://dblp.org/rec/conf/eccv/YangHCZGSSA18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS {ShuffleNet,
author = {X. Zhang and X. Zhou and M. Lin and J. Sun},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices},
year = {2018},
volume = {},
issn = {},
pages = {6848-6856},
abstract = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13× actual speedup over AlexNet while maintaining comparable accuracy.},
keywords = {convolution;complexity theory;computer architecture;mobile handsets;computational modeling;task analysis;neural networks},
doi = {10.1109/CVPR.2018.00716},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00716},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@INPROCEEDINGS {GoogLeNet,
author = {C. Szegedy and Wei Liu and Yangqing Jia and P. Sermanet and S. Reed and D. Anguelov and D. Erhan and V. Vanhoucke and A. Rabinovich},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Going deeper with convolutions},
year = {2015},
volume = {},
issn = {1063-6919},
pages = {1-9},
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
keywords = {},
doi = {10.1109/CVPR.2015.7298594},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2015.7298594},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@article{HydraClassifier,
author = {Dempster, Angus and Schmidt, Daniel F. and Webb, Geoffrey I.},
title = {Hydra: competing convolutional kernels for fast and accurate time series classification},
year = {2023},
issue_date = {Sep 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {37},
number = {5},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-023-00939-3},
doi = {10.1007/s10618-023-00939-3},
abstract = {We demonstrate a simple connection between dictionary methods for time series classification, which involve extracting and counting symbolic patterns in time series, and methods based on transforming input time series using convolutional kernels, namely Rocket and its variants. We show that by adjusting a single hyperparameter it is possible to move by degrees between models resembling dictionary methods and models resembling Rocket. We present Hydra, a simple, fast, and accurate dictionary method for time series classification using competing convolutional kernels, combining key aspects of both Rocket and conventional dictionary methods. Hydra is faster and more accurate than the most accurate existing dictionary methods, achieving similar accuracy to several of the most accurate current methods for time series classification. Hydra can also be combined with Rocket and its variants to significantly improve the accuracy of these methods.},
journal = {Data Min. Knowl. Discov.},
month = {may},
pages = {1779–1805},
numpages = {27},
keywords = {Rocket, Convolution, Random, Dictionary, Time series classification}
}

@INPROCEEDINGS {Xception,
author = {F. Chollet},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Xception: Deep Learning with Depthwise Separable Convolutions},
year = {2017},
volume = {},
issn = {1063-6919},
pages = {1800-1807},
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
keywords = {computer architecture;correlation;convolutional codes;google;biological neural networks},
doi = {10.1109/CVPR.2017.195},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.195},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}



@inproceedings{TimeSeriesAugmentation,
  author       = {Qingsong Wen and
                  Liang Sun and
                  Fan Yang and
                  Xiaomin Song and
                  Jingkun Gao and
                  Xue Wang and
                  Huan Xu},
  editor       = {Zhi{-}Hua Zhou},
  title        = {Time Series Data Augmentation for Deep Learning: {A} Survey},
  booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial
                  Intelligence, {IJCAI} 2021, Virtual Event / Montreal, Canada, 19-27
                  August 2021},
  pages        = {4653--4660},
  publisher    = {ijcai.org},
  year         = {2021},
  url          = {https://doi.org/10.24963/ijcai.2021/631},
  doi          = {10.24963/IJCAI.2021/631},
  timestamp    = {Tue, 12 Dec 2023 12:16:23 +0100},
  biburl       = {https://dblp.org/rec/conf/ijcai/Wen0YSGWX21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{AeonDoc,
  title        = {Aeon},
  author       = {Aeon Team},
  note         = {Accessed: May 18, 2024},
  howpublished = {\url{https://www.aeon-toolkit.org/en/stable/}}
}

@misc{PytorchDoc,
  title        = {PyTorch documentation},
  author       = {PyTorch Foundation},
  year         = 2023,
  note         = {Accessed: May 18, 2024},
  howpublished = {\url{https://pytorch.org/docs/stable/index.html}}
}

@misc{OptunaDoc,
  title        = {Optuna: A hyperparameter optimization framework},
  author       = {Optuna Contributors},
  year         = 2018,
  note         = {Accessed: May 18, 2024},
  howpublished = {\url{https://optuna.readthedocs.io/en/stable/}}
}

@misc{ImbalancedLearn,
  title        = {imbalanced-learn documentation},
  author       = {The imbalanced-learn developers},
  year         = 2024,
  note         = {Accessed: May 18, 2024},
  howpublished = {\url{https://imbalanced-learn.org/stable/}}
}

@misc{Sklearn,
  title        = {scikit-learn},
  author       = {scikit-learn developers},
  year         = 2024,
  note         = {Accessed: May 18, 2024},
  howpublished = {\url{https://scikit-learn.org/}}
}

@misc{Pympler,
  title        = {Pympler},
  author       = {Jean Brouwers, Ludwig Haehne, Robert Schuppenies},
  year         = 2020,
  note         = {Accessed: May 18, 2024},
  howpublished = {\url{https://pympler.readthedocs.io/en/latest/}}
}

@misc{Keras,
  title        = {Keras},
  author       = {Google Inc},
  note         = {Accessed: May 18, 2024},
  howpublished = {\url{https://keras.io/}}
}

@misc{ImageNet,
  title        = {ImageNet},
  author       = {Stanford Vision Lab, Stanford University, Princeton University},
  year         = 2020,
  note         = {Accessed: May 18, 2024},
  howpublished = {\url{https://www.image-net.org/}}
}
