% Arsenal usa a citação do HIVECOTEV

@article{HIVECOTEV2,
author = {Middlehurst, Matthew and Large, James and Flynn, Michael and Lines, Jason and Bostrom, Aaron and Bagnall, Anthony},
title = {HIVE-COTE 2.0: a new meta ensemble for time series classification},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {11–12},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-06057-9},
doi = {10.1007/s10994-021-06057-9},
abstract = {The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is a heterogeneous meta ensemble for time series classification. HIVE-COTE forms its ensemble from classifiers of multiple domains, including phase-independent shapelets, bag-of-words based dictionaries and phase-dependent intervals. Since it was first proposed in 2016, the algorithm has remained state of the art for accuracy on the UCR time series classification archive. Over time it has been incrementally updated, culminating in its current state, HIVE-COTE 1.0. During this time a number of algorithms have been proposed which match the accuracy of HIVE-COTE. We propose comprehensive changes to the HIVE-COTE algorithm which significantly improve its accuracy and usability, presenting this upgrade as HIVE-COTE 2.0. We introduce two novel classifiers, the Temporal Dictionary Ensemble and Diverse Representation Canonical Interval Forest, which replace existing ensemble members. Additionally, we introduce the Arsenal, an ensemble of ROCKET classifiers as a new HIVE-COTE 2.0 constituent. We demonstrate that HIVE-COTE 2.0 is significantly more accurate on average than the current state of the art on 112 univariate UCR archive datasets and 26 multivariate UEA archive datasets.},
journal = {Mach. Learn.},
month = {dec},
pages = {3211–3243},
numpages = {33},
keywords = {Time series classification, Multivariate time series, Heterogeneous ensembles, HIVE-COTE}
}



@article{RocketClassifier,
  author       = {Angus Dempster and
                  Fran{\c{c}}ois Petitjean and
                  Geoffrey I. Webb},
  title        = {{ROCKET:} exceptionally fast and accurate time series classification
                  using random convolutional kernels},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {34},
  number       = {5},
  pages        = {1454--1495},
  year         = {2020},
  url          = {https://doi.org/10.1007/s10618-020-00701-z},
  doi          = {10.1007/S10618-020-00701-Z},
  timestamp    = {Thu, 29 Jul 2021 13:41:10 +0200},
  biburl       = {https://dblp.org/rec/journals/datamine/DempsterPW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{BOSSEnsemble,
  author       = {Patrick Sch{\"{a}}fer},
  title        = {The {BOSS} is concerned with time series classification in the presence
                  of noise},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {29},
  number       = {6},
  pages        = {1505--1530},
  year         = {2015},
  url          = {https://doi.org/10.1007/s10618-014-0377-7},
  doi          = {10.1007/S10618-014-0377-7},
  timestamp    = {Wed, 13 Dec 2017 11:18:06 +0100},
  biburl       = {https://dblp.org/rec/journals/datamine/Schafer15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{CNNClassifier,
  author={Zhao, Bendong and Lu, Huanzhang and Chen, Shangfeng and Liu, Junliang and Wu, Dongya},
  journal={Journal of Systems Engineering and Electronics}, 
  title={Convolutional neural networks for time series classification}, 
  year={2017},
  volume={28},
  number={1},
  pages={162-169},
  keywords={Time series analysis;Convolution;Hidden Markov models;Training;Neural networks;Data mining;Neurons;time series;multivariate time series;classification;convolutional neural network (CNN);data mining},
  doi={10.21629/JSEE.2017.01.18}}


@INPROCEEDINGS{CanonicalIntervalForestClassifier,
  author={Middlehurst, Matthew and Large, James and Bagnall, Anthony},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)}, 
  title={The Canonical Interval Forest (CIF) Classifier for Time Series Classification}, 
  year={2020},
  volume={},
  number={},
  pages={188-195},
  keywords={Training;Time series analysis;Memory management;Forestry;Big Data;Prediction algorithms;Classification algorithms;Time series;Classification;Ensembles;HIVE-COTE;Multivariate},
  doi={10.1109/BigData50022.2020.9378424}}



@article{Catch22Classifier,
  author       = {Carl Henning Lubba and
                  Sarab S. Sethi and
                  Philip Knaute and
                  Simon R. Schultz and
                  Ben D. Fulcher and
                  Nick S. Jones},
  title        = {catch22: CAnonical Time-series CHaracteristics - Selected through
                  highly comparative time-series analysis},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {33},
  number       = {6},
  pages        = {1821--1852},
  year         = {2019},
  url          = {https://doi.org/10.1007/s10618-019-00647-x},
  doi          = {10.1007/S10618-019-00647-X},
  timestamp    = {Thu, 07 Nov 2019 09:20:02 +0100},
  biburl       = {https://dblp.org/rec/journals/datamine/LubbaSKSFJ19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ContinuousIntervalTree,
title = {A time series forest for classification and feature extraction},
journal = {Information Sciences},
volume = {239},
pages = {142-153},
year = {2013},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2013.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S0020025513001473},
author = {Houtao Deng and George Runger and Eugene Tuv and Martyanov Vladimir},
keywords = {Decision tree, Ensemble, Entrance gain, Interpretability, Large margin, Time series classification},
abstract = {A tree-ensemble method, referred to as time series forest (TSF), is proposed for time series classification. TSF employs a combination of entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain improves the accuracy of TSF. TSF randomly samples features at each tree node and has computational complexity linear in the length of time series, and can be built using parallel computing techniques. The temporal importance curve is proposed to capture the temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, standard deviation and slope is computationally efficient and outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping.}
}

@InProceedings{ContractableBOSS,
author="Middlehurst, Matthew
and Vickers, William
and Bagnall, Anthony",
editor="Yin, Hujun
and Camacho, David
and Tino, Peter
and Tall{\'o}n-Ballesteros, Antonio J.
and Menezes, Ronaldo
and Allmendinger, Richard",
title="Scalable Dictionary Classifiers for Time Series Classification",
booktitle="Intelligent Data Engineering and Automated Learning -- IDEAL 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="11--19",
abstract="Dictionary based classifiers are a family of algorithms for time series classification (TSC) that focus on capturing the frequency of pattern occurrences in a time series. The ensemble based Bag of Symbolic Fourier Approximation Symbols (BOSS) was found to be a top performing TSC algorithm in a recent evaluation, as well as the best performing dictionary based classifier. However, BOSS does not scale well. We evaluate changes to the way BOSS chooses classifiers for its ensemble, replacing its parameter search with random selection. This change allows for the easy implementation of contracting (setting a build time limit for the classifier) and check-pointing (saving progress during the classifiers build). We achieve a significant reduction in build time without a significant change in accuracy on average when compared to BOSS by creating a fixed size weighted ensemble selecting the best performers from a randomly chosen parameter set. Our experiments are conducted on datasets from the recently expanded UCR time series archive. We demonstrate the usability improvements to randomised BOSS with a case study using a large whale acoustics dataset for which BOSS proved infeasible.",
isbn="978-3-030-33607-3"
}

% DrCIFClassifier é introduzido no paper do HIVECOTE V2

@article{ElasticEnsemble,
  author       = {Jason Lines and
                  Anthony J. Bagnall},
  title        = {Time series classification with ensembles of elastic distance measures},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {29},
  number       = {3},
  pages        = {565--592},
  year         = {2015},
  url          = {https://doi.org/10.1007/s10618-014-0361-2},
  doi          = {10.1007/S10618-014-0361-2},
  timestamp    = {Wed, 14 Nov 2018 10:41:43 +0100},
  biburl       = {https://dblp.org/rec/journals/datamine/LinesB15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{FCNClassifier-MLPClassifier,
  author       = {Zhiguang Wang and
                  Weizhong Yan and
                  Tim Oates},
  title        = {Time series classification from scratch with deep neural networks:
                  {A} strong baseline},
  booktitle    = {2017 International Joint Conference on Neural Networks, {IJCNN} 2017,
                  Anchorage, AK, USA, May 14-19, 2017},
  pages        = {1578--1585},
  publisher    = {{IEEE}},
  year         = {2017},
  url          = {https://doi.org/10.1109/IJCNN.2017.7966039},
  doi          = {10.1109/IJCNN.2017.7966039},
  timestamp    = {Sun, 16 Oct 2022 14:19:57 +0200},
  biburl       = {https://dblp.org/rec/conf/ijcnn/WangYO17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Inception1,
author = {Ismail Fawaz, Hassan and Lucas, Benjamin and Forestier, Germain and Pelletier, Charlotte and Schmidt, Daniel F. and Weber, Jonathan and Webb, Geoffrey I. and Idoumghar, Lhassane and Muller, Pierre-Alain and Petitjean, Fran\c{c}ois},
title = {InceptionTime: Finding AlexNet for time series classification},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {6},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-020-00710-y},
doi = {10.1007/s10618-020-00710-y},
abstract = {This paper brings deep learning at the forefront of research into time series classification (TSC). TSC is the area of machine learning tasked with the categorization (or labelling) of time series. The last few decades of work in this area have led to significant progress in the accuracy of classifiers, with the state of the art now represented by the HIVE-COTE algorithm. While extremely accurate, HIVE-COTE cannot be applied to many real-world datasets because of its high training time complexity in O(N2·T4) for a dataset with N time series of length T. For example, it takes HIVE-COTE more than 8 days to learn from a small dataset with N=1500 time series of short length T=46. Meanwhile deep learning has received enormous attention because of its high accuracy and scalability. Recent approaches to deep learning for TSC have been scalable, but less accurate than HIVE-COTE. We introduce InceptionTime—an ensemble of deep Convolutional Neural Network models, inspired by the Inception-v4 architecture. Our experiments show that InceptionTime is on par with HIVE-COTE in terms of accuracy while being much more scalable: not only can it learn from 1500 time series in one hour but it can also learn from 8M time series in 13 h, a quantity of data that is fully out of reach of HIVE-COTE.},
journal = {Data Min. Knowl. Discov.},
month = {nov},
pages = {1936–1962},
numpages = {27},
keywords = {Inception, Scalable model, Deep learning, Time series classification}
}

@INPROCEEDINGS{Inception2,
  author={Ismail-Fawaz, Ali and Devanne, Maxime and Weber, Jonathan and Forestier, Germain},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Deep Learning For Time Series Classification Using New Hand-Crafted Convolution Filters}, 
  year={2022},
  volume={},
  number={},
  pages={972-981},
  keywords={Training;Deep learning;Convolution;Image color analysis;Time series analysis;Computer architecture;Big Data;Time Series Classification;Convolution Neural Networks;Pattern Recognition;Feature Engineering;handcrafted Filters},
  doi={10.1109/BigData55660.2022.10020496}}

@INPROCEEDINGS{LITETimeClassifier,
  author={Ismail-Fawaz, Ali and Devanne, Maxime and Berretti, Stefano and Weber, Jonathan and Forestier, Germain},
  booktitle={2023 IEEE 10th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={LITE: Light Inception with boosTing tEchniques for Time Series Classification}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  keywords={Training;Deep learning;Representation learning;Multiplexing;Power demand;Time series analysis;Neural networks;Time Series Classification;Neural Networks;Convolutional Neural Networks;DepthWise Separable Convolutions},
  doi={10.1109/DSAA60987.2023.10302569}}

@inproceedings{TDE,
author = {Middlehurst, Matthew and Large, James and Cawley, Gavin and Bagnall, Anthony},
title = {The Temporal Dictionary Ensemble (TDE) Classifier for Time Series Classification},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_38},
doi = {10.1007/978-3-030-67658-2_38},
abstract = {Using bag of words representations of time series is a popular approach to time series classification (TSC). These algorithms involve approximating and discretising windows over a series to form words, then forming a count of words over a given dictionary. Classifiers are constructed on the resulting histograms of word counts. A 2017 evaluation of a range of time series classifiers found the bag of symbolic-Fourier approximation symbols (BOSS) ensemble the best of the dictionary based classifiers. It forms one of the components of hierarchical vote collective of transformation-based ensembles (HIVE-COTE), which represents the current state of the art. Since then, several new dictionary based algorithms have been proposed that are more accurate or more scalable (or both) than BOSS. We propose a further extension of these dictionary based classifiers that combines the best elements of the others combined with a novel approach to constructing ensemble members based on an adaptive Gaussian process model of the parameter space. We demonstrate that the Temporal Dictionary Ensemble (TDE) is more accurate than other dictionary based approaches. Furthermore, unlike the other classifiers, if we replace BOSS in HIVE-COTE with TDE, HIVE-COTE becomes significantly more accurate. We also show this new version of HIVE-COTE is significantly more accurate than the current top performing classifiers on the UCR time series archive. This advance represents a new state of the art for time series classification.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {660–676},
numpages = {17},
keywords = {Time series, Classification, Bag of words, HIVE-COTE},
location = {<conf-loc content-type="InPerson">Ghent, Belgium</conf-loc>}
}



@article{MUSE,
  author       = {Patrick Sch{\"{a}}fer and
                  Ulf Leser},
  title        = {Multivariate Time Series Classification with {WEASEL+MUSE}},
  journal      = {CoRR},
  volume       = {abs/1711.11343},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.11343},
  eprinttype    = {arXiv},
  eprint       = {1711.11343},
  timestamp    = {Mon, 13 Aug 2018 16:48:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-11343.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{WEASEL,
author = {Sch\"{a}fer, Patrick and Leser, Ulf},
title = {Fast and Accurate Time Series Classification with WEASEL},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132980},
doi = {10.1145/3132847.3132980},
abstract = {Time series (TS) occur in many scientific and commercial applications, ranging from earth surveillance to industry automation to the smart grids. An important type of TS analysis is classification, which can, for instance, improve energy load forecasting in smart grids by detecting the types of electronic devices based on their energy consumption profiles recorded by automatic sensors. Such sensor-driven applications are very often characterized by (a) very long TS and (b) very large TS datasets needing classification. However, current methods to time series classification (TSC) cannot cope with such data volumes at acceptable accuracy; they are either scalable but offer only inferior classification quality, or they achieve state-of-the-art classification quality but cannot scale to large data volumes. In this paper, we present WEASEL (Word ExtrAction for time SEries cLassification), a novel TSC method which is both fast and accurate. Like other state-of-the-art TSC methods, WEASEL transforms time series into feature vectors, using a sliding-window approach, which are then analyzed through a machine learning classifier. The novelty of WEASEL lies in its specific method for deriving features, resulting in a much smaller yet much more discriminative feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more accurate than the best current non-ensemble algorithms at orders-of-magnitude lower classification and training times, and it is almost as accurate as ensemble classifiers, whose computational complexity makes them inapplicable even for mid-size datasets. The outstanding robustness of WEASEL is also confirmed by experiments on two real smart grid datasets, where it out-of-the-box achieves almost the same accuracy as highly tuned, domain-specific methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {637–646},
numpages = {10},
keywords = {bag-of-patterns, classification, feature selection, time series, word co-occurrences},
location = {Singapore, Singapore},
series = {CIKM '17}
}



@article{WEASELV2,
  author       = {Patrick Sch{\"{a}}fer and
                  Ulf Leser},
  title        = {{WEASEL} 2.0: a random dilated dictionary transform for fast, accurate
                  and memory constrained time series classification},
  journal      = {Mach. Learn.},
  volume       = {112},
  number       = {12},
  pages        = {4763--4788},
  year         = {2023},
  url          = {https://doi.org/10.1007/s10994-023-06395-w},
  doi          = {10.1007/S10994-023-06395-W},
  timestamp    = {Sun, 10 Dec 2023 17:00:49 +0100},
  biburl       = {https://dblp.org/rec/journals/ml/SchaferL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{REDCOMETS-1,
  author       = {Luca A. Bennett and
                  Zahraa S. Abdallah},
  editor       = {Georgiana Ifrim and
                  Romain Tavenard and
                  Anthony J. Bagnall and
                  Patrick Sch{\"{a}}fer and
                  Simon Malinowski and
                  Thomas Guyet and
                  Vincent Lemaire},
  title        = {{RED} CoMETS: An Ensemble Classifier for Symbolically Represented
                  Multivariate Time Series},
  booktitle    = {Advanced Analytics and Learning on Temporal Data - 8th {ECML} {PKDD}
                  Workshop, {AALTD} 2023, Turin, Italy, September 18-22, 2023, Revised
                  Selected Papers},
  series       = {Lecture Notes in Computer Science},
  volume       = {14343},
  pages        = {76--91},
  publisher    = {Springer},
  year         = {2023},
  url          = {https://doi.org/10.1007/978-3-031-49896-1\_6},
  doi          = {10.1007/978-3-031-49896-1\_6},
  timestamp    = {Fri, 26 Jan 2024 07:55:30 +0100},
  biburl       = {https://dblp.org/rec/conf/aaltd/BennettA23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{REDCOMETS-2,
  author       = {Zahraa S. Abdallah and
                  Mohamed Medhat Gaber},
  title        = {Co-eye: a multi-resolution ensemble classifier for symbolically approximated
                  time series},
  journal      = {Mach. Learn.},
  volume       = {109},
  number       = {11},
  pages        = {2029--2061},
  year         = {2020},
  url          = {https://doi.org/10.1007/s10994-020-05887-3},
  doi          = {10.1007/S10994-020-05887-3},
  timestamp    = {Fri, 14 May 2021 08:31:53 +0200},
  biburl       = {https://dblp.org/rec/journals/ml/AbdallahG20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{ShapeDTW,
title = {shapeDTW: Shape Dynamic Time Warping},
journal = {Pattern Recognition},
volume = {74},
pages = {171-184},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2017.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317303710},
author = {Jiaping Zhao and Laurent Itti},
keywords = {Dynamic Time Warping, Sequence alignment, Time series classification},
abstract = {Dynamic Time Warping (DTW) is an algorithm to align temporal sequences with possible local non-linear distortions, and has been widely applied to audio, video and graphics data alignments. DTW is essentially a point-to-point matching method under some boundary and temporal consistency constraints. Although DTW obtains a global optimal solution, it does not necessarily achieve locally sensible matchings. Concretely, two temporal points with entirely dissimilar local structures may be matched by DTW. To address this problem, we propose an improved alignment algorithm, named shape Dynamic Time Warping (shapeDTW), which enhances DTW by taking point-wise local structural information into consideration. shapeDTW is inherently a DTW algorithm, but additionally attempts to pair locally similar structures and to avoid matching points with distinct neighborhood structures. We apply shapeDTW to align audio signal pairs having ground-truth alignments, as well as artificially simulated pairs of aligned sequences, and obtain quantitatively much lower alignment errors than DTW and its two variants. When shapeDTW is used as a distance measure in a nearest neighbor classifier (NN-shapeDTW) to classify time series, it beats DTW on 64 out of 84 UCR time series datasets, with significantly improved classification accuracies. By using a properly designed local structure descriptor, shapeDTW improves accuracies by more than 10% on 18 datasets. To the best of our knowledge, shapeDTW is the first distance measure under the nearest neighbor classifier scheme to significantly outperform DTW, which had been widely recognized as the best distance measure to date. Our code is publicly accessible at: https://github.com/jiapingz/shapeDTW.}
}

@article{TSFreshClassifier,
title = {Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package)},
journal = {Neurocomputing},
volume = {307},
pages = {72-77},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.03.067},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218304843},
author = {Maximilian Christ and Nils Braun and Julius Neuffer and Andreas W. Kempa-Liehr},
keywords = {Feature engineering, Time series, Feature extraction, Feature selection, Machine learning},
abstract = {Time series feature engineering is a time-consuming process because scientists and engineers have to consider the multifarious algorithms of signal processing and time series analysis for identifying and extracting meaningful features from time series. The Python package tsfresh (Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests) accelerates this process by combining 63 time series characterization methods, which by default compute a total of 794 time series features, with feature selection on basis automatically configured hypothesis tests. By identifying statistically significant time series characteristics in an early stage of the data science process, tsfresh closes feedback loops with domain experts and fosters the development of domain specific features early on. The package implements standard APIs of time series and machine learning libraries (e.g. pandas and scikit-learn) and is designed for both exploratory analyses as well as straightforward integration into operational data science applications.}
}

@inproceedings{HIVECOTEV1,
author = {Bagnall, Anthony and Flynn, Michael and Large, James and Lines, Jason and Middlehurst, Matthew},
title = {On the Usage and Performance of the Hierarchical Vote Collective of Transformation-Based Ensembles Version 1.0 (HIVE-COTE v1.0)},
year = {2020},
isbn = {978-3-030-65741-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65742-0_1},
doi = {10.1007/978-3-030-65742-0_1},
abstract = {The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is a heterogeneous meta ensemble for time series classification. Since it was first proposed in 2016, the algorithm has undergone some minor changes and there is now a configurable, scalable and easy to use version available in two open source repositories. We present an overview of the latest stable HIVE-COTE, version 1.0, and describe how it differs to the original. We provide a walkthrough guide of how to use the classifier, and conduct extensive experimental evaluation of its predictive performance and resource usage. We compare the performance of HIVE-COTE to three recently proposed algorithms.},
booktitle = {Advanced Analytics and Learning on Temporal Data: 5th ECML PKDD Workshop, AALTD 2020, Ghent, Belgium, September 18, 2020, Revised Selected Papers},
pages = {3–18},
numpages = {16},
keywords = {HIVE-COTE, Heterogeneous ensembles, Classification, Time series},
location = {Ghent, Belgium}
}

@article{RandomIntervalSpectralEnsembleClassifier,
author = {Lines, Jason and Taylor, Sarah and Bagnall, Anthony},
title = {Time Series Classification with HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3182382},
doi = {10.1145/3182382},
abstract = {A recent experimental evaluation assessed 19 time series classification (TSC) algorithms and found that one was significantly more accurate than all others: the Flat Collective of Transformation-based Ensembles (Flat-COTE). Flat-COTE is an ensemble that combines 35 classifiers over four data representations. However, while comprehensive, the evaluation did not consider deep learning approaches. Convolutional neural networks (CNN) have seen a surge in popularity and are now state of the art in many fields and raises the question of whether CNNs could be equally transformative for TSC.We implement a benchmark CNN for TSC using a common structure and use results from a TSC-specific CNN from the literature. We compare both to Flat-COTE and find that the collective is significantly more accurate than both CNNs. These results are impressive, but Flat-COTE is not without deficiencies. We significantly improve the collective by proposing a new hierarchical structure with probabilistic voting, defining and including two novel ensemble classifiers built in existing feature spaces, and adding further modules to represent two additional transformation domains. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE), encapsulates classifiers built on five data representations. We demonstrate that HIVE-COTE is significantly more accurate than Flat-COTE (and all other TSC algorithms that we are aware of) over 100 resamples of 85 TSC problems and is the new state of the art for TSC. Further analysis is included through the introduction and evaluation of 3 new case studies and extensive experimentation on 1,000 simulated datasets of 5 different types.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {52},
numpages = {35},
keywords = {meta ensembles, heterogeneous ensembles, deep learning, Time series classification}
}

@INPROCEEDINGS{SupervisedTimeSeriesForest,
  author={Cabello, Nestor and Naghizade, Elham and Qi, Jianzhong and Kulik, Lars},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)}, 
  title={Fast and Accurate Time Series Classification Through Supervised Interval Search}, 
  year={2020},
  volume={},
  number={},
  pages={948-953},
  keywords={Training;Computational modeling;Time series analysis;Vegetation;Forestry;Search problems;Time measurement;Time series classification;Interval-based classifier;Feature selection},
  doi={10.1109/ICDM50108.2020.00107}}

@article{TimeSeriesForestClassifier,
title = {A time series forest for classification and feature extraction},
journal = {Information Sciences},
volume = {239},
pages = {142-153},
year = {2013},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2013.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S0020025513001473},
author = {Houtao Deng and George Runger and Eugene Tuv and Martyanov Vladimir},
keywords = {Decision tree, Ensemble, Entrance gain, Interpretability, Large margin, Time series classification},
abstract = {A tree-ensemble method, referred to as time series forest (TSF), is proposed for time series classification. TSF employs a combination of entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain improves the accuracy of TSF. TSF randomly samples features at each tree node and has computational complexity linear in the length of time series, and can be built using parallel computing techniques. The temporal importance curve is proposed to capture the temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, standard deviation and slope is computationally efficient and outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping.}
}



@article{ShapeletTransformClassifier-1,
  author       = {Jon Hills and
                  Jason Lines and
                  Edgaras Baranauskas and
                  James Mapp and
                  Anthony J. Bagnall},
  title        = {Classification of time series by shapelet transformation},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {28},
  number       = {4},
  pages        = {851--881},
  year         = {2014},
  url          = {https://doi.org/10.1007/s10618-013-0322-1},
  doi          = {10.1007/S10618-013-0322-1},
  timestamp    = {Wed, 14 Nov 2018 10:41:43 +0100},
  biburl       = {https://dblp.org/rec/journals/datamine/HillsLBMB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{ShapeletTransformClassifier-2,
  author       = {Aaron Bostrom and
                  Anthony J. Bagnall},
  title        = {Binary Shapelet Transform for Multiclass Time Series Classification},
  journal      = {Trans. Large Scale Data Knowl. Centered Syst.},
  volume       = {32},
  pages        = {24--46},
  year         = {2017},
  url          = {https://doi.org/10.1007/978-3-662-55608-5\_2},
  doi          = {10.1007/978-3-662-55608-5\_2},
  timestamp    = {Mon, 02 Mar 2020 16:30:08 +0100},
  biburl       = {https://dblp.org/rec/journals/tlsdkcs/BostromB17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{RDSTClassifier-1,
  author       = {Antoine Guillaume and
                  Christel Vrain and
                  Wael Elloumi},
  editor       = {Mounim A. El{-}Yacoubi and
                  Eric Granger and
                  Pong Chi Yuen and
                  Umapada Pal and
                  Nicole Vincent},
  title        = {Random Dilated Shapelet Transform: {A} New Approach for Time Series
                  Shapelets},
  booktitle    = {Pattern Recognition and Artificial Intelligence - Third International
                  Conference, {ICPRAI} 2022, Paris, France, June 1-3, 2022, Proceedings,
                  Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {13363},
  pages        = {653--664},
  publisher    = {Springer},
  year         = {2022},
  url          = {https://doi.org/10.1007/978-3-031-09037-0\_53},
  doi          = {10.1007/978-3-031-09037-0\_53},
  timestamp    = {Mon, 13 Jun 2022 20:56:40 +0200},
  biburl       = {https://dblp.org/rec/conf/icprai/GuillaumeVE22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@phdthesis{RDSTClassifier-2,
  author       = {Antoine Guillaume and
                  Christel Vrain and
                  Wael Elloumi},
  title        = {Time series classification with Shapelets: Application to predictive
                  maintenance on event logs. (Classification de s{\'{e}}ries temporelles
                  avec les Shapelets : application {\`{a}} la maintenance pr{\'{e}}dictive
                  via journaux d'{\'{e}}v{\'{e}}nements)},
  school       = {University of Orl{\'{e}}ans, France},
  year         = {2023},
  url          = {https://tel.archives-ouvertes.fr/tel-04368849},
  timestamp    = {Wed, 24 Jan 2024 08:30:10 +0100},
  biburl       = {https://dblp.org/rec/phd/hal/GuillaumeCW23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{OrdinalTDE,
author = {Ayll\'{o}n-Gavil\'{a}n, Rafael and Guijo-Rubio, David and Guti\'{e}rrez, Pedro Antonio and Herv\'{a}s-Mart\'{\i}nez, C\'{e}sar},
title = {A Dictionary-Based Approach to Time Series Ordinal Classification},
year = {2023},
isbn = {978-3-031-43077-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43078-7_44},
doi = {10.1007/978-3-031-43078-7_44},
abstract = {Time Series Classification (TSC) is an extensively researched field from which a broad range of real-world problems can be addressed obtaining excellent results. One sort of the approaches performing well are the so-called dictionary-based techniques. The Temporal Dictionary Ensemble (TDE) is the current state-of-the-art dictionary-based TSC approach. In many TSC problems we find a natural ordering in the labels associated with the time series. This characteristic is referred to as ordinality, and can be exploited to improve the methods performance. The area dealing with ordinal time series is the Time Series Ordinal Classification (TSOC) field, which is yet unexplored. In this work, we present an ordinal adaptation of the TDE algorithm, known as ordinal TDE (O-TDE). For this, a comprehensive comparison using a set of 18 TSOC problems is performed. Experiments conducted show the improvement achieved by the ordinal dictionary-based approach in comparison to four other existing nominal dictionary-based techniques.},
booktitle = {Advances in Computational Intelligence: 17th International Work-Conference on Artificial Neural Networks, IWANN 2023, Ponta Delgada, Portugal, June 19–21, 2023, Proceedings, Part II},
pages = {541–552},
numpages = {12},
keywords = {ordinal classification, dictionary-based approaches, time series},
location = {Ponta Delgada, Portugal}
}



@article{RotationForestClassifier,
  author       = {Juan Jos{\'{e}} Rodr{\'{\i}}guez and
                  Ludmila I. Kuncheva and
                  Carlos J. Alonso},
  title        = {Rotation Forest: {A} New Classifier Ensemble Method},
  journal      = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  volume       = {28},
  number       = {10},
  pages        = {1619--1630},
  year         = {2006},
  url          = {https://doi.org/10.1109/TPAMI.2006.211},
  doi          = {10.1109/TPAMI.2006.211},
  timestamp    = {Wed, 14 Nov 2018 10:51:14 +0100},
  biburl       = {https://dblp.org/rec/journals/pami/RodriguezKA06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





