% Arsenal usa a citação do HIVECOTEV

@article{HIVECOTEV2,
author = {Middlehurst, Matthew and Large, James and Flynn, Michael and Lines, Jason and Bostrom, Aaron and Bagnall, Anthony},
title = {HIVE-COTE 2.0: a new meta ensemble for time series classification},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {11–12},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-06057-9},
doi = {10.1007/s10994-021-06057-9},
abstract = {The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is a heterogeneous meta ensemble for time series classification. HIVE-COTE forms its ensemble from classifiers of multiple domains, including phase-independent shapelets, bag-of-words based dictionaries and phase-dependent intervals. Since it was first proposed in 2016, the algorithm has remained state of the art for accuracy on the UCR time series classification archive. Over time it has been incrementally updated, culminating in its current state, HIVE-COTE 1.0. During this time a number of algorithms have been proposed which match the accuracy of HIVE-COTE. We propose comprehensive changes to the HIVE-COTE algorithm which significantly improve its accuracy and usability, presenting this upgrade as HIVE-COTE 2.0. We introduce two novel classifiers, the Temporal Dictionary Ensemble and Diverse Representation Canonical Interval Forest, which replace existing ensemble members. Additionally, we introduce the Arsenal, an ensemble of ROCKET classifiers as a new HIVE-COTE 2.0 constituent. We demonstrate that HIVE-COTE 2.0 is significantly more accurate on average than the current state of the art on 112 univariate UCR archive datasets and 26 multivariate UEA archive datasets.},
journal = {Mach. Learn.},
month = {dec},
pages = {3211–3243},
numpages = {33},
keywords = {Time series classification, Multivariate time series, Heterogeneous ensembles, HIVE-COTE}
}



@article{RocketClassifier,
  author       = {Angus Dempster and
                  Fran{\c{c}}ois Petitjean and
                  Geoffrey I. Webb},
  title        = {{ROCKET:} exceptionally fast and accurate time series classification
                  using random convolutional kernels},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {34},
  number       = {5},
  pages        = {1454--1495},
  year         = {2020},
  url          = {https://doi.org/10.1007/s10618-020-00701-z},
  doi          = {10.1007/S10618-020-00701-Z},
  timestamp    = {Thu, 29 Jul 2021 13:41:10 +0200},
  biburl       = {https://dblp.org/rec/journals/datamine/DempsterPW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{BOSSEnsemble,
  author       = {Patrick Sch{\"{a}}fer},
  title        = {The {BOSS} is concerned with time series classification in the presence
                  of noise},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {29},
  number       = {6},
  pages        = {1505--1530},
  year         = {2015},
  url          = {https://doi.org/10.1007/s10618-014-0377-7},
  doi          = {10.1007/S10618-014-0377-7},
  timestamp    = {Wed, 13 Dec 2017 11:18:06 +0100},
  biburl       = {https://dblp.org/rec/journals/datamine/Schafer15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{CNNClassifier,
  author={Zhao, Bendong and Lu, Huanzhang and Chen, Shangfeng and Liu, Junliang and Wu, Dongya},
  journal={Journal of Systems Engineering and Electronics}, 
  title={Convolutional neural networks for time series classification}, 
  year={2017},
  volume={28},
  number={1},
  pages={162-169},
  keywords={Time series analysis;Convolution;Hidden Markov models;Training;Neural networks;Data mining;Neurons;time series;multivariate time series;classification;convolutional neural network (CNN);data mining},
  doi={10.21629/JSEE.2017.01.18}}


@INPROCEEDINGS{CanonicalIntervalForestClassifier,
  author={Middlehurst, Matthew and Large, James and Bagnall, Anthony},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)}, 
  title={The Canonical Interval Forest (CIF) Classifier for Time Series Classification}, 
  year={2020},
  volume={},
  number={},
  pages={188-195},
  keywords={Training;Time series analysis;Memory management;Forestry;Big Data;Prediction algorithms;Classification algorithms;Time series;Classification;Ensembles;HIVE-COTE;Multivariate},
  doi={10.1109/BigData50022.2020.9378424}}



@article{Catch22Classifier,
  author       = {Carl Henning Lubba and
                  Sarab S. Sethi and
                  Philip Knaute and
                  Simon R. Schultz and
                  Ben D. Fulcher and
                  Nick S. Jones},
  title        = {catch22: CAnonical Time-series CHaracteristics - Selected through
                  highly comparative time-series analysis},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {33},
  number       = {6},
  pages        = {1821--1852},
  year         = {2019},
  url          = {https://doi.org/10.1007/s10618-019-00647-x},
  doi          = {10.1007/S10618-019-00647-X},
  timestamp    = {Thu, 07 Nov 2019 09:20:02 +0100},
  biburl       = {https://dblp.org/rec/journals/datamine/LubbaSKSFJ19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ContinuousIntervalTree,
title = {A time series forest for classification and feature extraction},
journal = {Information Sciences},
volume = {239},
pages = {142-153},
year = {2013},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2013.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S0020025513001473},
author = {Houtao Deng and George Runger and Eugene Tuv and Martyanov Vladimir},
keywords = {Decision tree, Ensemble, Entrance gain, Interpretability, Large margin, Time series classification},
abstract = {A tree-ensemble method, referred to as time series forest (TSF), is proposed for time series classification. TSF employs a combination of entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain improves the accuracy of TSF. TSF randomly samples features at each tree node and has computational complexity linear in the length of time series, and can be built using parallel computing techniques. The temporal importance curve is proposed to capture the temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, standard deviation and slope is computationally efficient and outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping.}
}

@InProceedings{ContractableBOSS,
author="Middlehurst, Matthew
and Vickers, William
and Bagnall, Anthony",
editor="Yin, Hujun
and Camacho, David
and Tino, Peter
and Tall{\'o}n-Ballesteros, Antonio J.
and Menezes, Ronaldo
and Allmendinger, Richard",
title="Scalable Dictionary Classifiers for Time Series Classification",
booktitle="Intelligent Data Engineering and Automated Learning -- IDEAL 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="11--19",
abstract="Dictionary based classifiers are a family of algorithms for time series classification (TSC) that focus on capturing the frequency of pattern occurrences in a time series. The ensemble based Bag of Symbolic Fourier Approximation Symbols (BOSS) was found to be a top performing TSC algorithm in a recent evaluation, as well as the best performing dictionary based classifier. However, BOSS does not scale well. We evaluate changes to the way BOSS chooses classifiers for its ensemble, replacing its parameter search with random selection. This change allows for the easy implementation of contracting (setting a build time limit for the classifier) and check-pointing (saving progress during the classifiers build). We achieve a significant reduction in build time without a significant change in accuracy on average when compared to BOSS by creating a fixed size weighted ensemble selecting the best performers from a randomly chosen parameter set. Our experiments are conducted on datasets from the recently expanded UCR time series archive. We demonstrate the usability improvements to randomised BOSS with a case study using a large whale acoustics dataset for which BOSS proved infeasible.",
isbn="978-3-030-33607-3"
}

% DrCIFClassifier é introduzido no paper do HIVECOTE V2

@article{ElasticEnsemble,
  author       = {Jason Lines and
                  Anthony J. Bagnall},
  title        = {Time series classification with ensembles of elastic distance measures},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {29},
  number       = {3},
  pages        = {565--592},
  year         = {2015},
  url          = {https://doi.org/10.1007/s10618-014-0361-2},
  doi          = {10.1007/S10618-014-0361-2},
  timestamp    = {Wed, 14 Nov 2018 10:41:43 +0100},
  biburl       = {https://dblp.org/rec/journals/datamine/LinesB15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{FCNClassifier-MLPClassifier,
  author       = {Zhiguang Wang and
                  Weizhong Yan and
                  Tim Oates},
  title        = {Time series classification from scratch with deep neural networks:
                  {A} strong baseline},
  booktitle    = {2017 International Joint Conference on Neural Networks, {IJCNN} 2017,
                  Anchorage, AK, USA, May 14-19, 2017},
  pages        = {1578--1585},
  publisher    = {{IEEE}},
  year         = {2017},
  url          = {https://doi.org/10.1109/IJCNN.2017.7966039},
  doi          = {10.1109/IJCNN.2017.7966039},
  timestamp    = {Sun, 16 Oct 2022 14:19:57 +0200},
  biburl       = {https://dblp.org/rec/conf/ijcnn/WangYO17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Inception1,
author = {Ismail Fawaz, Hassan and Lucas, Benjamin and Forestier, Germain and Pelletier, Charlotte and Schmidt, Daniel F. and Weber, Jonathan and Webb, Geoffrey I. and Idoumghar, Lhassane and Muller, Pierre-Alain and Petitjean, Fran\c{c}ois},
title = {InceptionTime: Finding AlexNet for time series classification},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {6},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-020-00710-y},
doi = {10.1007/s10618-020-00710-y},
abstract = {This paper brings deep learning at the forefront of research into time series classification (TSC). TSC is the area of machine learning tasked with the categorization (or labelling) of time series. The last few decades of work in this area have led to significant progress in the accuracy of classifiers, with the state of the art now represented by the HIVE-COTE algorithm. While extremely accurate, HIVE-COTE cannot be applied to many real-world datasets because of its high training time complexity in O(N2·T4) for a dataset with N time series of length T. For example, it takes HIVE-COTE more than 8 days to learn from a small dataset with N=1500 time series of short length T=46. Meanwhile deep learning has received enormous attention because of its high accuracy and scalability. Recent approaches to deep learning for TSC have been scalable, but less accurate than HIVE-COTE. We introduce InceptionTime—an ensemble of deep Convolutional Neural Network models, inspired by the Inception-v4 architecture. Our experiments show that InceptionTime is on par with HIVE-COTE in terms of accuracy while being much more scalable: not only can it learn from 1500 time series in one hour but it can also learn from 8M time series in 13 h, a quantity of data that is fully out of reach of HIVE-COTE.},
journal = {Data Min. Knowl. Discov.},
month = {nov},
pages = {1936–1962},
numpages = {27},
keywords = {Inception, Scalable model, Deep learning, Time series classification}
}

@INPROCEEDINGS{Inception2,
  author={Ismail-Fawaz, Ali and Devanne, Maxime and Weber, Jonathan and Forestier, Germain},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Deep Learning For Time Series Classification Using New Hand-Crafted Convolution Filters}, 
  year={2022},
  volume={},
  number={},
  pages={972-981},
  keywords={Training;Deep learning;Convolution;Image color analysis;Time series analysis;Computer architecture;Big Data;Time Series Classification;Convolution Neural Networks;Pattern Recognition;Feature Engineering;handcrafted Filters},
  doi={10.1109/BigData55660.2022.10020496}}

@INPROCEEDINGS{LITETimeClassifier,
  author={Ismail-Fawaz, Ali and Devanne, Maxime and Berretti, Stefano and Weber, Jonathan and Forestier, Germain},
  booktitle={2023 IEEE 10th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={LITE: Light Inception with boosTing tEchniques for Time Series Classification}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  keywords={Training;Deep learning;Representation learning;Multiplexing;Power demand;Time series analysis;Neural networks;Time Series Classification;Neural Networks;Convolutional Neural Networks;DepthWise Separable Convolutions},
  doi={10.1109/DSAA60987.2023.10302569}}

@inproceedings{TDE,
author = {Middlehurst, Matthew and Large, James and Cawley, Gavin and Bagnall, Anthony},
title = {The Temporal Dictionary Ensemble (TDE) Classifier for Time Series Classification},
year = {2020},
isbn = {978-3-030-67657-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67658-2_38},
doi = {10.1007/978-3-030-67658-2_38},
abstract = {Using bag of words representations of time series is a popular approach to time series classification (TSC). These algorithms involve approximating and discretising windows over a series to form words, then forming a count of words over a given dictionary. Classifiers are constructed on the resulting histograms of word counts. A 2017 evaluation of a range of time series classifiers found the bag of symbolic-Fourier approximation symbols (BOSS) ensemble the best of the dictionary based classifiers. It forms one of the components of hierarchical vote collective of transformation-based ensembles (HIVE-COTE), which represents the current state of the art. Since then, several new dictionary based algorithms have been proposed that are more accurate or more scalable (or both) than BOSS. We propose a further extension of these dictionary based classifiers that combines the best elements of the others combined with a novel approach to constructing ensemble members based on an adaptive Gaussian process model of the parameter space. We demonstrate that the Temporal Dictionary Ensemble (TDE) is more accurate than other dictionary based approaches. Furthermore, unlike the other classifiers, if we replace BOSS in HIVE-COTE with TDE, HIVE-COTE becomes significantly more accurate. We also show this new version of HIVE-COTE is significantly more accurate than the current top performing classifiers on the UCR time series archive. This advance represents a new state of the art for time series classification.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part I},
pages = {660–676},
numpages = {17},
keywords = {Time series, Classification, Bag of words, HIVE-COTE},
location = {<conf-loc content-type="InPerson">Ghent, Belgium</conf-loc>}
}



@article{MUSE,
  author       = {Patrick Sch{\"{a}}fer and
                  Ulf Leser},
  title        = {Multivariate Time Series Classification with {WEASEL+MUSE}},
  journal      = {CoRR},
  volume       = {abs/1711.11343},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.11343},
  eprinttype    = {arXiv},
  eprint       = {1711.11343},
  timestamp    = {Mon, 13 Aug 2018 16:48:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-11343.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{WEASEL,
author = {Sch\"{a}fer, Patrick and Leser, Ulf},
title = {Fast and Accurate Time Series Classification with WEASEL},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132980},
doi = {10.1145/3132847.3132980},
abstract = {Time series (TS) occur in many scientific and commercial applications, ranging from earth surveillance to industry automation to the smart grids. An important type of TS analysis is classification, which can, for instance, improve energy load forecasting in smart grids by detecting the types of electronic devices based on their energy consumption profiles recorded by automatic sensors. Such sensor-driven applications are very often characterized by (a) very long TS and (b) very large TS datasets needing classification. However, current methods to time series classification (TSC) cannot cope with such data volumes at acceptable accuracy; they are either scalable but offer only inferior classification quality, or they achieve state-of-the-art classification quality but cannot scale to large data volumes. In this paper, we present WEASEL (Word ExtrAction for time SEries cLassification), a novel TSC method which is both fast and accurate. Like other state-of-the-art TSC methods, WEASEL transforms time series into feature vectors, using a sliding-window approach, which are then analyzed through a machine learning classifier. The novelty of WEASEL lies in its specific method for deriving features, resulting in a much smaller yet much more discriminative feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more accurate than the best current non-ensemble algorithms at orders-of-magnitude lower classification and training times, and it is almost as accurate as ensemble classifiers, whose computational complexity makes them inapplicable even for mid-size datasets. The outstanding robustness of WEASEL is also confirmed by experiments on two real smart grid datasets, where it out-of-the-box achieves almost the same accuracy as highly tuned, domain-specific methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {637–646},
numpages = {10},
keywords = {bag-of-patterns, classification, feature selection, time series, word co-occurrences},
location = {Singapore, Singapore},
series = {CIKM '17}
}



@article{WEASELV2,
  author       = {Patrick Sch{\"{a}}fer and
                  Ulf Leser},
  title        = {{WEASEL} 2.0: a random dilated dictionary transform for fast, accurate
                  and memory constrained time series classification},
  journal      = {Mach. Learn.},
  volume       = {112},
  number       = {12},
  pages        = {4763--4788},
  year         = {2023},
  url          = {https://doi.org/10.1007/s10994-023-06395-w},
  doi          = {10.1007/S10994-023-06395-W},
  timestamp    = {Sun, 10 Dec 2023 17:00:49 +0100},
  biburl       = {https://dblp.org/rec/journals/ml/SchaferL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{REDCOMETS-1,
  author       = {Luca A. Bennett and
                  Zahraa S. Abdallah},
  editor       = {Georgiana Ifrim and
                  Romain Tavenard and
                  Anthony J. Bagnall and
                  Patrick Sch{\"{a}}fer and
                  Simon Malinowski and
                  Thomas Guyet and
                  Vincent Lemaire},
  title        = {{RED} CoMETS: An Ensemble Classifier for Symbolically Represented
                  Multivariate Time Series},
  booktitle    = {Advanced Analytics and Learning on Temporal Data - 8th {ECML} {PKDD}
                  Workshop, {AALTD} 2023, Turin, Italy, September 18-22, 2023, Revised
                  Selected Papers},
  series       = {Lecture Notes in Computer Science},
  volume       = {14343},
  pages        = {76--91},
  publisher    = {Springer},
  year         = {2023},
  url          = {https://doi.org/10.1007/978-3-031-49896-1\_6},
  doi          = {10.1007/978-3-031-49896-1\_6},
  timestamp    = {Fri, 26 Jan 2024 07:55:30 +0100},
  biburl       = {https://dblp.org/rec/conf/aaltd/BennettA23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{REDCOMETS-2,
  author       = {Zahraa S. Abdallah and
                  Mohamed Medhat Gaber},
  title        = {Co-eye: a multi-resolution ensemble classifier for symbolically approximated
                  time series},
  journal      = {Mach. Learn.},
  volume       = {109},
  number       = {11},
  pages        = {2029--2061},
  year         = {2020},
  url          = {https://doi.org/10.1007/s10994-020-05887-3},
  doi          = {10.1007/S10994-020-05887-3},
  timestamp    = {Fri, 14 May 2021 08:31:53 +0200},
  biburl       = {https://dblp.org/rec/journals/ml/AbdallahG20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





